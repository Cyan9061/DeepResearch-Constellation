import requests
import time
import re
import threading
import json
from typing import List, Dict, Tuple
from concurrent.futures import ThreadPoolExecutor, as_completed
from config import (
    API_KEYS, 
    API_KEYS_2,
    API_ENDPOINT, 
    MODEL_NAME,
    SUMMARY_MODEL_NAME,
    SUMMARY_API_ENDPOINT,
    MAX_TOKENS_PER_REQUEST,
    SUMMARY_MAX_TOKENS_PER_REQUEST,
    MAX_CONTENT_LENGTH,
    API_TIMEOUT,
    SUMMARY_API_TIMEOUT,
    API_RETRY_COUNT,
    SUMMARY_API_RETRY_COUNT,
    SUMMARY_TEMPERATURE,
    MAX_INPUT_TOKENS,
    ENABLE_CONCURRENT_ANALYSIS,
    MAX_CONCURRENT_ANALYSIS,
    CONCURRENT_BATCH_SIZE,
    ANALYSIS_RATE_LIMIT_DELAY,
    MAX_ANAYLISE_PAPERS,
    SINGLE_ANAYLISE_LENTH,
    MAX_ANAYLISE_OUTPUT_LENGTH,
    ADEQUACY_EVALUATION_THRESHOLD,
    MIN_DEPTH_SEARCH_SCORE,
    MAX_NEW_KEYWORDS_PER_DEPTH,
)

class DeepSeekClient:
    def __init__(self):
        self.api_keys = API_KEYS
        self.summary_api_keys = API_KEYS_2 if API_KEYS_2 else API_KEYS  # å¦‚æœæ²¡æœ‰ä¸“ç”¨å¯†é’¥ï¼Œä½¿ç”¨æ™®é€šå¯†é’¥
        self.endpoint = API_ENDPOINT
        self.summary_endpoint = SUMMARY_API_ENDPOINT
        self.model = MODEL_NAME
        self.summary_model = SUMMARY_MODEL_NAME
        self.current_key_index = 0
        self.current_summary_key_index = 0
        self.key_lock = threading.Lock()  # çº¿ç¨‹å®‰å…¨çš„å¯†é’¥è½®æ¢
        self.summary_key_lock = threading.Lock()  # æ€»ç»“APIå¯†é’¥é”
        
        # ä»é…ç½®æ–‡ä»¶è¯»å–è®¾ç½®
        self.max_tokens_per_request = MAX_TOKENS_PER_REQUEST
        self.max_content_length = MAX_CONTENT_LENGTH
        self.api_timeout = API_TIMEOUT
        self.max_retries = API_RETRY_COUNT
        
        print(f"ğŸ”§ DeepSeekå®¢æˆ·ç«¯åˆå§‹åŒ–å®Œæˆ")
        print(f"   - å¯ç”¨APIå¯†é’¥æ•°é‡: {len(self.api_keys)}")
        print(f"   - æ€»ç»“ä¸“ç”¨å¯†é’¥æ•°é‡: {len(self.summary_api_keys)}")
        print(f"   - æ™®é€šæ¨¡å‹: {self.model}")
        print(f"   - æ€»ç»“æ¨¡å‹: {self.summary_model}")
        print(f"   - æœ€å¤§tokenæ•°: {self.max_tokens_per_request}")
        print(f"   - å¹¶å‘åˆ†æ: {'å¯ç”¨' if ENABLE_CONCURRENT_ANALYSIS else 'ç¦ç”¨'}")
        if ENABLE_CONCURRENT_ANALYSIS:
            print(f"   - æœ€å¤§å¹¶å‘æ•°: {MAX_CONCURRENT_ANALYSIS}")
    
    def _get_next_api_key(self):
        """çº¿ç¨‹å®‰å…¨çš„APIå¯†é’¥è½®æ¢"""
        with self.key_lock:
            key = self.api_keys[self.current_key_index]
            self.current_key_index = (self.current_key_index + 1) % len(self.api_keys)
            return key
    
    def _get_next_summary_api_key(self):
        """çº¿ç¨‹å®‰å…¨çš„æ€»ç»“APIå¯†é’¥è½®æ¢"""
        with self.summary_key_lock:
            key = self.summary_api_keys[self.current_summary_key_index]
            self.current_summary_key_index = (self.current_summary_key_index + 1) % len(self.summary_api_keys)
            return key
    
    def _estimate_tokens(self, text: str) -> int:
        """ä¼°ç®—æ–‡æœ¬çš„tokenæ•°é‡"""
        # ç²—ç•¥ä¼°ç®—ï¼šè‹±æ–‡çº¦4å­—ç¬¦=1tokenï¼Œä¸­æ–‡çº¦1.5å­—ç¬¦=1token
        chinese_chars = len(re.findall(r'[\u4e00-\u9fff]', text))
        english_chars = len(text) - chinese_chars
        estimated_tokens = (english_chars // 4) + (chinese_chars * 2 // 3)
        return estimated_tokens
    
    def _truncate_content_if_needed(self, content: str, max_tokens: int = None) -> str:
        """å¦‚æœå†…å®¹å¤ªé•¿ï¼Œæ™ºèƒ½æˆªæ–­"""
        if max_tokens is None:
            max_tokens = MAX_INPUT_TOKENS  # ç•™ä¸€åŠç»™å›å¤
        
        estimated_tokens = self._estimate_tokens(content)
        
        if estimated_tokens <= max_tokens:
            return content
        
        # éœ€è¦æˆªæ–­
        target_length = int(len(content) * (max_tokens / estimated_tokens) * 0.9)  # ä¿å®ˆä¼°è®¡
        
        if target_length < len(content):
            # å°è¯•åœ¨å¥å·å¤„æˆªæ–­
            truncated = content[:target_length]
            last_period = truncated.rfind('.')
            
            if last_period > target_length // 2:
                truncated = truncated[:last_period + 1]
            
            # æ·»åŠ æˆªæ–­æç¤º
            truncated += f"\n\n[ç”±äºé•¿åº¦é™åˆ¶ï¼Œå†…å®¹å·²ä»{len(content)}å­—ç¬¦æˆªæ–­è‡³{len(truncated)}å­—ç¬¦]"
            
            print(f"âš ï¸ å†…å®¹å·²ä»{len(content)}å­—ç¬¦æˆªæ–­è‡³{len(truncated)}å­—ç¬¦")
            return truncated
        
        return content
    
    def ask(self, prompt: str, max_retries: int = None, temperature: float = 0.3, use_summary_api: bool = False) -> str:
        """å‘é€è¯·æ±‚åˆ°DeepSeekï¼Œæ”¯æŒé€‰æ‹©ä½¿ç”¨æ€»ç»“API"""
        if max_retries is None:
            max_retries = SUMMARY_API_RETRY_COUNT if use_summary_api else self.max_retries
        
        # æ ¹æ®æ˜¯å¦ä½¿ç”¨æ€»ç»“APIé€‰æ‹©é…ç½®
        if use_summary_api:
            api_key = self._get_next_summary_api_key()
            endpoint = self.summary_endpoint
            model = self.summary_model
            max_tokens = SUMMARY_MAX_TOKENS_PER_REQUEST
            timeout = SUMMARY_API_TIMEOUT
            max_input_tokens = int(MAX_INPUT_TOKENS * 1.5)  # æ€»ç»“APIå¯ä»¥å¤„ç†æ›´å¤štoken
        else:
            api_key = self._get_next_api_key()
            endpoint = self.endpoint
            model = self.model
            max_tokens = self.max_tokens_per_request
            timeout = self.api_timeout
            max_input_tokens = MAX_INPUT_TOKENS
        
        # ä¼°ç®—tokenæ•°é‡ï¼Œå¦‚æœè¶…å‡ºé™åˆ¶åˆ™æˆªæ–­
        estimated_tokens = self._estimate_tokens(prompt)
        if estimated_tokens > max_input_tokens:
            print(f"âš ï¸ è¾“å…¥å†…å®¹tokenæ•°é‡ ({estimated_tokens}) è¶…è¿‡é™åˆ¶ ({max_input_tokens})ï¼Œæ­£åœ¨æˆªæ–­...")
            prompt = self._truncate_content_if_needed(prompt, max_tokens=max_input_tokens)
        
        # æ£€æŸ¥å¹¶æˆªæ–­è¿‡é•¿çš„å†…å®¹ï¼ˆåŸºäºå­—ç¬¦é•¿åº¦é™åˆ¶ï¼‰
        if len(prompt) > self.max_content_length:
            prompt = self._truncate_content_if_needed(prompt)
        
        headers = {
            "Authorization": f"Bearer {api_key}",
            "Content-Type": "application/json"
        }
        
        payload = {
            "model": model,
            "messages": [{"role": "user", "content": prompt}],
            "stream": False,
            "max_tokens": min(max_tokens, 16384),
            "temperature": temperature,
        }
        
        for attempt in range(max_retries):
            try:
                response = requests.post(
                    endpoint, 
                    json=payload, 
                    headers=headers, 
                    timeout=timeout
                )
                
                if response.status_code == 200:
                    result = response.json()
                    return result['choices'][0]['message']['content'].strip()
                elif response.status_code == 429:
                    # Rate limit exceeded
                    print(f"âš ï¸ APIè°ƒç”¨é¢‘ç‡é™åˆ¶ï¼Œåˆ‡æ¢å¯†é’¥...")
                    if use_summary_api:
                        api_key = self._get_next_summary_api_key()
                    else:
                        api_key = self._get_next_api_key()
                    headers["Authorization"] = f"Bearer {api_key}"
                    time.sleep(2)
                    continue
                else:
                    print(f"APIé”™è¯¯: {response.status_code} - {response.text}")
                    
            except Exception as e:
                print(f"è¯·æ±‚å¤±è´¥ (å°è¯• {attempt + 1}): {e}")
                if attempt < max_retries - 1:
                    time.sleep(2 ** attempt)
        
        return "é”™è¯¯: æ— æ³•è·å–å“åº”"
    
    def generate_search_queries(self, research_topic: str, num_queries: int = 3) -> List[str]:
        """ç”ŸæˆarXivå…¼å®¹çš„æœç´¢æŸ¥è¯¢"""
        
        prompt = f"""
ä½ éœ€è¦ä¸ºç ”ç©¶ä¸»é¢˜ç”Ÿæˆ{num_queries}ä¸ªçº¯å‡€çš„è‹±æ–‡æœç´¢æŸ¥è¯¢ï¼Œç”¨äºåœ¨arXivä¸Šæœç´¢è®ºæ–‡ã€‚

ç ”ç©¶ä¸»é¢˜: "{research_topic}"

è¦æ±‚:
1. åªè¿”å›çº¯è‹±æ–‡å…³é”®è¯ç»„åˆï¼Œæ¯è¡Œä¸€ä¸ªæŸ¥è¯¢
2. ä¸è¦åŒ…å«ä»»ä½•è§£é‡Šã€æè¿°æˆ–ä¸­æ–‡
3. ä¸è¦ä½¿ç”¨ç‰¹æ®Šç¬¦å·å¦‚**ã€()ã€å¼•å·ç­‰
4. ä¸è¦ä½¿ç”¨å¤æ‚è¯­æ³•å¦‚PA:ã€TS:ã€PUBYEARç­‰
5. ä¿æŒç®€æ´ï¼Œæ¯ä¸ªæŸ¥è¯¢2-4ä¸ªå…³é”®è¯

ç¤ºä¾‹æ ¼å¼ï¼ˆç›´æ¥è¿”å›æŸ¥è¯¢ï¼Œä¸è¦ç¼–å·ï¼‰:
transformer attention mechanism
neural networks attention
self attention transformer

ç°åœ¨ä¸ºä¸»é¢˜"{research_topic}"ç”Ÿæˆ{num_queries}ä¸ªæŸ¥è¯¢:
"""
        
        response = self.ask(prompt)
        
        # è§£æå¹¶æ¸…ç†æŸ¥è¯¢
        queries = self._parse_and_clean_queries_improved(response, num_queries)
        
        # å¦‚æœç”Ÿæˆçš„æŸ¥è¯¢ä¸å¤Ÿï¼Œæ·»åŠ fallbackæŸ¥è¯¢
        if len(queries) < num_queries:
            print(f"âš ï¸ ç”Ÿæˆçš„æŸ¥è¯¢ä¸è¶³ï¼Œæ·»åŠ fallbackæŸ¥è¯¢...")
            fallback_queries = self._generate_fallback_queries(research_topic)
            queries.extend(fallback_queries)
        
        return queries[:num_queries]
    
    def _parse_and_clean_queries_improved(self, response: str, num_queries: int) -> List[str]:
        """æ”¹è¿›çš„æŸ¥è¯¢è§£æå’Œæ¸…ç†"""
        queries = []
        
        # åˆ†è¡Œå¤„ç†
        lines = response.split('\n')
        
        for line in lines:
            line = line.strip()
            
            # è·³è¿‡ç©ºè¡Œã€æ ‡é¢˜è¡Œã€è¯´æ˜è¡Œ
            if not line:
                continue
            if any(keyword in line.lower() for keyword in ['è¦æ±‚', 'ç¤ºä¾‹', 'æ ¼å¼', 'ç°åœ¨', 'ä¸»é¢˜', 'ç”Ÿæˆ', 'æŸ¥è¯¢', 'requirement', 'example']):
                continue
            
            # æå–å¯èƒ½çš„æŸ¥è¯¢
            potential_query = self._extract_query_from_line(line)
            
            if potential_query:
                cleaned_query = self._clean_query_aggressive(potential_query)
                
                if self._is_valid_query(cleaned_query):
                    queries.append(cleaned_query)
                    if len(queries) >= num_queries:
                        break
        
        return queries
    
    def _extract_query_from_line(self, line: str) -> str:
        """ä»ä¸€è¡Œæ–‡æœ¬ä¸­æå–æŸ¥è¯¢"""
        # ç§»é™¤å¸¸è§çš„ç¼–å·å’Œå‰ç¼€
        line = re.sub(r'^\d+[\.\)]\s*', '', line)  # ç§»é™¤ "1. " æˆ– "1) "
        line = re.sub(r'^[-*â€¢]\s*', '', line)      # ç§»é™¤ "- " æˆ– "* "
        
        # ç§»é™¤ markdown æ ¼å¼
        line = re.sub(r'\*\*(.*?)\*\*', r'\1', line)  # ç§»é™¤ **text**
        line = re.sub(r'\*(.*?)\*', r'\1', line)      # ç§»é™¤ *text*
        
        # ç§»é™¤å¼•å·
        line = re.sub(r'["""\'\'`]', '', line)
        
        # å¦‚æœåŒ…å«ä¸­æ–‡è§£é‡Šï¼Œåªå–è‹±æ–‡éƒ¨åˆ†
        chinese_match = re.search(r'[\u4e00-\u9fff]', line)
        if chinese_match:
            line = line[:chinese_match.start()]
        
        # ç§»é™¤ç‰¹æ®Šç¬¦å·å’Œå¤šä½™ç©ºæ ¼
        line = re.sub(r'[^\w\s]', ' ', line)
        line = re.sub(r'\s+', ' ', line)
        
        return line.strip()
    
    def _clean_query_aggressive(self, query: str) -> str:
        """ç§¯æçš„æŸ¥è¯¢æ¸…ç†"""
        if not query:
            return ""
        
        # ç§»é™¤æ‰€æœ‰éè‹±æ–‡å­—æ¯ã€æ•°å­—ã€ç©ºæ ¼çš„å­—ç¬¦
        query = re.sub(r'[^\w\s]', ' ', query)
        
        # ç§»é™¤ä¸­æ–‡å­—ç¬¦
        query = re.sub(r'[\u4e00-\u9fff]', ' ', query)
        
        # ç§»é™¤æ•°å­—ï¼ˆé€šå¸¸ä¸éœ€è¦ï¼‰
        query = re.sub(r'\b\d+\b', ' ', query)
        
        # ç§»é™¤å•ä¸ªå­—æ¯ï¼ˆé™¤äº†å¸¸è§ç¼©å†™ï¼‰
        words = query.split()
        valid_words = []
        for word in words:
            word = word.strip().lower()
            if len(word) >= 2 or word in ['ai', 'ml', 'dl', 'cv', 'nlp', 'db']:
                valid_words.append(word)
        
        # é‡æ–°ç»„åˆ
        query = ' '.join(valid_words)
        
        # ç§»é™¤å¤šä½™ç©ºæ ¼
        query = re.sub(r'\s+', ' ', query).strip()
        
        return query
    
    def _is_valid_query(self, query: str) -> bool:
        """éªŒè¯æŸ¥è¯¢æ˜¯å¦æœ‰æ•ˆ"""
        if not query or len(query) < 3:
            return False
        
        # æ£€æŸ¥æ˜¯å¦åŒ…å«è¶³å¤Ÿçš„è‹±æ–‡å­—æ¯
        english_chars = len(re.findall(r'[a-zA-Z]', query))
        if english_chars < 3:
            return False
        
        # æ£€æŸ¥å•è¯æ•°é‡ï¼ˆè‡³å°‘1ä¸ªï¼Œæœ€å¤š8ä¸ªï¼‰
        words = query.split()
        if len(words) < 1 or len(words) > 8:
            return False
        
        # æ£€æŸ¥æ˜¯å¦åŒ…å«æ— æ•ˆå†…å®¹
        invalid_patterns = ['è¦æ±‚', 'ç¤ºä¾‹', 'æ ¼å¼', 'ä¸»é¢˜', 'æŸ¥è¯¢', 'example', 'query', 'search']
        for pattern in invalid_patterns:
            if pattern.lower() in query.lower():
                return False
        
        return True
    
    def _generate_fallback_queries(self, research_topic: str) -> List[str]:
        """ç”ŸæˆfallbackæŸ¥è¯¢"""
        print(f"ğŸ“ ç”ŸæˆfallbackæŸ¥è¯¢ï¼Œä¸»é¢˜: {research_topic}")
        
        keywords = self._extract_keywords_from_topic(research_topic)
        
        fallback_queries = []
        
        if keywords:
            # ç”Ÿæˆç®€å•çš„å…³é”®è¯ç»„åˆ
            if len(keywords) >= 2:
                fallback_queries.append(' '.join(keywords[:2]))
            if len(keywords) >= 3:
                fallback_queries.append(' '.join(keywords[:3]))
            
            # å•ä¸ªé‡è¦å…³é”®è¯
            for keyword in keywords[:2]:
                if len(keyword) > 4:
                    fallback_queries.append(keyword)
        
        # æ·»åŠ ä¸€äº›é€šç”¨çš„å¤‡ç”¨æŸ¥è¯¢
        if 'transformer' in research_topic.lower():
            fallback_queries.extend([
                'transformer attention',
                'attention mechanism',
                'self attention'
            ])
        elif 'neural' in research_topic.lower():
            fallback_queries.extend([
                'neural networks',
                'deep learning',
                'machine learning'
            ])
        else:
            # é€šç”¨æŸ¥è¯¢
            fallback_queries.extend([
                'deep learning',
                'machine learning',
                'artificial intelligence'
            ])
        
        print(f"ğŸ“ ç”Ÿæˆçš„fallbackæŸ¥è¯¢: {fallback_queries}")
        return fallback_queries
    
    def _extract_keywords_from_topic(self, topic: str) -> List[str]:
        """ä»ç ”ç©¶ä¸»é¢˜ä¸­æå–å…³é”®è¯"""
        # ç§»é™¤ä¸­æ–‡å’Œç‰¹æ®Šå­—ç¬¦ï¼Œæå–è‹±æ–‡å…³é”®è¯
        english_text = re.sub(r'[^\w\s]', ' ', topic)
        english_text = re.sub(r'[\u4e00-\u9fff]', ' ', english_text)  # ç§»é™¤ä¸­æ–‡
        
        # åˆ†è¯å¹¶è¿‡æ»¤
        words = english_text.split()
        
        # è¿‡æ»¤åœç”¨è¯å’ŒçŸ­è¯
        stop_words = {'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with', 'by'}
        keywords = [word.lower() for word in words 
                   if word.lower() not in stop_words and len(word) > 2]
        
        return keywords[:5]
    
    def analyze_paper_text(self, title: str, abstract: str, text_chunks: List[str] = None) -> str:
        """
        åˆ†æå•ç¯‡è®ºæ–‡ï¼Œä½¿ç”¨ç´¯ç§¯å¼åˆ†æå¤„ç†å¤šä¸ªchunks
        
        Args:
            title: è®ºæ–‡æ ‡é¢˜
            abstract: è®ºæ–‡æ‘˜è¦
            text_chunks: PDFå¤„ç†å™¨æä¾›çš„æ–‡æœ¬å—åˆ—è¡¨
        
        Returns:
            åˆ†æç»“æœ
        """
        
        # å¦‚æœæ²¡æœ‰æä¾›chunksæˆ–åªæœ‰ä¸€ä¸ªchunkï¼Œç›´æ¥åˆ†æ
        if not text_chunks or len(text_chunks) == 1:
            content_to_analyze = text_chunks[0] if text_chunks else abstract
            content_type = "å®Œæ•´æ–‡æœ¬" if text_chunks else "æ‘˜è¦"
            return self._analyze_single_content(title, content_to_analyze, content_type)
        
        # å¤šä¸ªchunksçš„ç´¯ç§¯å¼åˆ†æ
        print(f"ğŸ“š ä½¿ç”¨ç´¯ç§¯å¼åˆ†æå¤„ç†{len(text_chunks)}ä¸ªæ–‡æœ¬å—...")
        return self._analyze_with_cumulative_approach(title, abstract, text_chunks)
    
    def _analyze_single_content(self, title: str, content: str, content_type: str) -> str:
        """åˆ†æå•ä¸ªå†…å®¹å—"""
        prompt = f"""
è¯·åŸºäº{content_type}åˆ†æè¿™ç¯‡ç ”ç©¶è®ºæ–‡å¹¶æä¾›ç»“æ„åŒ–æ€»ç»“:

æ ‡é¢˜: {title}

å†…å®¹:
{content}

è¯·æä¾›:
1. ä¸»è¦è´¡çŒ® (1-2å¥è¯)
2. æŠ€æœ¯æ–¹æ³• (3-4å¥è¯ï¼ŒåŒ…å«å…·ä½“æ–¹æ³•/ç®—æ³•)
3. å…³é”®ç»“æœ/å‘ç° (2-3å¥è¯)
4. å¯¹è¯¥é¢†åŸŸçš„æ„ä¹‰ (2å¥è¯)
5. å±€é™æ€§æˆ–æœªæ¥å·¥ä½œ (1-2å¥è¯)
6. å…³é”®æŠ€æœ¯è¯æ±‡ (5-7ä¸ªæŠ€æœ¯æœ¯è¯­)

è¯·ä¿æŒç®€æ´ä½†å…¨é¢ï¼Œé‡ç‚¹å…³æ³¨æŠ€æœ¯ç»†èŠ‚å’Œç ”ç©¶å½±å“ã€‚
"""
        
        return self.ask(prompt, temperature=0.2)
    
    def _analyze_with_cumulative_approach(self, title: str, abstract: str, text_chunks: List[str]) -> str:
        """
        ä½¿ç”¨ç´¯ç§¯å¼æ–¹æ³•åˆ†æå¤šä¸ªæ–‡æœ¬å—
        
        åˆ†ææµç¨‹: chunk1 -> a1, a1+chunk2 -> a2, a2+chunk3 -> a3, ...
        """
        
        # åˆå§‹åˆ†æï¼šåŸºäºæ‘˜è¦å’Œç¬¬ä¸€ä¸ªchunk
        print(f"  ğŸ“ åˆ†æç¬¬1ä¸ªå—...")
        
        first_chunk_prompt = f"""
è¯·åˆ†æè¿™ç¯‡ç ”ç©¶è®ºæ–‡çš„å¼€å§‹éƒ¨åˆ†å¹¶æä¾›åˆæ­¥æ€»ç»“:

æ ‡é¢˜: {title}
æ‘˜è¦: {abstract}

è®ºæ–‡å¼€å§‹éƒ¨åˆ†:
{text_chunks[0]}

è¯·æä¾›ç®€æ´çš„åˆæ­¥åˆ†æ:
1. ä¸»è¦ç ”ç©¶ç›®æ ‡å’Œè´¡çŒ®
2. ä½¿ç”¨çš„æŠ€æœ¯æ–¹æ³•
3. åˆæ­¥å‘ç°çš„å…³é”®ä¿¡æ¯
4. é‡è¦çš„æŠ€æœ¯æœ¯è¯­

è¿™æ˜¯å¤šéƒ¨åˆ†åˆ†æçš„ç¬¬1éƒ¨åˆ†ï¼Œè¯·ä¿æŒåˆ†æç®€æ´ï¼Œä¸ºåç»­éƒ¨åˆ†ç•™å‡ºç©ºé—´ã€‚
"""
        
        current_analysis = self.ask(first_chunk_prompt, temperature=0.2)
        
        # é€æ­¥ç´¯ç§¯åˆ†æåç»­chunks
        for i, chunk in enumerate(text_chunks[1:], 2):
            print(f"  ğŸ“ åˆ†æç¬¬{i}ä¸ªå—...")
            
            cumulative_prompt = f"""
ç»§ç»­åˆ†æè¿™ç¯‡ç ”ç©¶è®ºæ–‡ï¼Œè¯·åŸºäºä¹‹å‰çš„åˆ†æç»“æœå’Œæ–°çš„å†…å®¹éƒ¨åˆ†æ›´æ–°æ€»ç»“:

è®ºæ–‡æ ‡é¢˜: {title}

ä¹‹å‰çš„åˆ†æç»“æœ:
{current_analysis}

æ–°çš„å†…å®¹éƒ¨åˆ† (ç¬¬{i}éƒ¨åˆ†):
{chunk}

è¯·æ›´æ–°å’Œå®Œå–„åˆ†æï¼Œé‡ç‚¹å…³æ³¨:
1. æ–°å†…å®¹ä¸­çš„å…³é”®ä¿¡æ¯
2. ä¸ä¹‹å‰åˆ†æçš„å…³è”å’Œè¡¥å……
3. æ›´å®Œæ•´çš„æŠ€æœ¯æ–¹æ³•æè¿°
4. æ–°å‘ç°çš„ç»“æœæˆ–å‘ç°
5. è¡¥å……çš„æŠ€æœ¯æœ¯è¯­

è¯·æä¾›æ›´æ–°åçš„å®Œæ•´åˆ†æï¼Œä¿æŒç»“æ„æ¸…æ™°ã€‚è¿™æ˜¯ç¬¬{i}/{len(text_chunks)}éƒ¨åˆ†ã€‚
"""
            
            try:
                current_analysis = self.ask(cumulative_prompt, temperature=0.2)
                time.sleep(0.3)  # é¿å…è¯·æ±‚è¿‡å¿«
            except Exception as e:
                print(f"    âš ï¸ ç¬¬{i}å—åˆ†æå¤±è´¥: {e}")
                # å¦‚æœæŸä¸ªå—åˆ†æå¤±è´¥ï¼Œç»§ç»­ä½¿ç”¨ä¹‹å‰çš„åˆ†æç»“æœ
                break
        
        # æœ€ç»ˆæ•´ç†åˆ†æç»“æœ
        print(f"  ğŸ”„ æ•´ç†æœ€ç»ˆåˆ†æç»“æœ...")
        
        final_prompt = f"""
åŸºäºå¯¹è®ºæ–‡å„éƒ¨åˆ†çš„ç´¯ç§¯åˆ†æï¼Œè¯·æä¾›æœ€ç»ˆçš„ç»“æ„åŒ–æ€»ç»“:

è®ºæ–‡æ ‡é¢˜: {title}

ç´¯ç§¯åˆ†æç»“æœ:
{current_analysis}

è¯·æä¾›æœ€ç»ˆçš„ç»“æ„åŒ–æ€»ç»“:
1. ä¸»è¦è´¡çŒ® (1-2å¥è¯)
2. æŠ€æœ¯æ–¹æ³• (3-4å¥è¯ï¼ŒåŒ…å«å…·ä½“æ–¹æ³•/ç®—æ³•)
3. å…³é”®ç»“æœ/å‘ç° (2-3å¥è¯)
4. å¯¹è¯¥é¢†åŸŸçš„æ„ä¹‰ (2å¥è¯)
5. å±€é™æ€§æˆ–æœªæ¥å·¥ä½œ (1-2å¥è¯)
6. å…³é”®æŠ€æœ¯è¯æ±‡ (5-7ä¸ªæŠ€æœ¯æœ¯è¯­)

è¯·ç»¼åˆæ‰€æœ‰åˆ†æå†…å®¹ï¼Œæä¾›å®Œæ•´è€Œè¿è´¯çš„æœ€ç»ˆæ€»ç»“ã€‚
"""
        
        try:
            final_analysis = self.ask(final_prompt, temperature=0.2)
            print(f"  âœ… ç´¯ç§¯å¼åˆ†æå®Œæˆ")
            return final_analysis
        except Exception as e:
            print(f"  âŒ æœ€ç»ˆåˆ†æå¤±è´¥: {e}")
            # å¦‚æœæœ€ç»ˆåˆ†æå¤±è´¥ï¼Œè¿”å›æœ€åçš„ç´¯ç§¯ç»“æœ
            return current_analysis + "\n\n[æ³¨: æœ€ç»ˆæ•´ç†æ­¥éª¤å¤±è´¥ï¼Œä½¿ç”¨ç´¯ç§¯åˆ†æç»“æœ]"
    
    def analyze_papers_concurrently(self, papers: List[Dict]) -> List[Dict]:
        """å¹¶å‘åˆ†æå¤šç¯‡è®ºæ–‡"""
        if not ENABLE_CONCURRENT_ANALYSIS or len(papers) <= 1:
            # å¦‚æœç¦ç”¨å¹¶å‘æˆ–è®ºæ–‡æ•°é‡å¤ªå°‘ï¼Œä½¿ç”¨ä¸²è¡Œå¤„ç†
            return self._analyze_papers_sequentially(papers)
        
        print(f"ğŸš€ å¼€å§‹å¹¶å‘åˆ†æ{len(papers)}ç¯‡è®ºæ–‡...")
        print(f"   - æœ€å¤§å¹¶å‘æ•°: {MAX_CONCURRENT_ANALYSIS}")
        print(f"   - æ‰¹å¤„ç†å¤§å°: {CONCURRENT_BATCH_SIZE}")
        
        analyses = []
        total_papers = len(papers)
        
        # åˆ†æ‰¹å¤„ç†è®ºæ–‡
        for batch_start in range(0, total_papers, CONCURRENT_BATCH_SIZE):
            batch_end = min(batch_start + CONCURRENT_BATCH_SIZE, total_papers)
            batch_papers = papers[batch_start:batch_end]
            
            print(f"ğŸ“Š å¤„ç†æ‰¹æ¬¡ {batch_start//CONCURRENT_BATCH_SIZE + 1}: è®ºæ–‡ {batch_start+1}-{batch_end}")
            
            batch_analyses = self._analyze_batch_concurrently(batch_papers, batch_start)
            analyses.extend(batch_analyses)
            
            # æ‰¹æ¬¡ä¹‹é—´æ·»åŠ å»¶è¿Ÿ
            if batch_end < total_papers:
                time.sleep(ANALYSIS_RATE_LIMIT_DELAY * 2)
        
        print(f"âœ… å¹¶å‘åˆ†æå®Œæˆï¼ŒæˆåŠŸåˆ†æ{len(analyses)}ç¯‡è®ºæ–‡")
        return analyses
    
    def _analyze_batch_concurrently(self, papers: List[Dict], batch_offset: int = 0) -> List[Dict]:
        """å¹¶å‘åˆ†æä¸€æ‰¹è®ºæ–‡"""
        analyses = []
        max_workers = min(MAX_CONCURRENT_ANALYSIS, len(self.api_keys), len(papers))
        
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            # æäº¤æ‰€æœ‰åˆ†æä»»åŠ¡
            future_to_paper = {}
            for i, paper in enumerate(papers):
                future = executor.submit(self._analyze_single_paper_with_retry, paper, batch_offset + i + 1)
                future_to_paper[future] = paper
            
            # æ”¶é›†ç»“æœ
            for future in as_completed(future_to_paper):
                paper = future_to_paper[future]
                try:
                    analysis = future.result()
                    if analysis:
                        analyses.append(analysis)
                        print(f"  âœ… å®Œæˆåˆ†æ: {paper['title']}")
                    else:
                        print(f"  âŒ åˆ†æå¤±è´¥: {paper['title']}")
                except Exception as e:
                    print(f"  âŒ åˆ†æå¼‚å¸¸: {paper['title']} - {e}")
        
        return analyses
    
    def _analyze_single_paper_with_retry(self, paper: Dict, paper_index: int) -> Dict:
        """åˆ†æå•ç¯‡è®ºæ–‡å¹¶åŒ…å«é‡è¯•æœºåˆ¶"""
        try:
            # æ·»åŠ å°çš„éšæœºå»¶è¿Ÿä»¥é¿å…åŒæ—¶è¯·æ±‚
            time.sleep(ANALYSIS_RATE_LIMIT_DELAY * (paper_index % 4))
            
            # ä½¿ç”¨æ–°çš„åˆ†ææ–¹æ³•
            text_chunks = paper.get('text_chunks', [])
            analysis = self.analyze_paper_text(paper['title'], paper['abstract'], text_chunks)
            
            return {
                'paper': paper['title'],
                'paper_id': paper.get('arxiv_id', ''),
                'analysis': analysis,
                'text_length': paper.get('text_length', 0),
                'chunks_count': len(text_chunks) if text_chunks else 0
            }
        except Exception as e:
            print(f"âš ï¸ è®ºæ–‡åˆ†æå¤±è´¥: {paper['title'][:50]}... - {e}")
            return None
    
    def _analyze_papers_sequentially(self, papers: List[Dict]) -> List[Dict]:
        """ä¸²è¡Œåˆ†æè®ºæ–‡ï¼ˆfallbackæ–¹æ³•ï¼‰"""
        print(f"ğŸ“ å¼€å§‹ä¸²è¡Œåˆ†æ{len(papers)}ç¯‡è®ºæ–‡...")
        analyses = []
        
        for i, paper in enumerate(papers, 1):
            print(f"  åˆ†æè®ºæ–‡ {i}/{len(papers)}: {paper['title'][:50]}...")
            
            try:
                text_chunks = paper.get('text_chunks', [])
                analysis = self.analyze_paper_text(paper['title'], paper['abstract'], text_chunks)
                
                analyses.append({
                    'paper': paper['title'],
                    'paper_id': paper.get('arxiv_id', ''),
                    'analysis': analysis,
                    'text_length': paper.get('text_length', 0),
                    'chunks_count': len(text_chunks) if text_chunks else 0
                })
                print(f"  âœ… å®Œæˆåˆ†æ")
            except Exception as e:
                print(f"  âŒ åˆ†æå¤±è´¥: {e}")
        
        return analyses
    
    def analyze_multiple_papers_summary(self, paper_analyses: List[Dict], research_topic: str, depth_round: int = 1) -> str:
        """åŸºäºå¤šç¯‡è®ºæ–‡çš„åˆ†æç”Ÿæˆç»¼åˆæ€»ç»“ï¼Œä½¿ç”¨ä¸“ç”¨æ€»ç»“API"""
        
        if not paper_analyses:
            return "æ²¡æœ‰åˆ†æçš„è®ºæ–‡ã€‚"
        
        # æ„å»ºåˆ†ææ‘˜è¦
        analyses_text = ""
        for i, analysis in enumerate(paper_analyses[:MAX_ANAYLISE_PAPERS], 1):  # é™åˆ¶æœ€å¤šMAX_ANAYLISE_PAPERSç¯‡è®ºæ–‡
            analyses_text += f"\nè®ºæ–‡ {i}: {analysis['paper']}\n"
            analyses_text += f"åˆ†æ: {analysis['analysis'][:SINGLE_ANAYLISE_LENTH]}...\n"  # é™åˆ¶æ¯ä¸ªåˆ†æçš„é•¿åº¦
        
        # æˆªæ–­å¦‚æœå¤ªé•¿
        analyses_text = self._truncate_content_if_needed(analyses_text, max_tokens=MAX_ANAYLISE_OUTPUT_LENGTH)
        
        depth_info = f"(ç¬¬{depth_round}è½®æœç´¢)" if depth_round > 1 else ""
        
        prompt = f"""
åŸºäºå¯¹{len(paper_analyses)}ç¯‡å…³äº"{research_topic}"çš„ç ”ç©¶è®ºæ–‡çš„è¯¦ç»†åˆ†æ{depth_info}ï¼Œè¯·æä¾›ä¸€ä¸ªå…¨é¢çš„ç ”ç©¶ç»¼è¿°:

è®ºæ–‡åˆ†æ:
{analyses_text}

è¯·æä¾›:
1. **å½“å‰ç ”ç©¶ç°çŠ¶** (4-5ä¸ªå…³é”®è¶‹åŠ¿å’Œæ¨¡å¼)
2. **ä¸»è¦æŠ€æœ¯æ–¹æ³•** (æ­£åœ¨ä½¿ç”¨çš„æ–¹æ³•ã€ç®—æ³•ã€æ¡†æ¶)
3. **å…³é”®åˆ›æ–°å’Œçªç ´** (æ–°é¢–å’Œé‡è¦çš„å†…å®¹)
4. **ç ”ç©¶ç©ºç™½å’ŒæŒ‘æˆ˜** (ç¼ºå¤±æˆ–å­˜åœ¨é—®é¢˜çš„æ–¹é¢)
5. **æœªæ¥ç ”ç©¶æ–¹å‘** (å…·ä½“å»ºè®®)
6. **å®é™…åº”ç”¨æ„ä¹‰** (ç°å®ä¸–ç•Œçš„åº”ç”¨å’Œå½±å“)

è¯·ç”¨æ¸…æ™°çš„ç« èŠ‚æ ¼å¼å›åº”ï¼Œå…·ä½“ã€æŠ€æœ¯åŒ–ä¸”å¯æ“ä½œã€‚
"""
        
        print(f"ğŸ§  æ­£åœ¨ä½¿ç”¨ä¸“ç”¨æ€»ç»“APIç”Ÿæˆç ”ç©¶ç»¼è¿°...")
        return self.ask(prompt, temperature=SUMMARY_TEMPERATURE, use_summary_api=True)
    
    def evaluate_research_adequacy(self, research_summary: str, research_topic: str, total_papers_analyzed: int) -> Tuple[float, str, List[str]]:
        """
        è¯„ä¼°ç ”ç©¶èµ„æ–™çš„å……åˆ†æ€§ï¼Œè¿”å›(è¯„åˆ†, è¯„ä¼°æŠ¥å‘Š, ç¼ºå¤±é¢†åŸŸåˆ—è¡¨)
        
        Args:
            research_summary: å½“å‰çš„ç ”ç©¶æ€»ç»“
            research_topic: ç ”ç©¶ä¸»é¢˜
            total_papers_analyzed: å·²åˆ†æçš„è®ºæ–‡æ€»æ•°
            
        Returns:
            (adequacy_score, evaluation_report, missing_areas)
        """
        
        print(f"ğŸ” æ­£åœ¨è¯„ä¼°ç ”ç©¶èµ„æ–™å……åˆ†æ€§...")
        
        prompt = f"""
ä½œä¸ºä¸€ä½èµ„æ·±å­¦æœ¯ç ”ç©¶ä¸“å®¶ï¼Œè¯·è¯„ä¼°å…³äº"{research_topic}"çš„å½“å‰ç ”ç©¶èµ„æ–™æ˜¯å¦å……åˆ†ã€‚

å½“å‰ç ”ç©¶æ€»ç»“:
{research_summary}

å·²åˆ†æè®ºæ–‡æ•°é‡: {total_papers_analyzed}

è¯·ä»ä»¥ä¸‹è§’åº¦è¿›è¡Œè¯„ä¼°:

1. **è¦†ç›–åº¦è¯„ä¼°**: 
   - ä¸»è¦ç ”ç©¶æ–¹å‘æ˜¯å¦éƒ½æœ‰æ¶‰åŠ
   - å…³é”®æŠ€æœ¯æ–¹æ³•æ˜¯å¦å…¨é¢
   - é‡è¦åº”ç”¨é¢†åŸŸæ˜¯å¦è¦†ç›–

2. **æ·±åº¦è¯„ä¼°**:
   - ç†è®ºåŸºç¡€æ˜¯å¦å……åˆ†
   - æŠ€æœ¯ç»†èŠ‚æ˜¯å¦è¯¦ç»†
   - å®éªŒéªŒè¯æ˜¯å¦å……åˆ†

3. **æ—¶æ•ˆæ€§è¯„ä¼°**:
   - æ˜¯å¦åŒ…å«æœ€æ–°ç ”ç©¶è¿›å±•
   - æ˜¯å¦é—æ¼é‡è¦çš„æ–°æ–¹æ³•

4. **å®Œæ•´æ€§è¯„ä¼°**:
   - æ˜¯å¦å­˜åœ¨æ˜æ˜¾çš„ç ”ç©¶ç©ºç™½
   - æ˜¯å¦éœ€è¦è¡¥å……ç‰¹å®šæ–¹å‘çš„è®ºæ–‡

è¯·æŒ‰ä»¥ä¸‹æ ¼å¼è¿”å›è¯„ä¼°ç»“æœ:

**å……åˆ†æ€§è¯„åˆ†**: [0.0-1.0çš„åˆ†æ•°ï¼Œ1.0è¡¨ç¤ºå®Œå…¨å……åˆ†]

**è¯¦ç»†è¯„ä¼°æŠ¥å‘Š**:
[å…·ä½“åˆ†æå½“å‰èµ„æ–™çš„ä¼˜åŠ¿å’Œä¸è¶³]

**ç¼ºå¤±çš„ç ”ç©¶é¢†åŸŸ**:
1. [å…·ä½“çš„ç¼ºå¤±é¢†åŸŸ1]
2. [å…·ä½“çš„ç¼ºå¤±é¢†åŸŸ2]
3. [å…·ä½“çš„ç¼ºå¤±é¢†åŸŸ3]
[å¦‚æœèµ„æ–™å……åˆ†ï¼Œå¯ä»¥å›å¤"æ— æ˜æ˜¾ç¼ºå¤±"]

**è¿›ä¸€æ­¥æœç´¢å»ºè®®**:
[æ˜¯å¦å»ºè®®è¿›è¡Œè¿›ä¸€æ­¥æœç´¢ï¼Œä»¥åŠé‡ç‚¹æ–¹å‘]
"""
        
        response = self.ask(prompt, temperature=0.2, use_summary_api=True)
        
        # è§£æå“åº”
        adequacy_score = self._extract_adequacy_score(response)
        missing_areas = self._extract_missing_areas(response)
        
        print(f"ğŸ“Š å……åˆ†æ€§è¯„ä¼°å®Œæˆ - è¯„åˆ†: {adequacy_score:.2f}")
        
        return adequacy_score, response, missing_areas
    
    def _extract_adequacy_score(self, evaluation_text: str) -> float:
        """ä»è¯„ä¼°æ–‡æœ¬ä¸­æå–å……åˆ†æ€§è¯„åˆ†"""
        # å¯»æ‰¾è¯„åˆ†æ¨¡å¼
        score_patterns = [
            r'å……åˆ†æ€§è¯„åˆ†.*?([0-1]\.?\d*)',
            r'è¯„åˆ†.*?([0-1]\.?\d*)',
            r'åˆ†æ•°.*?([0-1]\.?\d*)',
        ]
        
        for pattern in score_patterns:
            match = re.search(pattern, evaluation_text, re.IGNORECASE)
            if match:
                try:
                    score = float(match.group(1))
                    return min(max(score, 0.0), 1.0)  # ç¡®ä¿åœ¨0-1èŒƒå›´å†…
                except:
                    continue
        
        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°æ˜ç¡®åˆ†æ•°ï¼ŒåŸºäºå…³é”®è¯ä¼°ç®—
        if any(keyword in evaluation_text for keyword in ['å……åˆ†', 'å®Œæ•´', 'å…¨é¢', 'adequate', 'sufficient', 'comprehensive']):
            return 0.8
        elif any(keyword in evaluation_text for keyword in ['ä¸è¶³', 'ç¼ºå¤±', 'éœ€è¦è¡¥å……', 'insufficient', 'lacking', 'need more']):
            return 0.4
        else:
            return 0.6  # é»˜è®¤ä¸­ç­‰è¯„åˆ†
    
    def _extract_missing_areas(self, evaluation_text: str) -> List[str]:
        """ä»è¯„ä¼°æ–‡æœ¬ä¸­æå–ç¼ºå¤±çš„ç ”ç©¶é¢†åŸŸ"""
        missing_areas = []
        
        # å¯»æ‰¾ç¼ºå¤±é¢†åŸŸéƒ¨åˆ†
        lines = evaluation_text.split('\n')
        in_missing_section = False
        
        for line in lines:
            line = line.strip()
            
            # æ£€æŸ¥æ˜¯å¦è¿›å…¥ç¼ºå¤±é¢†åŸŸéƒ¨åˆ†
            if any(keyword in line for keyword in ['ç¼ºå¤±', 'ç ”ç©¶é¢†åŸŸ', 'missing', 'areas', 'éœ€è¦è¡¥å……']):
                in_missing_section = True
                continue
            
            # å¦‚æœåœ¨ç¼ºå¤±é¢†åŸŸéƒ¨åˆ†
            if in_missing_section:
                # æ£€æŸ¥æ˜¯å¦ç»“æŸ
                if line.startswith('**') and 'å»ºè®®' in line:
                    break
                
                # æå–å…·ä½“é¢†åŸŸ
                if line and (line.startswith(('1.', '2.', '3.', '4.', '5.', '-', 'â€¢'))):
                    # æ¸…ç†å‰ç¼€
                    area = re.sub(r'^[\d\.\-â€¢\s]+', '', line).strip()
                    if area and area != "æ— æ˜æ˜¾ç¼ºå¤±":
                        missing_areas.append(area)
        
        return missing_areas[:MAX_NEW_KEYWORDS_PER_DEPTH]  # é™åˆ¶æ•°é‡
    
    def generate_depth_search_queries(self, research_topic: str, missing_areas: List[str], previous_queries: List[str], num_queries: int = 2) -> List[str]:
        """
        åŸºäºç¼ºå¤±é¢†åŸŸç”Ÿæˆæ·±åº¦æœç´¢æŸ¥è¯¢
        
        Args:
            research_topic: åŸå§‹ç ”ç©¶ä¸»é¢˜
            missing_areas: ç¼ºå¤±çš„ç ”ç©¶é¢†åŸŸ
            previous_queries: ä¹‹å‰ä½¿ç”¨çš„æŸ¥è¯¢
            num_queries: è¦ç”Ÿæˆçš„æŸ¥è¯¢æ•°é‡
            
        Returns:
            æ–°çš„æœç´¢æŸ¥è¯¢åˆ—è¡¨
        """
        
        print(f"ğŸ¯ æ­£åœ¨ç”Ÿæˆæ·±åº¦æœç´¢æŸ¥è¯¢...")
        
        missing_areas_text = "\n".join([f"- {area}" for area in missing_areas])
        previous_queries_text = "\n".join([f"- {query}" for query in previous_queries])
        
        prompt = f"""
åŸºäºç ”ç©¶èµ„æ–™å……åˆ†æ€§è¯„ä¼°ï¼Œéœ€è¦ä¸º"{research_topic}"ç”Ÿæˆ{num_queries}ä¸ªæ–°çš„æ·±åº¦æœç´¢æŸ¥è¯¢ã€‚

ç¼ºå¤±çš„ç ”ç©¶é¢†åŸŸ:
{missing_areas_text}

å·²ä½¿ç”¨çš„æŸ¥è¯¢ï¼ˆé¿å…é‡å¤ï¼‰:
{previous_queries_text}

è¦æ±‚:
1. ä¸“é—¨é’ˆå¯¹ç¼ºå¤±é¢†åŸŸè®¾è®¡æŸ¥è¯¢
2. é¿å…ä¸ä¹‹å‰æŸ¥è¯¢é‡å¤
3. åªè¿”å›çº¯è‹±æ–‡å…³é”®è¯ç»„åˆï¼Œæ¯è¡Œä¸€ä¸ª
4. ä¸è¦è§£é‡Šæˆ–æè¿°ï¼Œç›´æ¥ç»™å‡ºæŸ¥è¯¢
5. æ¯ä¸ªæŸ¥è¯¢2-5ä¸ªå…³é”®è¯
6. é‡ç‚¹å…³æ³¨å…·ä½“çš„æŠ€æœ¯æ–¹æ³•ã€åº”ç”¨é¢†åŸŸæˆ–ç†è®ºæ–¹å‘

ç¤ºä¾‹æ ¼å¼:
multimodal transformer architectures
few shot learning transformers
transformer optimization techniques

ç°åœ¨ç”Ÿæˆ{num_queries}ä¸ªæ·±åº¦æœç´¢æŸ¥è¯¢:
"""
        
        response = self.ask(prompt, temperature=0.3)
        
        # è§£ææŸ¥è¯¢
        new_queries = self._parse_and_clean_queries_improved(response, num_queries)
        
        # è¿‡æ»¤æ‰ä¸ä¹‹å‰æŸ¥è¯¢è¿‡äºç›¸ä¼¼çš„æŸ¥è¯¢
        filtered_queries = self._filter_similar_queries(new_queries, previous_queries)
        
        # å¦‚æœè¿‡æ»¤åæŸ¥è¯¢ä¸å¤Ÿï¼Œç”Ÿæˆè¡¥å……æŸ¥è¯¢
        if len(filtered_queries) < num_queries:
            supplement_queries = self._generate_supplement_queries(research_topic, missing_areas, filtered_queries + previous_queries)
            filtered_queries.extend(supplement_queries)
        
        final_queries = filtered_queries[:num_queries]
        print(f"ğŸ¯ ç”Ÿæˆçš„æ·±åº¦æœç´¢æŸ¥è¯¢: {final_queries}")
        
        return final_queries
    
    def _filter_similar_queries(self, new_queries: List[str], previous_queries: List[str]) -> List[str]:
        """è¿‡æ»¤æ‰ä¸ä¹‹å‰æŸ¥è¯¢è¿‡äºç›¸ä¼¼çš„æŸ¥è¯¢"""
        filtered = []
        
        for new_query in new_queries:
            is_similar = False
            new_words = set(new_query.lower().split())
            
            for prev_query in previous_queries:
                prev_words = set(prev_query.lower().split())
                
                # è®¡ç®—è¯æ±‡é‡å åº¦
                overlap = len(new_words.intersection(prev_words))
                similarity = overlap / max(len(new_words), len(prev_words), 1)
                
                if similarity > 0.6:  # å¦‚æœé‡å åº¦è¶…è¿‡60%ï¼Œè®¤ä¸ºå¤ªç›¸ä¼¼
                    is_similar = True
                    break
            
            if not is_similar:
                filtered.append(new_query)
        
        return filtered
    
    def _generate_supplement_queries(self, research_topic: str, missing_areas: List[str], existing_queries: List[str]) -> List[str]:
        """ç”Ÿæˆè¡¥å……æŸ¥è¯¢"""
        supplements = []
        
        # ä»ç ”ç©¶ä¸»é¢˜å’Œç¼ºå¤±é¢†åŸŸæå–å…³é”®è¯
        all_keywords = self._extract_keywords_from_topic(research_topic)
        for area in missing_areas:
            all_keywords.extend(self._extract_keywords_from_topic(area))
        
        # å»é‡å¹¶é€‰æ‹©é‡è¦å…³é”®è¯
        unique_keywords = list(set(all_keywords))[:10]
        
        # ç”Ÿæˆç®€å•çš„å…³é”®è¯ç»„åˆ
        if len(unique_keywords) >= 2:
            for i in range(0, len(unique_keywords) - 1, 2):
                if len(supplements) >= 3:  # æœ€å¤š3ä¸ªè¡¥å……æŸ¥è¯¢
                    break
                supplement = f"{unique_keywords[i]} {unique_keywords[i+1]}"
                if supplement not in existing_queries:
                    supplements.append(supplement)
        
        return supplements