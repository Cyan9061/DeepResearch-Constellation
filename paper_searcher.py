import requests
import arxiv
import re
import time
import json
import random
from typing import List, Dict, Optional, Tuple
from datetime import datetime, timedelta
from dataclasses import dataclass
from urllib.parse import urljoin, urlparse, quote
from bs4 import BeautifulSoup
from tqdm import tqdm
import warnings
import difflib
from fuzzywuzzy import fuzz, process
import xml.etree.ElementTree as ET
warnings.filterwarnings('ignore')

# å°è¯•å¯¼å…¥scholarlyåº“
try:
    from scholarly import scholarly, ProxyGenerator
    SCHOLARLY_AVAILABLE = True
except ImportError:
    SCHOLARLY_AVAILABLE = False
    print("âš ï¸ scholarlyåº“æœªå®‰è£…ï¼Œå°†è·³è¿‡scholarlyæœç´¢")

try:
    from config import PAPERS_PER_QUERY, DEPTH_SEARCH_QUERIES
except ImportError:
    PAPERS_PER_QUERY = 10
    DEPTH_SEARCH_QUERIES = 2

@dataclass
class SearchFilters:
    """è®ºæ–‡æœç´¢è¿‡æ»¤å™¨é…ç½®"""
    # æ—¶é—´è¿‡æ»¤
    start_date: Optional[datetime] = None
    end_date: Optional[datetime] = None
    
    # ä¼šè®®è¿‡æ»¤
    conferences: List[str] = None
    exclude_conferences: List[str] = None
    
    # å¼•ç”¨æ•°è¿‡æ»¤
    min_citations: int = 0
    max_citations: Optional[int] = None
    
    # å…¶ä»–è¿‡æ»¤
    categories: List[str] = None
    min_abstract_length: int = 10
    
    # æ¨¡ç³ŠåŒ¹é…å‚æ•°
    fuzzy_matching: bool = True
    similarity_threshold: int = 75  # æ¨¡ç³ŠåŒ¹é…ç›¸ä¼¼åº¦é˜ˆå€¼ (0-100)

class EnhancedMultiSourcePaperSearcher:
    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        })
        
        # åˆå§‹åŒ–scholarlyï¼ˆå¦‚æœå¯ç”¨ï¼‰
        self.scholarly_available = SCHOLARLY_AVAILABLE
        if self.scholarly_available:
            try:
                # è®¾ç½®ä»£ç†ï¼ˆå¯é€‰ï¼‰
                # pg = ProxyGenerator()
                # scholarly.use_proxy(pg)
                print("âœ… scholarlyåº“åˆå§‹åŒ–æˆåŠŸ")
            except Exception as e:
                print(f"âš ï¸ scholarlyåº“åˆå§‹åŒ–å¤±è´¥: {e}")
                self.scholarly_available = False
        
        # æ‰©å±•çš„è®¡ç®—æœºé¢†åŸŸä¼šè®®æ•°æ®åº“
        self.conference_categories = {
            'Machine Learning': ['ICML', 'NIPS', 'NeurIPS', 'ICLR', 'AISTATS', 'UAI', 'COLT', 'AAAI', 'IJCAI'],
            'Computer Vision': ['CVPR', 'ICCV', 'ECCV', 'BMVC', 'WACV', 'AAAI', 'IJCAI'],
            'Natural Language Processing': ['ACL', 'EMNLP', 'NAACL', 'COLING', 'EACL', 'AAAI', 'IJCAI'],
            'Data Mining': ['KDD', 'ICDM', 'SDM', 'WSDM', 'CIKM', 'WWW'],
            'Systems': ['OSDI', 'SOSP', 'NSDI', 'SIGCOMM', 'MOBICOM', 'INFOCOM'],
            'Security': ['CCS', 'S&P', 'USENIX Security', 'NDSS', 'OAKLAND'],
            'Theory': ['STOC', 'FOCS', 'SODA', 'ICALP', 'ITCS'],
            'HCI': ['CHI', 'UIST', 'CSCW', 'UBICOMP', 'IUI'],
            'Graphics': ['SIGGRAPH', 'SIGGRAPH Asia', 'EUROGRAPHICS', 'I3D'],
            'Databases': ['SIGMOD', 'VLDB', 'ICDE', 'PODS']
        }
        
        # ä¼šè®®æ˜ å°„ï¼ˆç”¨äºæ¨¡ç³ŠåŒ¹é…ï¼‰
        self.conference_mappings = {
            # Machine Learning
            'ICML': ['International Conference on Machine Learning', 'ICML', 'PMLR'],
            'NIPS': ['Neural Information Processing Systems', 'NeurIPS', 'NIPS', 'Advances in Neural Information Processing Systems'],
            'NeurIPS': ['Neural Information Processing Systems', 'NeurIPS', 'NIPS', 'Conference and Workshop on Neural Information Processing Systems', 'Advances in Neural Information Processing Systems'],
            'ICLR': ['International Conference on Learning Representations', 'ICLR'],
            'AISTATS': ['International Conference on Artificial Intelligence and Statistics', 'AISTATS', 'PMLR'],
            'UAI': ['Conference on Uncertainty in Artificial Intelligence', 'UAI'],
            'COLT': ['Conference on Learning Theory', 'COLT', 'Conference on Computational Learning Theory'],
            'AAAI': ['AAAI Conference on Artificial Intelligence', 'AAAI', 'Association for the Advancement of Artificial Intelligence'],
            'IJCAI': ['International Joint Conference on Artificial Intelligence', 'IJCAI'],
            
            # Computer Vision  
            'CVPR': ['IEEE Conference on Computer Vision and Pattern Recognition', 'CVPR', 'IEEE / CVF Computer Vision and Pattern Recognition Conference', 'Computer Vision and Pattern Recognition'],
            'ICCV': ['International Conference on Computer Vision', 'ICCV', 'IEEE International Conference on Computer Vision'],
            'ECCV': ['European Conference on Computer Vision', 'ECCV'],
            'BMVC': ['British Machine Vision Conference', 'BMVC'],
            'WACV': ['Winter Conference on Applications of Computer Vision', 'WACV', 'IEEE Winter Conference on Applications of Computer Vision'],
            
            # Natural Language Processing
            'ACL': ['Annual Meeting of the Association for Computational Linguistics', 'ACL', 'Association for Computational Linguistics', 'Proceedings of the ACL'],
            'EMNLP': ['Conference on Empirical Methods in Natural Language Processing', 'EMNLP', 'Empirical Methods in Natural Language Processing'],
            'NAACL': ['Conference of the North American Chapter of the Association for Computational Linguistics', 'NAACL', 'NAACL-HLT', 'North American Chapter of the Association for Computational Linguistics'],
            'COLING': ['International Conference on Computational Linguistics', 'COLING'],
            'EACL': ['Conference of the European Chapter of the Association for Computational Linguistics', 'EACL', 'European Chapter of the Association for Computational Linguistics'],
            'TACL': ['Transactions of the Association for Computational Linguistics', 'TACL'],
            
            # Data Mining
            'KDD': ['ACM SIGKDD Conference on Knowledge Discovery and Data Mining', 'KDD', 'Knowledge Discovery and Data Mining'],
            'ICDM': ['IEEE International Conference on Data Mining', 'ICDM', 'International Conference on Data Mining'],
            'SDM': ['SIAM International Conference on Data Mining', 'SDM'],
            'WSDM': ['ACM International Conference on Web Search and Data Mining', 'WSDM', 'Web Search and Data Mining'],
            'CIKM': ['ACM International Conference on Information and Knowledge Management', 'CIKM'],
            'WWW': ['International World Wide Web Conference', 'WWW', 'The Web Conference', 'World Wide Web Conference'],
            
            # Systems
            'OSDI': ['USENIX Symposium on Operating Systems Design and Implementation', 'OSDI', 'Operating Systems Design and Implementation'],
            'SOSP': ['ACM Symposium on Operating Systems Principles', 'SOSP', 'ACM SIGOPS Symposium on Operating Systems Principles', 'Symposium on Operating Systems Principles'],
            'NSDI': ['USENIX Symposium on Networked Systems Design and Implementation', 'NSDI', 'Networked Systems Design and Implementation'],
            'SIGCOMM': ['ACM SIGCOMM Conference', 'SIGCOMM', 'ACM Conference on Data Communication'],
            'MOBICOM': ['ACM International Conference on Mobile Computing and Networking', 'MOBICOM', 'MobiCom'],
            'INFOCOM': ['IEEE International Conference on Computer Communications', 'INFOCOM', 'IEEE INFOCOM'],
            
            # Security
            'CCS': ['ACM Conference on Computer and Communications Security', 'CCS', 'ACM CCS'],
            'S&P': ['IEEE Symposium on Security and Privacy', 'S&P', 'IEEE S&P', 'Oakland', 'IEEE Security and Privacy'],
            'USENIX Security': ['USENIX Security Symposium', 'USENIX Security', 'Security Symposium'],
            'NDSS': ['Network and Distributed System Security Symposium', 'NDSS'],
            'OAKLAND': ['IEEE Symposium on Security and Privacy', 'Oakland', 'S&P', 'IEEE S&P'],
            
            # Theory
            'STOC': ['ACM Symposium on Theory of Computing', 'STOC', 'Symposium on Theory of Computing'],
            'FOCS': ['IEEE Symposium on Foundations of Computer Science', 'FOCS', 'Foundations of Computer Science'],
            'SODA': ['ACM-SIAM Symposium on Discrete Algorithms', 'SODA', 'Symposium on Discrete Algorithms'],
            'ICALP': ['International Colloquium on Automata, Languages and Programming', 'ICALP'],
            'ITCS': ['Innovations in Theoretical Computer Science', 'ITCS', 'Innovation in Theoretical Computer Science'],
            
            # HCI
            'CHI': ['ACM Conference on Human Factors in Computing Systems', 'CHI', 'Human Factors in Computing Systems'],
            'UIST': ['ACM Symposium on User Interface Software and Technology', 'UIST', 'User Interface Software and Technology'],
            'CSCW': ['ACM Conference on Computer-Supported Cooperative Work and Social Computing', 'CSCW'],
            'UBICOMP': ['ACM International Joint Conference on Pervasive and Ubiquitous Computing', 'UbiComp', 'UBICOMP'],
            'IUI': ['ACM International Conference on Intelligent User Interfaces', 'IUI', 'Intelligent User Interfaces'],
            
            # Graphics
            'SIGGRAPH': ['ACM SIGGRAPH Conference', 'SIGGRAPH', 'Special Interest Group on Computer Graphics and Interactive Techniques'],
            'SIGGRAPH Asia': ['ACM SIGGRAPH Conference and Exhibition on Computer Graphics and Interactive Techniques in Asia', 'SIGGRAPH Asia'],
            'EUROGRAPHICS': ['Annual Conference of the European Association for Computer Graphics', 'Eurographics', 'EUROGRAPHICS'],
            'I3D': ['ACM SIGGRAPH Symposium on Interactive 3D Graphics and Games', 'I3D', 'Interactive 3D Graphics'],
            
            # Databases
            'SIGMOD': ['ACM SIGMOD Conference on Management of Data', 'SIGMOD', 'PACMMOD', 'Proceedings of the ACM on Management of Data', 'International Conference on Management of Data'],
            'VLDB': ['International Conference on Very Large Data Bases', 'VLDB', 'Very Large Data Bases', 'PVLDB', 'Proceedings of the VLDB Endowment'],
            'ICDE': ['IEEE International Conference on Data Engineering', 'ICDE', 'International Conference on Data Engineering'],
            'PODS': ['ACM SIGMOD-SIGACT-SIGAI Symposium on Principles of Database Systems', 'PODS', 'Principles of Database Systems'],
            
            # Additional important conferences and journals
            'JMLR': ['Journal of Machine Learning Research', 'JMLR'],
            'PMLR': ['Proceedings of Machine Learning Research', 'PMLR', 'JMLR Workshop and Conference Proceedings'],
            'CL': ['Computational Linguistics', 'CL'],
            'TODS': ['ACM Transactions on Database Systems', 'TODS'],
            'VLDBJ': ['VLDB Journal', 'VLDBJ', 'The VLDB Journal'],
            'TKDE': ['IEEE Transactions on Knowledge and Data Engineering', 'TKDE'],
            'TPAMI': ['IEEE Transactions on Pattern Analysis and Machine Intelligence', 'TPAMI'],
            'JAIR': ['Journal of Artificial Intelligence Research', 'JAIR'],
            'AIJ': ['Artificial Intelligence', 'AIJ', 'Artificial Intelligence Journal'],
            'TOCS': ['ACM Transactions on Computer Systems', 'TOCS'],
            'TON': ['IEEE/ACM Transactions on Networking', 'TON'],
        }
        
        # arXivç±»åˆ«æ˜ å°„
        self.category_mappings = {
            'machine_learning': ['cs.LG', 'cs.AI', 'stat.ML'],
            'computer_vision': ['cs.CV'],
            'nlp': ['cs.CL', 'cs.AI'],
            'systems': ['cs.DC', 'cs.OS', 'cs.NI'],
            'theory': ['cs.DS', 'cs.CC', 'cs.DM'],
            'security': ['cs.CR'],
            'databases': ['cs.DB'],
            'hci': ['cs.HC'],
            'graphics': ['cs.GR'],
            'robotics': ['cs.RO'],
            'all_cs': ['cs.*']
        }
        
        print(f"ğŸ”§ Enhanced Multi-Source Paper Searcher åˆå§‹åŒ–å®Œæˆ")
        print(f"   - ä¸»è¦æœç´¢æº: Google Scholar")
        print(f"   - è¡¥å……æœç´¢æº: {'scholarly, ' if self.scholarly_available else ''}DBLP, arXiv")
        print(f"   - æ¨¡ç³ŠåŒ¹é…: å¯ç”¨")
        print(f"   - æ”¯æŒ {len(self.conference_mappings)} ä¸ªä¸»è¦ä¼šè®®")
        print(f"   - æ”¯æŒ {len(self.conference_categories)} ä¸ªé¢†åŸŸåˆ†ç±»")
    
    def fuzzy_match_title(self, title1: str, title2: str, threshold: int = 75) -> bool:
        """æ¨¡ç³ŠåŒ¹é…ä¸¤ä¸ªæ ‡é¢˜"""
        if not title1 or not title2:
            return False
        
        # æ¸…ç†æ ‡é¢˜
        clean1 = self._clean_text_for_matching(title1)
        clean2 = self._clean_text_for_matching(title2)
        
        # ä½¿ç”¨å¤šç§åŒ¹é…ç®—æ³•
        ratio = fuzz.ratio(clean1, clean2)
        partial_ratio = fuzz.partial_ratio(clean1, clean2)
        token_sort_ratio = fuzz.token_sort_ratio(clean1, clean2)
        token_set_ratio = fuzz.token_set_ratio(clean1, clean2)
        
        # å–æœ€é«˜åˆ†
        max_similarity = max(ratio, partial_ratio, token_sort_ratio, token_set_ratio)
        
        return max_similarity >= threshold
    
    def _clean_text_for_matching(self, text: str) -> str:
        """æ¸…ç†æ–‡æœ¬ç”¨äºåŒ¹é…"""
        # è½¬å°å†™
        text = text.lower()
        # ç§»é™¤ç‰¹æ®Šå­—ç¬¦
        text = re.sub(r'[^\w\s]', ' ', text)
        # æ ‡å‡†åŒ–ç©ºæ ¼
        text = re.sub(r'\s+', ' ', text).strip()
        return text
    
    def fuzzy_match_conference(self, paper_text: str, target_conferences: List[str], threshold: int = 70) -> bool:
        """æ¨¡ç³ŠåŒ¹é…ä¼šè®®åç§°"""
        if not target_conferences:
            return True  # å¦‚æœæ²¡æœ‰æŒ‡å®šä¼šè®®ï¼Œåˆ™é€šè¿‡
        
        paper_text_clean = self._clean_text_for_matching(paper_text)
        
        for conf in target_conferences:
            # æ£€æŸ¥ä¼šè®®ç®€ç§°
            if fuzz.partial_ratio(conf.lower(), paper_text_clean) >= threshold:
                return True
            
            # æ£€æŸ¥ä¼šè®®å…¨ç§°
            if conf in self.conference_mappings:
                for conf_name in self.conference_mappings[conf]:
                    conf_name_clean = self._clean_text_for_matching(conf_name)
                    if fuzz.partial_ratio(conf_name_clean, paper_text_clean) >= threshold:
                        return True
        
        return False
    
    def search_google_scholar(self, query: str, max_results: int) -> List[Dict]:
        """åœ¨Google Scholarä¸­æœç´¢è®ºæ–‡ï¼ˆåŸæœ‰æ–¹æ³•ï¼‰"""
        print(f"ğŸ” åœ¨Google Scholarä¸­æœç´¢: {query}")
        
        papers = []
        start = 0
        
        while len(papers) < max_results:
            url = "https://scholar.google.com/scholar"
            params = {
                'q': query,
                'start': start,
                'hl': 'en'
            }
            
            try:
                # éšæœºå»¶è¿Ÿé˜²æ­¢è¢«å°
                time.sleep(random.uniform(2, 5))
                
                response = self.session.get(url, params=params, timeout=15)
                response.raise_for_status()
                
                soup = BeautifulSoup(response.content, 'html.parser')
                results = soup.find_all('div', {'class': 'gs_r gs_or gs_scl'})
                
                if not results:
                    print(f"  åœ¨ç¬¬{start//10 + 1}é¡µæœªæ‰¾åˆ°æ›´å¤šç»“æœ")
                    break
                
                for result in results:
                    if len(papers) >= max_results:
                        break
                    
                    paper_info = self._parse_scholar_result(result)
                    if paper_info:
                        papers.append(paper_info)
                
                start += 10
                
            except Exception as e:
                print(f"  âš ï¸ Google Scholaræœç´¢å‡ºé”™: {e}")
                break
        
        print(f"  âœ… Google Scholaræ‰¾åˆ° {len(papers)} ç¯‡è®ºæ–‡")
        return papers
    
    def search_scholarly_backup(self, query: str, max_results: int) -> List[Dict]:
        """ä½¿ç”¨scholarlyåº“ä½œä¸ºbackupæœç´¢"""
        if not self.scholarly_available:
            return []
        
        print(f"ğŸ” ä½¿ç”¨scholarlyåº“æœç´¢: {query}")
        
        papers = []
        try:
            search_query = scholarly.search_pubs(query)
            
            for i, pub in enumerate(search_query):
                if i >= max_results:
                    break
                
                try:
                    # è·å–è¯¦ç»†ä¿¡æ¯
                    pub_filled = scholarly.fill(pub)
                    
                    # è§£æå‘è¡¨å¹´ä»½
                    pub_year = None
                    if 'pub_year' in pub_filled and pub_filled['pub_year']:
                        try:
                            pub_year = int(pub_filled['pub_year'])
                        except:
                            pass
                    
                    # è§£æå¼•ç”¨æ•°
                    citations = 0
                    if 'num_citations' in pub_filled:
                        citations = pub_filled['num_citations'] or 0
                    
                    paper = {
                        'title': pub_filled.get('title', ''),
                        'authors': [author.get('name', '') for author in pub_filled.get('author', [])],
                        'abstract': pub_filled.get('abstract', ''),
                        'published': datetime(pub_year, 1, 1) if pub_year else None,
                        'published_str': str(pub_year) if pub_year else "Unknown",
                        'citations': citations,
                        'paper_url': pub_filled.get('pub_url', ''),
                        'pdf_url': pub_filled.get('eprint_url', ''),
                        'pdf_links': [pub_filled.get('eprint_url')] if pub_filled.get('eprint_url') else [],
                        'source': 'scholarly',
                        'venue': pub_filled.get('venue', ''),
                        'authors_text': ', '.join([author.get('name', '') for author in pub_filled.get('author', [])])
                    }
                    
                    papers.append(paper)
                    
                except Exception as e:
                    print(f"    âš ï¸ å¤„ç†scholarlyç»“æœå‡ºé”™: {e}")
                    continue
                
                # æ·»åŠ å»¶è¿Ÿé¿å…è¢«é™åˆ¶
                time.sleep(1)
            
            print(f"  âœ… scholarlyæ‰¾åˆ° {len(papers)} ç¯‡è®ºæ–‡")
            
        except Exception as e:
            print(f"  âŒ scholarlyæœç´¢å¤±è´¥: {e}")
        
        return papers
    
    def search_dblp_backup(self, query: str, max_results: int) -> List[Dict]:
        """ä½¿ç”¨DBLPä½œä¸ºbackupæœç´¢"""
        print(f"ğŸ” åœ¨DBLPä¸­æœç´¢: {query}")
        
        papers = []
        try:
            # DBLP APIæœç´¢
            dblp_url = "https://dblp.org/search/publ/api"
            params = {
                'q': query,
                'format': 'xml',
                'h': max_results
            }
            
            response = self.session.get(dblp_url, params=params, timeout=10)
            response.raise_for_status()
            
            # è§£æXMLå“åº”
            root = ET.fromstring(response.content)
            
            for hit in root.findall('.//hit'):
                try:
                    info = hit.find('info')
                    if info is None:
                        continue
                    
                    # æå–åŸºæœ¬ä¿¡æ¯
                    title = info.find('title')
                    title_text = title.text if title is not None else ''
                    
                    # æå–ä½œè€…
                    authors = []
                    for author in info.findall('authors/author'):
                        if author.text:
                            authors.append(author.text)
                    
                    # æå–å¹´ä»½
                    year_elem = info.find('year')
                    year = None
                    if year_elem is not None and year_elem.text:
                        try:
                            year = int(year_elem.text)
                        except:
                            pass
                    
                    # æå–ä¼šè®®/æœŸåˆŠä¿¡æ¯
                    venue = info.find('venue')
                    venue_text = venue.text if venue is not None else ''
                    
                    # æå–DOI/URL
                    doi = info.find('doi')
                    doi_text = doi.text if doi is not None else ''
                    paper_url = f"https://doi.org/{doi_text}" if doi_text else ''
                    
                    paper = {
                        'title': title_text,
                        'authors': authors,
                        'abstract': '',  # DBLPé€šå¸¸ä¸æä¾›æ‘˜è¦
                        'published': datetime(year, 1, 1) if year else None,
                        'published_str': str(year) if year else "Unknown",
                        'citations': 0,  # DBLPä¸æä¾›å¼•ç”¨æ•°
                        'paper_url': paper_url,
                        'pdf_url': None,
                        'pdf_links': [],
                        'source': 'dblp',
                        'venue': venue_text,
                        'doi': doi_text,
                        'authors_text': ', '.join(authors)
                    }
                    
                    papers.append(paper)
                    
                except Exception as e:
                    print(f"    âš ï¸ è§£æDBLPç»“æœå‡ºé”™: {e}")
                    continue
            
            print(f"  âœ… DBLPæ‰¾åˆ° {len(papers)} ç¯‡è®ºæ–‡")
            
        except Exception as e:
            print(f"  âŒ DBLPæœç´¢å¤±è´¥: {e}")
        
        return papers
    
    def search_arxiv_backup(self, query: str, max_results: int) -> List[Dict]:
        """åœ¨arXivä¸­æœç´¢è®ºæ–‡ä½œä¸ºå¤‡ç”¨ï¼ˆåŸæœ‰æ–¹æ³•ï¼Œç¨ä½œä¿®æ”¹ï¼‰"""
        print(f"ğŸ” åœ¨arXivä¸­æœç´¢: {query}")
        
        try:
            search = arxiv.Search(
                query=query,
                max_results=max_results,
                sort_by=arxiv.SortCriterion.Relevance
            )
            
            papers = []
            for result in tqdm(search.results(), desc="è·å–arXivè®ºæ–‡"):
                paper = {
                    'title': result.title,
                    'authors': [author.name for author in result.authors],
                    'abstract': result.summary,
                    'published': result.published,
                    'published_str': result.published.strftime('%Y-%m-%d'),
                    'pdf_url': result.pdf_url,
                    'pdf_links': [result.pdf_url],
                    'arxiv_id': result.get_short_id(),
                    'categories': result.categories,
                    'primary_category': result.primary_category,
                    'citations': 0,  # arXivé€šå¸¸æ²¡æœ‰å¼•ç”¨æ•°
                    'source': 'arxiv',
                    'paper_url': result.entry_id,
                    'authors_text': ', '.join([author.name for author in result.authors])
                }
                papers.append(paper)
            
            print(f"  âœ… arXivæ‰¾åˆ° {len(papers)} ç¯‡è®ºæ–‡")
            return papers
            
        except Exception as e:
            print(f"  âŒ arXivæœç´¢å¤±è´¥: {e}")
            return []
    
    def search_papers_multi_source(self, query: str, filters: SearchFilters) -> List[Dict]:
        """å¤šæºæœç´¢è®ºæ–‡ï¼Œå¢å¼ºç‰ˆ"""
        print(f"ğŸ” å¼€å§‹å¤šæºæœç´¢: {query}")
        
        # éªŒè¯æŸ¥è¯¢
        if not query or query.strip() == "" or query.strip() == "--":
            print("âš ï¸ æŸ¥è¯¢ä¸ºç©ºæˆ–æ— æ•ˆï¼Œè·³è¿‡...")
            return []
        
        all_papers = []
        
        try:
            # ç¬¬ä¸€çº§ï¼šGoogle Scholaræœç´¢
            print(f"ğŸ“Š ç¬¬ä¸€çº§æœç´¢ - Google Scholar...")
            scholar_papers = self.search_google_scholar(query, PAPERS_PER_QUERY)
            all_papers.extend(scholar_papers)
            
            # æ£€æŸ¥æ˜¯å¦éœ€è¦è¡¥å……æœç´¢
            if len(scholar_papers) < PAPERS_PER_QUERY:
                remaining_needed = PAPERS_PER_QUERY - len(scholar_papers)
                
                # ç¬¬äºŒçº§ï¼šscholarlyå¤‡ç”¨æœç´¢
                if self.scholarly_available:
                    print(f"ğŸ“Š ç¬¬äºŒçº§æœç´¢ - scholarlyåº“...")
                    scholarly_papers = self.search_scholarly_backup(query, remaining_needed)
                    all_papers.extend(scholarly_papers)
                    remaining_needed -= len(scholarly_papers)
                
                # ç¬¬ä¸‰çº§ï¼šDBLPå¤‡ç”¨æœç´¢
                if remaining_needed > 0:
                    print(f"ğŸ“Š ç¬¬ä¸‰çº§æœç´¢ - DBLP...")
                    dblp_papers = self.search_dblp_backup(query, remaining_needed)
                    all_papers.extend(dblp_papers)
                    remaining_needed -= len(dblp_papers)
                
                # ç¬¬å››çº§ï¼šarXivæœ€ç»ˆå¤‡ç”¨
                if remaining_needed > 0:
                    print(f"ğŸ“Š ç¬¬å››çº§æœç´¢ - arXiv...")
                    arxiv_papers = self.search_arxiv_backup(query, remaining_needed)
                    all_papers.extend(arxiv_papers)
            
            # åº”ç”¨å¢å¼ºè¿‡æ»¤å™¨ï¼ˆåŒ…å«æ¨¡ç³ŠåŒ¹é…ï¼‰
            filtered_papers = []
            for paper in all_papers:
                if self._apply_enhanced_filters(paper, filters):
                    validated_paper = self._validate_paper_data(paper)
                    filtered_papers.append(validated_paper)
            
            print(f"âœ… å¤šæºæœç´¢å®Œæˆï¼Œè¿‡æ»¤åå‰©ä½™ {len(filtered_papers)} ç¯‡è®ºæ–‡")
            return filtered_papers
            
        except Exception as e:
            print(f"âŒ å¤šæºæœç´¢å¤±è´¥ '{query}': {e}")
            return []
    
    def _apply_enhanced_filters(self, paper: Dict, filters: SearchFilters) -> bool:
        """åº”ç”¨å¢å¼ºè¿‡æ»¤æ¡ä»¶ï¼ˆåŒ…å«æ¨¡ç³ŠåŒ¹é…ï¼‰"""
        
        # æ—¶é—´è¿‡æ»¤
        if paper.get('published'):
            if filters.start_date and paper['published'] < filters.start_date:
                return False
            if filters.end_date and paper['published'] > filters.end_date:
                return False
        
        # å¼•ç”¨æ•°è¿‡æ»¤
        citations = paper.get('citations', 0)
        if citations < filters.min_citations:
            return False
        if filters.max_citations is not None and citations > filters.max_citations:
            return False
        
        # ä¼šè®®è¿‡æ»¤ï¼ˆä½¿ç”¨æ¨¡ç³ŠåŒ¹é…ï¼‰
        if filters.conferences:
            search_text = (paper.get('title', '') + ' ' + 
                          paper.get('authors_text', '') + ' ' + 
                          paper.get('venue', '') + ' ' +
                          ' '.join(paper.get('authors', []))).lower()
            
            if filters.fuzzy_matching:
                # ä½¿ç”¨æ¨¡ç³ŠåŒ¹é…
                if not self.fuzzy_match_conference(search_text, filters.conferences, 
                                                 threshold=filters.similarity_threshold):
                    return False
            else:
                # ä½¿ç”¨ç²¾ç¡®åŒ¹é…
                found_conference = False
                for conf in filters.conferences:
                    if conf in self.conference_mappings:
                        conf_names = [name.lower() for name in self.conference_mappings[conf]]
                        if any(name in search_text for name in conf_names):
                            found_conference = True
                            break
                    if conf.lower() in search_text:
                        found_conference = True
                        break
                
                if not found_conference:
                    return False
        
        # æ’é™¤ä¼šè®®ï¼ˆä¹Ÿä½¿ç”¨æ¨¡ç³ŠåŒ¹é…ï¼‰
        if filters.exclude_conferences:
            search_text = (paper.get('title', '') + ' ' + 
                          paper.get('authors_text', '') + ' ' + 
                          paper.get('venue', '') + ' ' +
                          ' '.join(paper.get('authors', []))).lower()
            
            for conf in filters.exclude_conferences:
                if filters.fuzzy_matching:
                    if self.fuzzy_match_conference(search_text, [conf], 
                                                 threshold=filters.similarity_threshold):
                        return False
                else:
                    if conf in self.conference_mappings:
                        conf_names = [name.lower() for name in self.conference_mappings[conf]]
                        if any(name in search_text for name in conf_names):
                            return False
                    if conf.lower() in search_text:
                        return False
        
        # arXivç±»åˆ«è¿‡æ»¤ï¼ˆä»…å¯¹arXivè®ºæ–‡æœ‰æ•ˆï¼‰
        if filters.categories and paper.get('source') == 'arxiv':
            paper_categories = paper.get('categories', [])
            category_match = False
            for filter_cat in filters.categories:
                if filter_cat.endswith('.*'):
                    prefix = filter_cat[:-2]
                    if any(cat.startswith(prefix) for cat in paper_categories):
                        category_match = True
                        break
                else:
                    if filter_cat in paper_categories:
                        category_match = True
                        break
            
            if not category_match:
                return False
        
        # æ‘˜è¦é•¿åº¦è¿‡æ»¤
        if len(paper.get('abstract', '')) < filters.min_abstract_length:
            return False
        
        return True
    
    def _validate_paper_data(self, paper: Dict) -> Dict:
        """éªŒè¯å¹¶æ ‡å‡†åŒ–è®ºæ–‡æ•°æ®"""
        validated_paper = paper.copy()
        
        # ç¡®ä¿å¿…éœ€å­—æ®µå­˜åœ¨
        required_fields = {
            'title': 'Unknown Title',
            'authors': [],
            'abstract': '',
            'citations': 0,
            'source': 'unknown',
            'published_str': 'Unknown'
        }
        
        for field, default_value in required_fields.items():
            if field not in validated_paper:
                validated_paper[field] = default_value
        
        # ç¡®ä¿pdf_urlå­—æ®µå­˜åœ¨
        if 'pdf_url' not in validated_paper or not validated_paper['pdf_url']:
            pdf_links = validated_paper.get('pdf_links', [])
            if pdf_links:
                validated_paper['pdf_url'] = pdf_links[0]
            else:
                validated_paper['pdf_url'] = None
        
        # ç¡®ä¿authorsæ˜¯åˆ—è¡¨æ ¼å¼
        if isinstance(validated_paper['authors'], str):
            validated_paper['authors'] = [validated_paper['authors']]
        elif not validated_paper['authors']:
            validated_paper['authors'] = ['Unknown Author']
        
        # ç¡®ä¿citationsæ˜¯æ•°å­—
        try:
            validated_paper['citations'] = int(validated_paper.get('citations', 0))
        except (ValueError, TypeError):
            validated_paper['citations'] = 0
        
        return validated_paper
    
    def search_multiple_queries_enhanced(self, queries: List[str], filters: SearchFilters) -> List[Dict]:
        """ä½¿ç”¨å¢å¼ºè¿‡æ»¤å™¨å’Œå¤šæºæœç´¢"""
        all_papers = []
        max_tries = 3
        
        while len(all_papers) < 1 and max_tries > 0:
            max_tries -= 1
            print(f"ğŸ”„ å¼€å§‹æœç´¢è½®æ¬¡ {3 - max_tries}")
            
            for i, query in enumerate(queries):
                if query and query.strip():
                    print(f"\nğŸ“ æ‰§è¡ŒæŸ¥è¯¢ {i+1}/{len(queries)}: {query}")
                    papers = self.search_papers_multi_source(query, filters)
                    all_papers.extend(papers)
                    
                    print(f"  æœ¬æ¬¡æŸ¥è¯¢æ‰¾åˆ°: {len(papers)} ç¯‡è®ºæ–‡")
                    print(f"  ç´¯è®¡æ‰¾åˆ°: {len(all_papers)} ç¯‡è®ºæ–‡")
                    
                    # æŸ¥è¯¢é—´å»¶è¿Ÿ
                    time.sleep(2)
            
            time.sleep(3)  # è½®æ¬¡é—´å»¶è¿Ÿ
        
        # å¢å¼ºå»é‡ï¼ˆä½¿ç”¨æ¨¡ç³ŠåŒ¹é…ï¼‰
        unique_papers = self._deduplicate_papers_enhanced(all_papers, filters)
        
        # éªŒè¯æ•°æ®æ ¼å¼
        validated_papers = []
        for paper in unique_papers:
            validated_paper = self._validate_paper_data(paper)
            validated_papers.append(validated_paper)
        
        # æŒ‰å¤šä¸ªç»´åº¦æ’åº
        sorted_papers = self._sort_papers_by_relevance(validated_papers, filters)
        
        print(f"ğŸ‰ å¤šæºæœç´¢å®Œæˆï¼")
        print(f"  æ€»è®¡æ‰¾åˆ°: {len(all_papers)} ç¯‡è®ºæ–‡")
        print(f"  å»é‡åå‰©ä½™: {len(unique_papers)} ç¯‡è®ºæ–‡")
        print(f"  æœ€ç»ˆè¿”å›: {len(sorted_papers)} ç¯‡è®ºæ–‡")
        
        # æ˜¾ç¤ºæ¥æºç»Ÿè®¡
        source_stats = {}
        for paper in sorted_papers:
            source = paper.get('source', 'unknown')
            source_stats[source] = source_stats.get(source, 0) + 1
        
        if source_stats:
            print(f"ğŸ“Š æ¥æºåˆ†å¸ƒ: {', '.join([f'{k}:{v}' for k, v in source_stats.items()])}")
        
        return sorted_papers
    
    def _deduplicate_papers_enhanced(self, papers: List[Dict], filters: SearchFilters) -> List[Dict]:
        """å¢å¼ºçš„å»é‡ç®—æ³•ï¼ˆä½¿ç”¨æ¨¡ç³ŠåŒ¹é…ï¼‰"""
        if not papers:
            return []
        
        unique_papers = []
        seen_titles = set()
        seen_arxiv_ids = set()
        
        for paper in papers:
            arxiv_id = paper.get('arxiv_id', '')
            title = paper.get('title', '')
            
            # æ£€æŸ¥arXiv IDé‡å¤
            if arxiv_id and arxiv_id in seen_arxiv_ids:
                continue
            
            # æ£€æŸ¥æ ‡é¢˜é‡å¤ï¼ˆæ¨¡ç³ŠåŒ¹é…ï¼‰
            is_duplicate = False
            if title and filters.fuzzy_matching:
                for seen_title in seen_titles:
                    if self.fuzzy_match_title(title, seen_title, 
                                            threshold=filters.similarity_threshold + 10):  # å»é‡æ—¶ä½¿ç”¨æ›´é«˜é˜ˆå€¼
                        is_duplicate = True
                        break
            else:
                # ç²¾ç¡®åŒ¹é…
                title_normalized = self._normalize_title(title)
                if title_normalized in seen_titles:
                    is_duplicate = True
            
            if not is_duplicate:
                unique_papers.append(paper)
                if title:
                    seen_titles.add(title)
                if arxiv_id:
                    seen_arxiv_ids.add(arxiv_id)
        
        return unique_papers
    
    def _normalize_title(self, title: str) -> str:
        """æ ‡å‡†åŒ–æ ‡é¢˜ç”¨äºå»é‡"""
        normalized = re.sub(r'[^\w\s]', ' ', title.lower())
        normalized = re.sub(r'\s+', ' ', normalized).strip()
        return normalized
    
    def _sort_papers_by_relevance(self, papers: List[Dict], filters: SearchFilters) -> List[Dict]:
        """æŒ‰ç›¸å…³æ€§å¯¹è®ºæ–‡æ’åº"""
        def relevance_score(paper):
            score = 0
            
            # å¼•ç”¨æ•°æƒé‡
            citations = paper.get('citations', 0)
            score += min(citations / 100, 10)
            
            # æ—¶é—´æƒé‡
            pub_date = paper.get('published')
            if pub_date:
                if hasattr(pub_date, 'replace'):
                    pub_date = pub_date.replace(tzinfo=None)
                days_old = (datetime.now() - pub_date).days
                time_score = max(0, 5 - (days_old / 365))
                score += time_score
            
            # æ¥æºæƒé‡ï¼ˆGoogle Scholar > scholarly > DBLP > arXivï¼‰
            source_weights = {
                'google_scholar': 4,
                'scholarly': 3,
                'dblp': 2,
                'arxiv': 1
            }
            score += source_weights.get(paper.get('source', 'unknown'), 0)
            
            # ä¼šè®®åŒ¹é…æƒé‡
            if filters.conferences:
                search_text = (paper.get('title', '') + ' ' + 
                              paper.get('authors_text', '') + ' ' +
                              paper.get('venue', '') + ' ' +
                              ' '.join(paper.get('authors', []))).lower()
                
                if filters.fuzzy_matching:
                    if self.fuzzy_match_conference(search_text, filters.conferences, 
                                                 threshold=filters.similarity_threshold):
                        score += 5
                else:
                    for conf in filters.conferences:
                        if conf.lower() in search_text:
                            score += 5
                            break
            
            return score
        
        return sorted(papers, key=relevance_score, reverse=True)
    
    def _parse_scholar_result(self, result) -> Optional[Dict]:
        """è§£æGoogle Scholaræœç´¢ç»“æœï¼ˆåŸæœ‰æ–¹æ³•ï¼‰"""
        try:
            # è·å–æ ‡é¢˜
            title_elem = result.find('h3', {'class': 'gs_rt'})
            if not title_elem:
                return None
            
            title_link = title_elem.find('a')
            title = title_link.get_text() if title_link else title_elem.get_text()
            paper_url = title_link.get('href') if title_link else None
            
            # è·å–ä½œè€…å’Œæ¥æºä¿¡æ¯
            authors_elem = result.find('div', {'class': 'gs_a'})
            authors_text = authors_elem.get_text() if authors_elem else ""
            
            # å°è¯•è§£æå¹´ä»½
            published_year = None
            year_match = re.search(r'\b(19|20)\d{2}\b', authors_text)
            if year_match:
                published_year = int(year_match.group())
            
            # è·å–æ‘˜è¦
            abstract_elem = result.find('div', {'class': 'gs_rs'})
            abstract = abstract_elem.get_text() if abstract_elem else ""
            
            # è·å–å¼•ç”¨æ•°
            citations = 0
            cited_elem = result.find('div', {'class': 'gs_fl'})
            if cited_elem:
                cited_links = cited_elem.find_all('a')
                for link in cited_links:
                    link_text = link.get_text()
                    if 'Cited by' in link_text:
                        try:
                            citations = int(re.search(r'Cited by (\d+)', link_text).group(1))
                        except:
                            citations = 0
                        break
            
            # æŸ¥æ‰¾PDFé“¾æ¥
            pdf_links = []
            pdf_elem = result.find('div', {'class': 'gs_or_ggsm'})
            if pdf_elem:
                pdf_link = pdf_elem.find('a')
                if pdf_link and pdf_link.get('href'):
                    pdf_links.append(pdf_link.get('href'))
            
            # æŸ¥æ‰¾å…¶ä»–æ ¼å¼é“¾æ¥
            all_links = result.find_all('a')
            for link in all_links:
                href = link.get('href')
                if href and ('.pdf' in href.lower() or 'arxiv.org' in href):
                    pdf_links.append(href)
            
            main_pdf_url = pdf_links[0] if pdf_links else None
            
            return {
                'title': title.strip(),
                'authors': [author.strip() for author in authors_text.split(',')[:5]],
                'abstract': abstract.strip(),
                'published': datetime(published_year, 1, 1) if published_year else None,
                'published_str': str(published_year) if published_year else "Unknown",
                'citations': citations,
                'paper_url': paper_url,
                'pdf_url': main_pdf_url,
                'pdf_links': list(set(pdf_links)),
                'source': 'google_scholar',
                'authors_text': authors_text
            }
        
        except Exception as e:
            return None
    
    def display_search_results(self, papers: List[Dict], max_display: int = 10):
        """æ˜¾ç¤ºæœç´¢ç»“æœæ‘˜è¦"""
        if not papers:
            print("ğŸ“­ æ²¡æœ‰æ‰¾åˆ°ç¬¦åˆæ¡ä»¶çš„è®ºæ–‡")
            return
        
        print(f"\nğŸ“‹ å¤šæºæœç´¢ç»“æœ (æ˜¾ç¤ºå‰{min(len(papers), max_display)}ç¯‡):")
        print("=" * 80)
        
        for i, paper in enumerate(papers[:max_display], 1):
            print(f"\n{i}. {paper['title']}")
            
            if isinstance(paper.get('authors'), list):
                authors_display = ', '.join(paper['authors'][:3])
                if len(paper['authors']) > 3:
                    authors_display += '...'
            else:
                authors_display = str(paper.get('authors', 'Unknown'))[:100]
            
            print(f"   ä½œè€…: {authors_display}")
            print(f"   å‘è¡¨æ—¥æœŸ: {paper.get('published_str', 'Unknown')}")
            print(f"   æ¥æº: {paper.get('source', 'unknown')}")
            print(f"   å¼•ç”¨æ•°: {paper.get('citations', 0)}")
            
            if paper.get('venue'):
                print(f"   ä¼šè®®/æœŸåˆŠ: {paper['venue']}")
            
            if paper.get('arxiv_id'):
                print(f"   arXiv ID: {paper['arxiv_id']}")
            
            if paper.get('doi'):
                print(f"   DOI: {paper['doi']}")
            
            # æ˜¾ç¤ºæ‘˜è¦å‰150å­—ç¬¦
            abstract = paper.get('abstract', '')
            abstract_preview = abstract[:150] + "..." if len(abstract) > 150 else abstract
            print(f"   æ‘˜è¦: {abstract_preview}")
            
            if paper.get('pdf_url'):
                print(f"   PDF: {paper['pdf_url']}")
            elif paper.get('pdf_links'):
                print(f"   PDFé“¾æ¥: {len(paper['pdf_links'])} ä¸ªå¯ç”¨é“¾æ¥")
        
        if len(papers) > max_display:
            print(f"\n... è¿˜æœ‰ {len(papers) - max_display} ç¯‡è®ºæ–‡æœªæ˜¾ç¤º")

    def get_user_search_preferences(self) -> SearchFilters:
        """è·å–ç”¨æˆ·çš„æœç´¢åå¥½è®¾ç½®ï¼ˆå¢å¼ºç‰ˆï¼‰"""
        print("\n=== å¢å¼ºå‹å¤šæºæœç´¢é…ç½® ===")
        
        # æ¨¡ç³ŠåŒ¹é…è®¾ç½®
        print(f"\nğŸ¯ æ¨¡ç³ŠåŒ¹é…è®¾ç½®:")
        fuzzy_choice = True#input("æ˜¯å¦å¯ç”¨æ¨¡ç³ŠåŒ¹é…? (Y/n): ").strip().lower()
        fuzzy_matching = fuzzy_choice != 'n'
        
        similarity_threshold = 75  # é»˜è®¤é˜ˆå€¼
        if fuzzy_matching:
            threshold_input = 75#input("æ¨¡ç³ŠåŒ¹é…ç›¸ä¼¼åº¦é˜ˆå€¼ (50-100, é»˜è®¤75): ").strip()
            if threshold_input.isdigit():
                threshold = int(threshold_input)
                if 50 <= threshold <= 100:
                    similarity_threshold = threshold
                    print(f"âœ… ç›¸ä¼¼åº¦é˜ˆå€¼è®¾ç½®ä¸º: {similarity_threshold}")
                else:
                    print("âš ï¸ é˜ˆå€¼è¶…å‡ºèŒƒå›´ï¼Œä½¿ç”¨é»˜è®¤å€¼75")
            print(f"âœ… æ¨¡ç³ŠåŒ¹é…å·²å¯ç”¨ï¼Œé˜ˆå€¼: {similarity_threshold}")
        else:
            print("âœ… ä½¿ç”¨ç²¾ç¡®åŒ¹é…")
        
        # æ˜¾ç¤ºå¯ç”¨ä¼šè®®
        all_conferences = []
        for conferences in self.conference_categories.values():
            all_conferences.extend(conferences)
        all_conferences = list(set(all_conferences))
        
        print(f"\nğŸ“‹ æ”¯æŒçš„ä¼šè®® (æ”¯æŒæ¨¡ç³ŠåŒ¹é…):")
        for category, conferences in self.conference_categories.items():
            print(f"  {category}: {', '.join(conferences[:3])}{'...' if len(conferences) > 100 else ''}")
        
        # ä¼šè®®ç­›é€‰
        selected_input = input("\nè¯·é€‰æ‹©ä¼šè®®æˆ–é¢†åŸŸ (ç”¨é€—å·åˆ†éš”ï¼Œç•™ç©ºè¡¨ç¤ºä¸é™åˆ¶): ").strip()
        selected_conferences = []
        
        if selected_input:
            selections = [item.strip() for item in selected_input.split(',')]
            
            for selection in selections:
                if selection in self.conference_categories:
                    category_conferences = self.conference_categories[selection]
                    selected_conferences.extend(category_conferences)
                    print(f"âœ… å·²é€‰æ‹© {selection} é¢†åŸŸçš„æ‰€æœ‰ä¼šè®®")
                elif selection.upper() in [conf.upper() for conf in all_conferences]:
                    selected_conferences.append(selection.upper())
                    print(f"âœ… å·²é€‰æ‹©ä¼šè®®: {selection.upper()}")
                else:
                    print(f"âš ï¸ æœªæ‰¾åˆ°ä¼šè®®æˆ–é¢†åŸŸ: {selection}")
            
            selected_conferences = list(set(selected_conferences))
            if selected_conferences:
                print(f"ğŸ¯ æœ€ç»ˆé€‰æ‹©çš„ä¼šè®®: {', '.join(selected_conferences)}")
        
        # æ—¶é—´èŒƒå›´
        print(f"\nğŸ“… å‘è¡¨æ—¶é—´ç­›é€‰:")
        time_preset = input("ä½¿ç”¨é¢„è®¾æ—¶é—´èŒƒå›´ï¼Ÿ(1=æœ€è¿‘1å¹´, 2=æœ€è¿‘2å¹´, 3=æœ€è¿‘5å¹´, ç•™ç©º=ä¸é™åˆ¶): ").strip()
        start_date = None
        
        if time_preset == "1":
            start_date = datetime.now() - timedelta(days=365)
            print(f"âœ… ä½¿ç”¨æœ€è¿‘1å¹´")
        elif time_preset == "2":
            start_date = datetime.now() - timedelta(days=730)
            print(f"âœ… ä½¿ç”¨æœ€è¿‘2å¹´")
        elif time_preset == "3":
            start_date = datetime.now() - timedelta(days=1825)
            print(f"âœ… ä½¿ç”¨æœ€è¿‘5å¹´")
        else:
            print(f"âœ… ä¸é™åˆ¶æ—¶é—´èŒƒå›´")
        
        # å¼•ç”¨æ•°ç­›é€‰
        print(f"\nğŸ“Š å¼•ç”¨æ•°ç­›é€‰:")
        min_citations_str = input("æœ€å°å¼•ç”¨æ•° (ç•™ç©ºè¡¨ç¤ºä¸é™åˆ¶): ").strip()
        min_citations = 0
        if min_citations_str.isdigit():
            min_citations = int(min_citations_str)
            print(f"âœ… æœ€å°å¼•ç”¨æ•°: {min_citations}")
        
        return SearchFilters(
            start_date=start_date,
            end_date=None,
            conferences=selected_conferences if selected_conferences else None,
            exclude_conferences=None,
            min_citations=min_citations,
            max_citations=None,
            categories=None,
            min_abstract_length=10,
            fuzzy_matching=fuzzy_matching,
            similarity_threshold=similarity_threshold
        )

# å…¼å®¹æ€§ç±»
class EnhancedPaperSearcher(EnhancedMultiSourcePaperSearcher):
    """ä¸ºäº†å‘åå…¼å®¹ï¼Œä¿æŒåŸæœ‰çš„ç±»å"""
    pass

class PaperSearcher(EnhancedMultiSourcePaperSearcher):
    """ä¸ºäº†å…¼å®¹æ€§ï¼Œæä¾›åŸæœ‰çš„æ¥å£"""
    
    def search_papers(self, query: str, max_results: int = 10) -> List[Dict]:
        """å…¼å®¹åŸæœ‰çš„search_papersæ¥å£"""
        filters = SearchFilters(fuzzy_matching=True)
        papers = self.search_papers_multi_source(query, filters)
        return [self._validate_paper_data(paper) for paper in papers]
    
    def search_multiple_queries(self, queries: List[str], max_per_query: int = 5) -> List[Dict]:
        """å…¼å®¹åŸæœ‰çš„search_multiple_queriesæ¥å£"""
        filters = SearchFilters(fuzzy_matching=True)
        global PAPERS_PER_QUERY
        original_papers_per_query = PAPERS_PER_QUERY
        PAPERS_PER_QUERY = max_per_query
        
        try:
            papers = self.search_multiple_queries_enhanced(queries, filters)
            return [self._validate_paper_data(paper) for paper in papers]
        finally:
            PAPERS_PER_QUERY = original_papers_per_query

def demo_multi_source_search():
    """æ¼”ç¤ºå¤šæºæœç´¢åŠŸèƒ½"""
    print("ğŸš€ å¢å¼ºå¤šæºè®ºæ–‡æœç´¢å™¨æ¼”ç¤º")
    
    # åˆå§‹åŒ–æœç´¢å™¨
    searcher = EnhancedMultiSourcePaperSearcher()
    
    # è·å–ç”¨æˆ·æœç´¢åå¥½
    filters = searcher.get_user_search_preferences()
    
    # è·å–æœç´¢æŸ¥è¯¢
    print("\nğŸ” è¯·è¾“å…¥æœç´¢æŸ¥è¯¢:")
    base_query = input("æœç´¢å…³é”®è¯: ").strip()
    
    if not base_query:
        base_query = "transformer attention mechanism"
        print(f"ä½¿ç”¨é»˜è®¤æŸ¥è¯¢: {base_query}")
    
    # æ‰§è¡Œæœç´¢
    queries = [base_query]
    results = searcher.search_multiple_queries_enhanced(queries, filters)
    
    # æ˜¾ç¤ºç»“æœ
    searcher.display_search_results(results)
    
    return results

if __name__ == "__main__":
    # è¿è¡Œæ¼”ç¤º
    results = demo_multi_source_search()