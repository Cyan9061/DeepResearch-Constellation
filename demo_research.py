#!/usr/bin/env python3
"""
æ·±ç ”æ˜Ÿå›¾ä¿®å¤ç‰ˆå¿«é€Ÿæ¼”ç¤ºè„šæœ¬ - ä½¿ç”¨å¤šæºæœç´¢å’Œæ¨¡ç³ŠåŒ¹é…
Research Constellation Fixed Quick Demo Script - Multi-source Search with Fuzzy Matching

ä¿®å¤è¿‡æ»¤é—®é¢˜ï¼Œä½¿ç”¨å¢å¼ºçš„å¤šæºæœç´¢å™¨
Fixed filtering issues using enhanced multi-source searcher
"""

import os
import sys
from pathlib import Path

# æ·»åŠ å½“å‰ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.append(str(Path(__file__).parent))

from deepseek_client import DeepSeekClient
from paper_searcher import EnhancedMultiSourcePaperSearcher, SearchFilters
from pdf_processor import EnhancedPDFProcessor
from datetime import datetime, timedelta

# æ¼”ç¤ºé…ç½®
DEMO_CONFIG = {
    'search_depth': 2,           # æœç´¢è½®æ•°ï¼š2è½®
    'num_search_queries': 1,     # åˆå§‹æŸ¥è¯¢æ•°ï¼š1ä¸ª
    'papers_per_query': 8,       # æ¯æŸ¥è¯¢è®ºæ–‡æ•°ï¼š8ç¯‡ (å¢åŠ ä»¥æé«˜æˆåŠŸç‡)
    'depth_search_queries': 1,   # æ·±åº¦æœç´¢æŸ¥è¯¢æ•°ï¼š1ä¸ª  
    'max_papers_per_depth': 6,   # æ¯è½®æœ€å¤§å¤„ç†è®ºæ–‡æ•°ï¼š6ç¯‡
    'adequacy_threshold': 0.6,   # å……åˆ†æ€§é˜ˆå€¼ï¼š0.6 (é™ä½ä»¥ä¾¿æ¼”ç¤º)
    'enable_pdf_download': False, # å…³é—­PDFä¸‹è½½ä»¥åŠ å¿«æ¼”ç¤ºé€Ÿåº¦
    'demo_research_topic': "transformer attention mechanisms",  # æ¼”ç¤ºä¸»é¢˜
    'enable_fuzzy_matching': True,    # å¯ç”¨æ¨¡ç³ŠåŒ¹é…
    'similarity_threshold': 70        # æ¨¡ç³ŠåŒ¹é…é˜ˆå€¼
}

def create_fixed_demo_filters():
    """åˆ›å»ºä¿®å¤åçš„æ¼”ç¤ºè¿‡æ»¤å™¨"""
    return SearchFilters(
        start_date=datetime.now() - timedelta(days=1825), # æœ€è¿‘5å¹´ (è¿›ä¸€æ­¥æ”¾å®½)
        end_date=None,                                     # ä¸é™åˆ¶ç»“æŸæ—¶é—´
        min_citations=0,                                   # ä¸é™åˆ¶å¼•ç”¨æ•°
        max_citations=None,                                # ä¸é™åˆ¶æœ€å¤§å¼•ç”¨æ•°
        conferences=None,                                  # ğŸ”§ æš‚æ—¶ç§»é™¤ä¼šè®®ç­›é€‰
        exclude_conferences=None,                          # ä¸æ’é™¤ä¼šè®®
        categories=None,                                   # ä¸é™åˆ¶ç±»åˆ«
        min_abstract_length=5,                             # ğŸ”§ è¿›ä¸€æ­¥é™ä½æ‘˜è¦é•¿åº¦è¦æ±‚
        fuzzy_matching=DEMO_CONFIG['enable_fuzzy_matching'],  # å¯ç”¨æ¨¡ç³ŠåŒ¹é…
        similarity_threshold=DEMO_CONFIG['similarity_threshold']  # è®¾ç½®ç›¸ä¼¼åº¦é˜ˆå€¼
    )

def demo_fixed_research():
    """
    ä¿®å¤ç‰ˆå¿«é€Ÿç ”ç©¶æ¼”ç¤ºä¸»å‡½æ•°
    Fixed quick research demo main function
    """
    print("ğŸŒŸ æ·±ç ”æ˜Ÿå›¾ä¿®å¤ç‰ˆæ¼”ç¤º (å¤šæºæœç´¢ + æ¨¡ç³ŠåŒ¹é…)")
    print("ğŸŒŸ Research Constellation Fixed Demo (Multi-source + Fuzzy Matching)")
    print("=" * 70)
    
    # åˆå§‹åŒ–ç»„ä»¶
    print("ğŸ”§ åˆå§‹åŒ–å¢å¼ºå‹å¤šæºæœç´¢ç³»ç»Ÿ...")
    print("ğŸ”§ Initializing enhanced multi-source search system...")
    
    try:
        ai_client = DeepSeekClient()
        searcher = EnhancedMultiSourcePaperSearcher()  # ä½¿ç”¨æ–°çš„å¤šæºæœç´¢å™¨
        processor = EnhancedPDFProcessor()
        print("âœ… ç»„ä»¶åˆå§‹åŒ–å®Œæˆ")
        print("âœ… Components initialized successfully")
        print(f"   - æ”¯æŒçš„æœç´¢æº: Google Scholar, {'scholarly, ' if searcher.scholarly_available else ''}DBLP, arXiv")
        print(f"   - Supported sources: Google Scholar, {'scholarly, ' if searcher.scholarly_available else ''}DBLP, arXiv")
        print(f"   - æ¨¡ç³ŠåŒ¹é…: {'å¯ç”¨' if DEMO_CONFIG['enable_fuzzy_matching'] else 'ç¦ç”¨'}")
        print(f"   - Fuzzy matching: {'Enabled' if DEMO_CONFIG['enable_fuzzy_matching'] else 'Disabled'}")
    except Exception as e:
        print(f"âŒ åˆå§‹åŒ–å¤±è´¥: {e}")
        print(f"âŒ Initialization failed: {e}")
        return
    
    # ç ”ç©¶ä¸»é¢˜
    research_topic = DEMO_CONFIG['demo_research_topic']
    print(f"\nğŸ¯ æ¼”ç¤ºç ”ç©¶ä¸»é¢˜: {research_topic}")
    print(f"ğŸ¯ Demo research topic: {research_topic}")
    
    # åˆ›å»ºä¸‹è½½ç›®å½•
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    download_dir = Path("demo_downloads") / f"fixed_demo_{timestamp}"
    download_dir.mkdir(parents=True, exist_ok=True)
    print(f"ğŸ“ åˆ›å»ºä¸‹è½½ç›®å½•: {download_dir}")
    print(f"ğŸ“ Created download directory: {download_dir}")
    
    # åˆ›å»ºä¿®å¤åçš„è¿‡æ»¤å™¨
    filters = create_fixed_demo_filters()
    print(f"ğŸ” åº”ç”¨ä¿®å¤åçš„è¿‡æ»¤æ¡ä»¶:")
    print(f"ğŸ” Applied fixed filter conditions:")
    print(f"   - æ—¶é—´èŒƒå›´: æœ€è¿‘5å¹´")
    print(f"   - Time range: Last 5 years")
    print(f"   - ä¼šè®®ç­›é€‰: None (æš‚æ—¶ç§»é™¤)")
    print(f"   - Conference filter: None (temporarily removed)")
    print(f"   - æœ€å°æ‘˜è¦é•¿åº¦: 5å­—ç¬¦")
    print(f"   - Minimum abstract length: 5 characters")
    print(f"   - æ¨¡ç³ŠåŒ¹é…: å¯ç”¨ï¼Œé˜ˆå€¼ {filters.similarity_threshold}")
    print(f"   - Fuzzy matching: Enabled, threshold {filters.similarity_threshold}")
    print(f"   - å¤šæºæœç´¢: Google Scholar â†’ scholarly â†’ DBLP â†’ arXiv")
    print(f"   - Multi-source: Google Scholar â†’ scholarly â†’ DBLP â†’ arXiv")
    
    all_analyses = []
    search_round = 1
    
    try:
        # ç¬¬ä¸€è½®æœç´¢
        print(f"\n{'='*60}")
        print(f"ğŸ”„ ç¬¬ {search_round} è½®æœç´¢ (å¤šæºå¢å¼º)")
        print(f"ğŸ”„ Round {search_round} Search (Multi-source Enhanced)")
        print(f"{'='*60}")
        
        # ç”Ÿæˆæœç´¢æŸ¥è¯¢
        print(f"ğŸ§  ç”Ÿæˆæœç´¢æŸ¥è¯¢...")
        print(f"ğŸ§  Generating search queries...")
        queries = ai_client.generate_search_queries(
            research_topic, 
            num_queries=DEMO_CONFIG['num_search_queries']
        )
        print(f"ğŸ“ ç”Ÿæˆçš„æŸ¥è¯¢: {queries}")
        print(f"ğŸ“ Generated queries: {queries}")
        
        # æ‰§è¡Œå¤šæºæœç´¢
        print(f"ğŸ” æ‰§è¡Œå¤šæºè®ºæ–‡æœç´¢...")
        print(f"ğŸ” Executing multi-source paper search...")
        papers = searcher.search_multiple_queries_enhanced(queries, filters)
        
        if not papers:
            print("âš ï¸ å¤šæºæœç´¢æœªæ‰¾åˆ°è®ºæ–‡ï¼Œè¿™å¯èƒ½è¡¨æ˜:")
            print("âš ï¸ Multi-source search found no papers, this might indicate:")
            print("   1. ç½‘ç»œè¿æ¥é—®é¢˜")
            print("   1. Network connection issues")
            print("   2. æ‰€æœ‰æœç´¢æºéƒ½æš‚æ—¶ä¸å¯ç”¨")
            print("   2. All search sources temporarily unavailable")
            print("   3. æŸ¥è¯¢è¿‡äºå…·ä½“")
            print("   3. Query too specific")
            
            # å°è¯•æ›´é€šç”¨çš„æŸ¥è¯¢ä½œä¸ºfallback
            print("\nğŸ”„ å°è¯•æ›´é€šç”¨çš„æŸ¥è¯¢ä½œä¸ºfallback...")
            print("ğŸ”„ Trying more general query as fallback...")
            fallback_queries = ["transformer neural networks", "attention mechanism deep learning"]
            for fallback_query in fallback_queries:
                print(f"   å°è¯•æŸ¥è¯¢: {fallback_query}")
                print(f"   Trying query: {fallback_query}")
                papers = searcher.search_papers_multi_source(fallback_query, filters)
                if papers:
                    print(f"âœ… FallbackæŸ¥è¯¢æˆåŠŸï¼Œæ‰¾åˆ° {len(papers)} ç¯‡è®ºæ–‡")
                    print(f"âœ… Fallback query successful, found {len(papers)} papers")
                    break
            
            if not papers:
                print("âŒ æ‰€æœ‰fallbackæŸ¥è¯¢éƒ½å¤±è´¥ï¼Œæ¼”ç¤ºç»“æŸ")
                print("âŒ All fallback queries failed, demo ended")
                return
        
        # é™åˆ¶å¤„ç†æ•°é‡
        papers_to_process = papers[:DEMO_CONFIG['max_papers_per_depth']]
        print(f"ğŸ“š æ‰¾åˆ° {len(papers)} ç¯‡è®ºæ–‡ï¼Œå°†å¤„ç† {len(papers_to_process)} ç¯‡")
        print(f"ğŸ“š Found {len(papers)} papers, will process {len(papers_to_process)}")
        
        # æ˜¾ç¤ºè®ºæ–‡æ¥æºç»Ÿè®¡
        source_stats = {}
        for paper in papers:
            source = paper.get('source', 'unknown')
            source_stats[source] = source_stats.get(source, 0) + 1
        
        print(f"ğŸ“Š è®ºæ–‡æ¥æºåˆ†å¸ƒ:")
        print(f"ğŸ“Š Paper source distribution:")
        for source, count in source_stats.items():
            print(f"   {source}: {count} ç¯‡")
            print(f"   {source}: {count} papers")
        
        # æ˜¾ç¤ºè®ºæ–‡åˆ—è¡¨
        print(f"\nğŸ“‹ å°†å¤„ç†çš„è®ºæ–‡:")
        print(f"ğŸ“‹ Papers to be processed:")
        for i, paper in enumerate(papers_to_process, 1):
            print(f"  {i}. {paper['title'][:60]}...")
            source_info = f"æ¥æº: {paper.get('source', 'unknown')}"
            citation_info = f"å¼•ç”¨: {paper.get('citations', 0)}"
            venue_info = f"ä¼šè®®: {paper.get('venue', 'N/A')}" if paper.get('venue') else ""
            print(f"     {source_info} | {citation_info} {('| ' + venue_info) if venue_info else ''}")
        
        # å¤„ç†è®ºæ–‡ï¼ˆç®€åŒ–ç‰ˆ - ä¸»è¦ä½¿ç”¨æ‘˜è¦ï¼‰
        processed_papers = []
        print(f"\nğŸ“¥ å¼€å§‹å¤„ç†è®ºæ–‡ (å¿«é€Ÿæ¨¡å¼)...")
        print(f"ğŸ“¥ Starting paper processing (quick mode)...")
        
        for i, paper in enumerate(papers_to_process, 1):
            print(f"ğŸ“„ å¤„ç†è®ºæ–‡ {i}/{len(papers_to_process)}: {paper['title']}...")
            print(f"ğŸ“„ Processing paper {i}/{len(papers_to_process)}: {paper['title']}...")
            
            if DEMO_CONFIG['enable_pdf_download']:
                # å°è¯•å®Œæ•´å¤„ç†
                processed_paper = processor.process_paper(paper, str(download_dir))
                if processed_paper:
                    processed_papers.append(processed_paper)
                    print(f"âœ… PDFå¤„ç†æˆåŠŸ")
                    print(f"âœ… PDF processing successful")
                else:
                    # Fallbackåˆ°æ‘˜è¦æ¨¡å¼
                    paper['extracted_text'] = paper.get('abstract', '')
                    paper['text_chunks'] = [paper.get('abstract', '')]
                    paper['text_length'] = len(paper.get('abstract', ''))
                    processed_papers.append(paper)
                    print(f"ğŸ“ Fallbackåˆ°æ‘˜è¦æ¨¡å¼")
                    print(f"ğŸ“ Fallback to abstract mode")
            else:
                # å¿«é€Ÿæ¨¡å¼ï¼šä»…ä½¿ç”¨æ‘˜è¦
                paper['extracted_text'] = paper.get('abstract', '')
                paper['text_chunks'] = [paper.get('abstract', '')]
                paper['text_length'] = len(paper.get('abstract', ''))
                processed_papers.append(paper)
                print(f"ğŸ“ å¿«é€Ÿæ¨¡å¼ (æ‘˜è¦)")
                print(f"ğŸ“ Quick mode (abstract)")
        
        print(f"\nğŸ‰ æˆåŠŸå¤„ç† {len(processed_papers)} ç¯‡è®ºæ–‡")
        print(f"ğŸ‰ Successfully processed {len(processed_papers)} papers")
        
        # AIåˆ†æè®ºæ–‡
        print(f"\nğŸ§  å¼€å§‹AIåˆ†æ...")
        print(f"ğŸ§  Starting AI analysis...")
        
        for i, paper in enumerate(processed_papers, 1):
            print(f"  åˆ†æè®ºæ–‡ {i}/{len(processed_papers)}: {paper['title']}...")
            print(f"  Analyzing paper {i}/{len(processed_papers)}: {paper['title'] }...")
            
            try:
                text_chunks = paper.get('text_chunks', [])
                analysis = ai_client.analyze_paper_text(
                    paper['title'], 
                    paper.get('abstract', ''), 
                    text_chunks
                )
                
                all_analyses.append({
                    'paper': paper['title'],
                    'analysis': analysis,
                    'citations': paper.get('citations', 0),
                    'source': paper.get('source', 'unknown'),
                    'venue': paper.get('venue', ''),
                    'published_str': paper.get('published_str', 'Unknown')
                })
                print(f"  âœ… åˆ†æå®Œæˆ")
                print(f"  âœ… Analysis completed")
                
            except Exception as e:
                print(f"  âŒ åˆ†æå¤±è´¥: {e}")
                print(f"  âŒ Analysis failed: {e}")
        
        # ç”Ÿæˆç ”ç©¶æ€»ç»“
        if all_analyses:
            print(f"\nğŸ“ ç”Ÿæˆç ”ç©¶æ€»ç»“...")
            print(f"ğŸ“ Generating research summary...")
            
            final_summary = ai_client.analyze_multiple_papers_summary(
                all_analyses, research_topic, depth_round=search_round
            )
            
            # æ˜¾ç¤ºç»“æœ
            print(f"\n{'='*70}")
            print(f"ğŸ¯ æ·±ç ”æ˜Ÿå›¾ä¿®å¤ç‰ˆæ¼”ç¤ºç»“æœ")
            print(f"ğŸ¯ Research Constellation Fixed Demo Results")
            print(f"{'='*70}")
            
            print(f"ğŸ”¬ ç ”ç©¶ä¸»é¢˜: {research_topic}")
            print(f"ğŸ”¬ Research topic: {research_topic}")
            print(f"ğŸ“Š æœç´¢è½®æ•°: {search_round}")
            print(f"ğŸ“Š Search rounds: {search_round}")
            print(f"ğŸ“š åˆ†æè®ºæ–‡æ€»æ•°: {len(all_analyses)}")
            print(f"ğŸ“š Total papers analyzed: {len(all_analyses)}")
            print(f"ğŸ” ä½¿ç”¨çš„æœç´¢æº: {', '.join(source_stats.keys())}")
            print(f"ğŸ” Search sources used: {', '.join(source_stats.keys())}")
            print(f"ğŸ¯ æ¨¡ç³ŠåŒ¹é…é˜ˆå€¼: {filters.similarity_threshold}")
            print(f"ğŸ¯ Fuzzy matching threshold: {filters.similarity_threshold}")
            
            # æ˜¾ç¤ºå„æºè®ºæ–‡ç»Ÿè®¡
            analysis_source_stats = {}
            for analysis in all_analyses:
                source = analysis.get('source', 'unknown')
                analysis_source_stats[source] = analysis_source_stats.get(source, 0) + 1
            
            print(f"\nğŸ“Š åˆ†æè®ºæ–‡æ¥æºåˆ†å¸ƒ:")
            print(f"ğŸ“Š Analyzed papers source distribution:")
            for source, count in analysis_source_stats.items():
                print(f"   {source}: {count} ç¯‡")
                print(f"   {source}: {count} papers")
            
            print(f"\nğŸ“ ç ”ç©¶æ€»ç»“:")
            print(f"ğŸ“ Research Summary:")
            print("-" * 60)
            print(final_summary)
            
            # æ˜¾ç¤ºè®ºæ–‡è¯¦æƒ…
            print(f"\nğŸ“š åˆ†æçš„è®ºæ–‡è¯¦æƒ…:")
            print(f"ğŸ“š Analyzed papers details:")
            print("-" * 60)
            for i, analysis in enumerate(all_analyses, 1):
                print(f"{i}. {analysis['paper']}")
                print(f"   æ¥æº: {analysis['source']} | å¼•ç”¨: {analysis['citations']} | å‘è¡¨: {analysis['published_str']}")
                if analysis.get('venue'):
                    print(f"   ä¼šè®®/æœŸåˆŠ: {analysis['venue']}")
                print(f"   åˆ†ææ‘˜è¦: {analysis['analysis'][:150]}...")
                print()
            
            # ä¿å­˜ç»“æœ
            result_file = download_dir / "fixed_demo_result.txt"
            with open(result_file, 'w', encoding='utf-8') as f:
                f.write(f"æ·±ç ”æ˜Ÿå›¾ä¿®å¤ç‰ˆæ¼”ç¤ºç»“æœ\n")
                f.write(f"Research Constellation Fixed Demo Results\n")
                f.write(f"=" * 50 + "\n\n")
                f.write(f"ç ”ç©¶ä¸»é¢˜: {research_topic}\n")
                f.write(f"æœç´¢è½®æ•°: {search_round}\n")
                f.write(f"åˆ†æè®ºæ–‡æ•°: {len(all_analyses)}\n")
                f.write(f"ä½¿ç”¨çš„æœç´¢æº: {', '.join(source_stats.keys())}\n")
                f.write(f"æ¨¡ç³ŠåŒ¹é…é˜ˆå€¼: {filters.similarity_threshold}\n\n")
                f.write(f"ç ”ç©¶æ€»ç»“:\n{final_summary}\n\n")
                f.write(f"è®ºæ–‡åˆ†æè¯¦æƒ…:\n")
                for i, analysis in enumerate(all_analyses, 1):
                    f.write(f"{i}. {analysis['paper']}\n")
                    f.write(f"æ¥æº: {analysis['source']} | å¼•ç”¨: {analysis['citations']}\n")
                    f.write(f"åˆ†æ: {analysis['analysis'][:300]}...\n\n")
            
            print(f"\nğŸ’¾ ç»“æœå·²ä¿å­˜åˆ°: {result_file}")
            print(f"ğŸ’¾ Results saved to: {result_file}")
            
        else:
            print("âŒ æ²¡æœ‰æˆåŠŸåˆ†æçš„è®ºæ–‡")
            print("âŒ No successfully analyzed papers")
    
    except Exception as e:
        print(f"âŒ æ¼”ç¤ºè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        print(f"âŒ Error during demo: {e}")
        import traceback
        traceback.print_exc()
    
    print(f"\nğŸ‰ æ·±ç ”æ˜Ÿå›¾ä¿®å¤ç‰ˆæ¼”ç¤ºå®Œæˆï¼")
    print(f"ğŸ‰ Research Constellation fixed demo completed!")
    print(f"ğŸŒŸ ç‰¹è‰²åŠŸèƒ½:")
    print(f"ğŸŒŸ Featured capabilities:")
    print(f"   âœ… å¤šæºæœç´¢: è‡ªåŠ¨fallbackåˆ°ä¸åŒæ•°æ®æº")
    print(f"   âœ… Multi-source search: Auto-fallback to different data sources")
    print(f"   âœ… æ¨¡ç³ŠåŒ¹é…: å®¹å¿ç»†èŠ‚è¯¯å·®ï¼Œæé«˜åŒ¹é…æˆåŠŸç‡")
    print(f"   âœ… Fuzzy matching: Tolerates detail errors, improves matching success rate")
    print(f"   âœ… æ™ºèƒ½å»é‡: åŸºäºå†…å®¹ç›¸ä¼¼åº¦å»é™¤é‡å¤è®ºæ–‡")
    print(f"   âœ… Smart deduplication: Remove duplicate papers based on content similarity")

if __name__ == "__main__":
    demo_fixed_research()