import requests
import fitz  # PyMuPDF
from pathlib import Path
from typing import Optional, Dict, List
import time
import re
from urllib.parse import urljoin, urlparse
from bs4 import BeautifulSoup
import arxiv
import random

from config import (
    DOWNLOAD_DIR, 
    EXTRACT_FULL_PDF, 
    MAX_PAGES_TO_EXTRACT, 
    MAX_INPUT_TOKENS,
    MAX_TEXT_LENGTH,
    PDF_CHUNK_SIZE
)

class EnhancedPDFProcessor:
    def __init__(self):
        self.download_dir = Path(DOWNLOAD_DIR)
        self.download_dir.mkdir(exist_ok=True)
        
        # ğŸ”§ å¢å¼ºçš„è¯·æ±‚ä¼šè¯é…ç½®ï¼Œæ›´å¥½åœ°æ¨¡æ‹ŸçœŸå®æµè§ˆå™¨
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7',
            'Accept-Language': 'en-US,en;q=0.9',
            'Accept-Encoding': 'gzip, deflate, br',
            'DNT': '1',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
            'Sec-Fetch-Dest': 'document',
            'Sec-Fetch-Mode': 'navigate',
            'Sec-Fetch-Site': 'none',
            'Sec-Fetch-User': '?1',
            'Cache-Control': 'max-age=0'
        })
        
        # æ”¯æŒçš„PDFåŸŸåå’Œå…¶ç‰¹æ®Šå¤„ç†æ–¹æ³•
        self.pdf_handlers = {
            'arxiv.org': self._handle_arxiv_pdf,
            'researchgate.net': self._handle_researchgate_pdf,
            'academia.edu': self._handle_academia_pdf,
            'semanticscholar.org': self._handle_semantic_scholar_pdf,
            'ieee.org': self._handle_ieee_pdf,
            'acm.org': self._handle_acm_pdf,
            'sciencedirect.com': self._handle_sciencedirect_pdf,
            'springer.com': self._handle_springer_pdf,
            'nature.com': self._handle_nature_pdf,
        }
        
        print(f"ğŸ”§ Enhanced PDF Processor åˆå§‹åŒ–å®Œæˆ")
        print(f"   - æ”¯æŒ {len(self.pdf_handlers)} ç§ä¸“é—¨çš„PDFå¤„ç†å™¨")
        print(f"   - å¢å¼ºçš„æµè§ˆå™¨æ¨¡æ‹Ÿ")
        print(f"   - å¤šé‡å¤‡ç”¨ä¸‹è½½æœºåˆ¶")
    
    def process_paper(self, paper: Dict, download_dir: str) -> Optional[Dict]:
        """
        å¤„ç†å•ç¯‡è®ºæ–‡ï¼šä¸‹è½½+æå– (å¢å¼ºç‰ˆ)
        """
        title = paper.get('title', 'Unknown')
        print(f"ğŸ” æ­£åœ¨å¤„ç†è®ºæ–‡: {title}")
        
        # ğŸ¯ å¤šé‡PDFè·å–ç­–ç•¥
        pdf_path = self._get_pdf_with_enhanced_strategies(paper, download_dir)
        
        if pdf_path:
            text = self.extract_text(pdf_path)
            if text:
                paper['local_path'] = str(pdf_path)
                paper['extracted_text'] = text
                paper['text_length'] = len(text)
                
                if len(text) > PDF_CHUNK_SIZE:
                    paper['text_chunks'] = self.split_text_into_chunks(text)
                    print(f"ğŸ“š è®ºæ–‡å¤„ç†å®Œæˆï¼Œåˆ†ä¸º {len(paper['text_chunks'])} ä¸ªæ–‡æœ¬å—")
                else:
                    paper['text_chunks'] = [text]
                    print(f"ğŸ“š è®ºæ–‡å¤„ç†å®Œæˆï¼Œå•ä¸ªæ–‡æœ¬å—")
                
                return paper
            else:
                print("âŒ æ–‡æœ¬æå–å¤±è´¥")
        else:
            print("âŒ PDFä¸‹è½½å¤±è´¥")
        
        # Fallbackï¼šä½¿ç”¨æ‘˜è¦
        if paper.get('abstract'):
            print("ğŸ“ ä½¿ç”¨æ‘˜è¦ä½œä¸ºfallback")
            paper['extracted_text'] = paper['abstract']
            paper['text_length'] = len(paper['abstract'])
            paper['text_chunks'] = [paper['abstract']]
            paper['local_path'] = None
            return paper
        
        return None
    
    def _get_pdf_with_enhanced_strategies(self, paper: Dict, download_dir: str) -> Optional[Path]:
        """å¢å¼ºçš„PDFè·å–ç­–ç•¥"""
        title = paper.get('title', 'Unknown')
        safe_title = self._generate_safe_filename(title)
        
        print(f"  ğŸ¯ å¼€å§‹å¢å¼ºPDFè·å–ç­–ç•¥...")
        
        # ç­–ç•¥1: ä¼˜å…ˆå¤„ç†å·²çŸ¥çš„é«˜æˆåŠŸç‡é“¾æ¥ï¼ˆarXivç­‰ï¼‰
        pdf_links = paper.get('pdf_links', [])
        if paper.get('pdf_url'):
            pdf_links = [paper['pdf_url']] + pdf_links
        
        prioritized_links = self._prioritize_pdf_links(pdf_links)
        
        print(f"  ğŸ“‹ æ‰¾åˆ° {len(prioritized_links)} ä¸ªPDFé“¾æ¥ï¼ŒæŒ‰ä¼˜å…ˆçº§æ’åº")
        
        # ç­–ç•¥2: æŒ‰ä¼˜å…ˆçº§å°è¯•ä¸‹è½½
        for i, link in enumerate(prioritized_links):
            print(f"    å°è¯•é“¾æ¥ {i+1}/{len(prioritized_links)}: {self._truncate_url_for_display(link)}")
            
            # æ ¹æ®åŸŸåé€‰æ‹©ä¸“é—¨çš„å¤„ç†å™¨
            domain = self._extract_domain(link)
            if domain in self.pdf_handlers:
                print(f"      ä½¿ç”¨ä¸“é—¨å¤„ç†å™¨: {domain}")
                pdf_path = self.pdf_handlers[domain](link, safe_title, download_dir)
            else:
                print(f"      ä½¿ç”¨é€šç”¨å¤„ç†å™¨")
                pdf_path = self._download_from_url_enhanced(link, safe_title, download_dir)
            
            if pdf_path:
                print(f"    âœ… æˆåŠŸä¸‹è½½: {domain}")
                return pdf_path
            
            # æ·»åŠ å»¶è¿Ÿé¿å…è¢«é™åˆ¶
            time.sleep(random.uniform(1, 2))
        
        # ç­–ç•¥3: å¦‚æœæœ‰Google Scholaré¡µé¢ï¼Œå°è¯•æ·±åº¦è§£æ
        if paper.get('source') == 'google_scholar' and paper.get('paper_url'):
            print(f"  ğŸ” ç­–ç•¥3: æ·±åº¦è§£æGoogle Scholaré¡µé¢")
            pdf_path = self._deep_parse_scholar_page(paper['paper_url'], safe_title, download_dir)
            if pdf_path:
                return pdf_path
        
        # ç­–ç•¥4: æ™ºèƒ½æœç´¢å¤‡ç”¨æº
        print(f"  ğŸ” ç­–ç•¥4: æ™ºèƒ½æœç´¢å¤‡ç”¨PDFæº")
        pdf_path = self._intelligent_search_fallback(title, safe_title, download_dir)
        if pdf_path:
            return pdf_path
        
        print(f"  âŒ æ‰€æœ‰PDFè·å–ç­–ç•¥éƒ½å¤±è´¥äº†")
        return None
    
    def _prioritize_pdf_links(self, links: List[str]) -> List[str]:
        """æŒ‰æˆåŠŸç‡å¯¹PDFé“¾æ¥æ’åº"""
        def link_priority(link):
            link_lower = link.lower()
            
            # arXiv - æœ€é«˜ä¼˜å…ˆçº§
            if 'arxiv.org' in link_lower:
                return 1
            
            # å¼€æ”¾è·å–å¹³å°
            if any(domain in link_lower for domain in ['researchgate.net', 'academia.edu', 'semanticscholar.org']):
                return 2
            
            # ç›´æ¥PDFæ–‡ä»¶
            if '.pdf' in link_lower:
                return 3
            
            # PMCç­‰å¼€æ”¾æœŸåˆŠ
            if any(domain in link_lower for domain in ['ncbi.nlm.nih.gov', 'pubmed', 'biorxiv', 'medrxiv']):
                return 4
            
            # æœºæ„ä»“åº“
            if any(keyword in link_lower for keyword in ['repository', 'dspace', 'eprints']):
                return 5
            
            # å‡ºç‰ˆå•†ï¼ˆæˆåŠŸç‡ç›¸å¯¹è¾ƒä½ï¼‰
            if any(domain in link_lower for domain in ['ieee.org', 'acm.org', 'springer.com']):
                return 6
            
            # ScienceDirectç­‰ï¼ˆæˆåŠŸç‡æœ€ä½ï¼‰
            if 'sciencedirect.com' in link_lower:
                return 7
            
            return 8
        
        return sorted(set(links), key=link_priority)  # å»é‡å¹¶æ’åº
    
    def _extract_domain(self, url: str) -> str:
        """æå–URLçš„åŸŸå"""
        try:
            parsed = urlparse(url)
            domain = parsed.netloc.lower()
            # ç§»é™¤wwwå‰ç¼€
            if domain.startswith('www.'):
                domain = domain[4:]
            return domain
        except:
            return ""
    
    def _truncate_url_for_display(self, url: str, max_length: int = 80) -> str:
        """æˆªæ–­URLç”¨äºæ˜¾ç¤º (ä¿ç•™å®Œæ•´åŸŸåä¿¡æ¯)"""
        if len(url) <= max_length:
            return url
        
        # å°è¯•ä¿ç•™é‡è¦éƒ¨åˆ†
        parsed = urlparse(url)
        domain = parsed.netloc
        path = parsed.path
        
        if len(domain) + 10 < max_length:
            # æ˜¾ç¤ºåŸŸå + è·¯å¾„çš„å¼€å§‹éƒ¨åˆ†
            remaining = max_length - len(domain) - 10
            if len(path) > remaining:
                return f"{domain}{path[:remaining]}..."
            else:
                return f"{domain}{path}..."
        else:
            # åŸŸåå¤ªé•¿ï¼Œç›´æ¥æˆªæ–­
            return url[:max_length-3] + "..."
    
    # ğŸ¯ ä¸“é—¨çš„PDFå¤„ç†å™¨
    
    def _handle_arxiv_pdf(self, url: str, filename: str, download_dir: str) -> Optional[Path]:
        """arXiv PDFå¤„ç†å™¨"""
        try:
            # ç¡®ä¿ä½¿ç”¨PDF URL
            if '/abs/' in url:
                pdf_url = url.replace('/abs/', '/pdf/') + '.pdf'
            elif '/pdf/' in url and not url.endswith('.pdf'):
                pdf_url = url + '.pdf'
            else:
                pdf_url = url
            
            return self._download_from_url_enhanced(pdf_url, filename, download_dir)
            
        except Exception as e:
            print(f"      âŒ arXivå¤„ç†å¤±è´¥: {e}")
            return None
    
    def _handle_researchgate_pdf(self, url: str, filename: str, download_dir: str) -> Optional[Path]:
        """ResearchGate PDFå¤„ç†å™¨"""
        try:
            # ResearchGateé€šå¸¸éœ€è¦ç‰¹æ®Šå¤„ç†ï¼Œå…ˆå°è¯•ç›´æ¥ä¸‹è½½
            return self._download_from_url_enhanced(url, filename, download_dir)
            
        except Exception as e:
            print(f"      âŒ ResearchGateå¤„ç†å¤±è´¥: {e}")
            return None
    
    def _handle_academia_pdf(self, url: str, filename: str, download_dir: str) -> Optional[Path]:
        """Academia.edu PDFå¤„ç†å™¨"""
        try:
            return self._download_from_url_enhanced(url, filename, download_dir)
            
        except Exception as e:
            print(f"      âŒ Academia.eduå¤„ç†å¤±è´¥: {e}")
            return None
    
    def _handle_semantic_scholar_pdf(self, url: str, filename: str, download_dir: str) -> Optional[Path]:
        """Semantic Scholar PDFå¤„ç†å™¨"""
        try:
            return self._download_from_url_enhanced(url, filename, download_dir)
            
        except Exception as e:
            print(f"      âŒ Semantic Scholarå¤„ç†å¤±è´¥: {e}")
            return None
    
    def _handle_ieee_pdf(self, url: str, filename: str, download_dir: str) -> Optional[Path]:
        """IEEE PDFå¤„ç†å™¨"""
        try:
            # IEEEçš„PDFé€šå¸¸éœ€è¦ç‰¹æ®ŠURLæ ¼å¼
            if '/document/' in url:
                doc_id = re.search(r'/document/(\d+)', url)
                if doc_id:
                    pdf_url = f"https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber={doc_id.group(1)}"
                    return self._download_from_url_enhanced(pdf_url, filename, download_dir)
            
            return self._download_from_url_enhanced(url, filename, download_dir)
            
        except Exception as e:
            print(f"      âŒ IEEEå¤„ç†å¤±è´¥: {e}")
            return None
    
    def _handle_acm_pdf(self, url: str, filename: str, download_dir: str) -> Optional[Path]:
        """ACM PDFå¤„ç†å™¨"""
        try:
            # ACMçš„PDFé“¾æ¥æ¨¡å¼
            if '/doi/' in url and '/pdf/' not in url:
                pdf_url = url.replace('/doi/', '/doi/pdf/')
                return self._download_from_url_enhanced(pdf_url, filename, download_dir)
            
            return self._download_from_url_enhanced(url, filename, download_dir)
            
        except Exception as e:
            print(f"      âŒ ACMå¤„ç†å¤±è´¥: {e}")
            return None
    
    def _handle_sciencedirect_pdf(self, url: str, filename: str, download_dir: str) -> Optional[Path]:
        """ScienceDirect PDFå¤„ç†å™¨ï¼ˆå¢å¼ºæŠ—åçˆ¬è™«ï¼‰"""
        try:
            # ScienceDirectéœ€è¦ç‰¹æ®Šçš„è¯·æ±‚å¤´
            special_headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
                'Accept-Language': 'en-US,en;q=0.5',
                'Accept-Encoding': 'gzip, deflate, br',
                'Referer': 'https://www.google.com/',  # é‡è¦ï¼šæ·»åŠ Googleå¼•è
                'DNT': '1',
                'Connection': 'keep-alive',
                'Upgrade-Insecure-Requests': '1',
                'Sec-Fetch-Dest': 'document',
                'Sec-Fetch-Mode': 'navigate',
                'Sec-Fetch-Site': 'cross-site',
            }
            
            # å°è¯•è½¬æ¢ä¸ºPDF URL
            if '/pii/' in url:
                pdf_url = url.replace('/pii/', '/pdf/') + '.pdf'
                return self._download_with_special_headers(pdf_url, filename, download_dir, special_headers)
            
            return self._download_with_special_headers(url, filename, download_dir, special_headers)
            
        except Exception as e:
            print(f"      âŒ ScienceDirectå¤„ç†å¤±è´¥: {e}")
            return None
    
    def _handle_springer_pdf(self, url: str, filename: str, download_dir: str) -> Optional[Path]:
        """Springer PDFå¤„ç†å™¨"""
        try:
            return self._download_from_url_enhanced(url, filename, download_dir)
            
        except Exception as e:
            print(f"      âŒ Springerå¤„ç†å¤±è´¥: {e}")
            return None
    
    def _handle_nature_pdf(self, url: str, filename: str, download_dir: str) -> Optional[Path]:
        """Nature PDFå¤„ç†å™¨"""
        try:
            return self._download_from_url_enhanced(url, filename, download_dir)
            
        except Exception as e:
            print(f"      âŒ Natureå¤„ç†å¤±è´¥: {e}")
            return None
    
    def _download_with_special_headers(self, url: str, filename: str, download_dir: str, headers: Dict) -> Optional[Path]:
        """ä½¿ç”¨ç‰¹æ®Šè¯·æ±‚å¤´ä¸‹è½½"""
        if not url:
            return None
            
        try:
            if not url.startswith(('http://', 'https://')):
                url = 'https://' + url
            
            download_dir_path = Path(download_dir)
            file_path = download_dir_path / f"{filename}.pdf"
            
            if file_path.exists() and file_path.stat().st_size > 1024:
                print(f"        âœ… æ–‡ä»¶å·²å­˜åœ¨")
                return file_path
            
            # åˆ›å»ºç‰¹æ®Šä¼šè¯
            special_session = requests.Session()
            special_session.headers.update(headers)
            
            print(f"        ğŸ“¥ ä½¿ç”¨ç‰¹æ®Šè¯·æ±‚å¤´ä¸‹è½½...")
            
            response = special_session.get(url, timeout=30, stream=True)
            response.raise_for_status()
            
            content_type = response.headers.get('content-type', '').lower()
            
            if 'application/pdf' in content_type:
                with open(file_path, 'wb') as f:
                    for chunk in response.iter_content(chunk_size=8192):
                        if chunk:
                            f.write(chunk)
                
                file_size = file_path.stat().st_size
                if file_size > 1024:
                    print(f"        âœ… ä¸‹è½½æˆåŠŸ: {file_size/1024:.1f} KB")
                    return file_path
                else:
                    file_path.unlink()
                    return None
            else:
                print(f"        âš ï¸ ä¸æ˜¯PDFæ–‡ä»¶: {content_type}")
                return None
            
        except Exception as e:
            print(f"        âŒ ç‰¹æ®Šä¸‹è½½å¤±è´¥: {e}")
            return None
    
    def _download_from_url_enhanced(self, url: str, filename: str, download_dir: str) -> Optional[Path]:
        """å¢å¼ºçš„URLä¸‹è½½æ–¹æ³•"""
        if not url:
            return None
            
        try:
            if not url.startswith(('http://', 'https://')):
                url = 'https://' + url
            
            download_dir_path = Path(download_dir)
            file_path = download_dir_path / f"{filename}.pdf"
            
            if file_path.exists() and file_path.stat().st_size > 1024:
                print(f"        âœ… æ–‡ä»¶å·²å­˜åœ¨")
                return file_path
            
            print(f"        ğŸ“¥ ä¸‹è½½ä¸­...")
            
            # å¢åŠ éšæœºå»¶è¿Ÿ
            time.sleep(random.uniform(0.5, 1.5))
            
            response = self.session.get(url, timeout=30, stream=True)
            response.raise_for_status()
            
            content_type = response.headers.get('content-type', '').lower()
            
            if 'application/pdf' in content_type:
                # ç›´æ¥æ˜¯PDF
                with open(file_path, 'wb') as f:
                    for chunk in response.iter_content(chunk_size=8192):
                        if chunk:
                            f.write(chunk)
                
                file_size = file_path.stat().st_size
                if file_size > 1024:
                    print(f"        âœ… ä¸‹è½½æˆåŠŸ: {file_size/1024:.1f} KB")
                    return file_path
                else:
                    file_path.unlink()
                    return None
            
            elif 'text/html' in content_type:
                # HTMLé¡µé¢ï¼Œå°è¯•è§£æPDFé“¾æ¥
                print(f"        ğŸ” è§£æHTMLé¡µé¢ä¸­çš„PDFé“¾æ¥...")
                soup = BeautifulSoup(response.content, 'html.parser')
                
                pdf_links = self._extract_pdf_links_from_html(soup, url)
                
                for pdf_link in pdf_links[:3]:
                    print(f"        ğŸ“¥ å°è¯•HTMLä¸­çš„PDFé“¾æ¥...")
                    pdf_path = self._download_from_url_enhanced(pdf_link, filename, download_dir)
                    if pdf_path:
                        return pdf_path
            
            return None
            
        except Exception as e:
            print(f"        âŒ ä¸‹è½½å¤±è´¥: {e}")
            return None
    
    def _extract_pdf_links_from_html(self, soup: BeautifulSoup, base_url: str) -> List[str]:
        """ä»HTMLä¸­æå–PDFé“¾æ¥"""
        pdf_links = []
        
        # æŸ¥æ‰¾æ˜ç¡®çš„PDFé“¾æ¥
        for link in soup.find_all('a', href=True):
            href = link['href']
            text = link.get_text().lower()
            
            # æ£€æŸ¥é“¾æ¥å’Œæ–‡æœ¬
            if (('.pdf' in href.lower()) or 
                ('download' in text) or 
                ('pdf' in text) or
                ('full text' in text) or
                ('view pdf' in text)):
                
                full_url = urljoin(base_url, href)
                pdf_links.append(full_url)
        
        # æŸ¥æ‰¾ç‰¹æ®Šçš„PDFæŒ‰é’®æˆ–é“¾æ¥
        pdf_selectors = [
            'a[href*=".pdf"]',
            'a[href*="download"]',
            'a[href*="pdf"]',
            '.pdf-download a',
            '.download-pdf a',
            '.full-text a'
        ]
        
        for selector in pdf_selectors:
            elements = soup.select(selector)
            for element in elements:
                href = element.get('href')
                if href:
                    full_url = urljoin(base_url, href)
                    pdf_links.append(full_url)
        
        return list(set(pdf_links))  # å»é‡
    
    def _deep_parse_scholar_page(self, scholar_url: str, filename: str, download_dir: str) -> Optional[Path]:
        """æ·±åº¦è§£æGoogle Scholaré¡µé¢"""
        try:
            print(f"    ğŸ” æ·±åº¦è§£æScholaré¡µé¢...")
            
            # å¢åŠ éšæœºå»¶è¿Ÿ
            time.sleep(random.uniform(2, 4))
            
            response = self.session.get(scholar_url, timeout=15)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # æ›´å…¨é¢çš„PDFé“¾æ¥æœç´¢
            pdf_links = []
            
            # æ–¹æ³•1: æŸ¥æ‰¾ç›´æ¥PDFé“¾æ¥
            for link in soup.find_all('a', href=True):
                href = link['href']
                text = link.get_text().lower()
                
                if (self._is_potential_pdf_link(href, text)):
                    full_url = urljoin(scholar_url, href)
                    pdf_links.append(full_url)
            
            # æ–¹æ³•2: æŸ¥æ‰¾ç‰¹æ®Šçš„Scholarå…ƒç´ 
            scholar_pdf_elements = soup.find_all(['div', 'span'], class_=re.compile(r'gs_or|gs_fl|gs_ggs'))
            for element in scholar_pdf_elements:
                links = element.find_all('a', href=True)
                for link in links:
                    href = link['href']
                    if self._is_potential_pdf_link(href, link.get_text().lower()):
                        full_url = urljoin(scholar_url, href)
                        pdf_links.append(full_url)
            
            # å°è¯•ä¸‹è½½æ‰¾åˆ°çš„é“¾æ¥
            for pdf_link in pdf_links[:5]:  # é™åˆ¶å°è¯•æ¬¡æ•°
                print(f"        ğŸ“¥ å°è¯•Scholarè§£æé“¾æ¥...")
                pdf_path = self._download_from_url_enhanced(pdf_link, filename, download_dir)
                if pdf_path:
                    return pdf_path
                time.sleep(1)
            
            return None
            
        except Exception as e:
            print(f"    âŒ Scholaræ·±åº¦è§£æå¤±è´¥: {e}")
            return None
    
    def _is_potential_pdf_link(self, href: str, text: str) -> bool:
        """åˆ¤æ–­æ˜¯å¦ä¸ºæ½œåœ¨çš„PDFé“¾æ¥"""
        href_lower = href.lower()
        text_lower = text.lower()
        
        # æ˜ç¡®çš„PDFæŒ‡æ ‡
        if '.pdf' in href_lower:
            return True
        
        # å·²çŸ¥çš„PDFåŸŸå
        pdf_domains = ['arxiv.org', 'researchgate.net', 'academia.edu', 'semanticscholar.org']
        if any(domain in href_lower for domain in pdf_domains):
            return True
        
        # PDFç›¸å…³çš„æ–‡æœ¬
        pdf_keywords = ['pdf', 'download', 'full text', 'view paper', 'get pdf', 'paper']
        if any(keyword in text_lower for keyword in pdf_keywords):
            return True
        
        # ä¸‹è½½ç±»å…³é”®è¯
        download_keywords = ['download', 'view', 'access', 'read']
        if any(keyword in href_lower for keyword in download_keywords):
            return True
        
        return False
    
    def _intelligent_search_fallback(self, title: str, filename: str, download_dir: str) -> Optional[Path]:
        """æ™ºèƒ½å¤šæºPDFæœç´¢å¤‡ç”¨æœºåˆ¶"""
        print(f"    ğŸ” å¯åŠ¨æ™ºèƒ½å¤šæºPDFæœç´¢...")
        
        # æ¸…ç†æ ‡é¢˜ç”¨äºæœç´¢
        clean_title = re.sub(r'[^\w\s]', ' ', title).strip()
        search_words = clean_title.split()[:8]  # å–æ›´å¤šå…³é”®è¯æé«˜åŒ¹é…ç‡
        
        if len(search_words) < 2:
            print(f"      âš ï¸ æ ‡é¢˜å…³é”®è¯ä¸è¶³ï¼Œè·³è¿‡å¤šæºæœç´¢")
            return None
        
        search_query = ' '.join(search_words)
        print(f"      ğŸ” æœç´¢æŸ¥è¯¢: {search_query}")
        
        # ç­–ç•¥1: arXivæœç´¢ï¼ˆæœ€å¯é çš„PDFæºï¼‰
        pdf_path = self._search_arxiv_for_pdf(search_query, title, filename, download_dir)
        if pdf_path:
            return pdf_path
        
        # ç­–ç•¥2: scholarlyåº“æœç´¢
        pdf_path = self._search_scholarly_for_pdf(search_query, title, filename, download_dir)
        if pdf_path:
            return pdf_path
        
        # ç­–ç•¥3: DBLPæœç´¢ï¼ˆé€šè¿‡DOIé“¾æ¥ï¼‰
        pdf_path = self._search_dblp_for_pdf(search_query, title, filename, download_dir)
        if pdf_path:
            return pdf_path
        
        print(f"    âŒ æ‰€æœ‰å¤šæºPDFæœç´¢éƒ½å¤±è´¥äº†")
        return None
    
    def _search_arxiv_for_pdf(self, search_query: str, original_title: str, filename: str, download_dir: str) -> Optional[Path]:
        """ä»arXivæœç´¢ç›¸åŒè®ºæ–‡çš„PDF"""
        try:
            print(f"      ğŸ“š åœ¨arXivä¸­æœç´¢ç›¸åŒè®ºæ–‡...")
            
            import arxiv
            search = arxiv.Search(
                query=search_query,
                max_results=10,  # å¢åŠ æœç´¢ç»“æœæ•°é‡
                sort_by=arxiv.SortCriterion.Relevance
            )
            
            for i, result in enumerate(search.results()):
                similarity = self._title_similarity(original_title, result.title)
                # print(f"        ğŸ“„ å€™é€‰ {i+1}: {result.title}")
                # print(f"           ç›¸ä¼¼åº¦: {similarity:.2f}, arXiv ID: {result.get_short_id()}")
                
                if similarity > 0.2:  # ç›¸å¯¹å®½æ¾çš„åŒ¹é…é˜ˆå€¼
                    print(f"        âœ… æ‰¾åˆ°åŒ¹é…è®ºæ–‡ï¼Œä¸‹è½½PDF...")
                    pdf_path = self._download_from_url_enhanced(result.pdf_url, filename, download_dir)
                    if pdf_path:
                        print(f"        âœ… arXiv PDFä¸‹è½½æˆåŠŸ!")
                        return pdf_path
                    else:
                        print(f"        âŒ arXiv PDFä¸‹è½½å¤±è´¥")
            
            print(f"      âŒ arXivä¸­æœªæ‰¾åˆ°åŒ¹é…è®ºæ–‡")
            return None
            
        except Exception as e:
            print(f"      âŒ arXivæœç´¢å¤±è´¥: {e}")
            return None
    
    def _search_scholarly_for_pdf(self, search_query: str, original_title: str, filename: str, download_dir: str) -> Optional[Path]:
        """ä»scholarlyåº“æœç´¢ç›¸åŒè®ºæ–‡çš„PDF"""
        try:
            print(f"      ğŸ“š åœ¨scholarlyä¸­æœç´¢ç›¸åŒè®ºæ–‡...")
            
            # æ£€æŸ¥scholarlyæ˜¯å¦å¯ç”¨
            try:
                from scholarly import scholarly
            except ImportError:
                print(f"        âš ï¸ scholarlyåº“ä¸å¯ç”¨")
                return None
            
            search_results = scholarly.search_pubs(search_query)
            
            processed_count = 0
            for i, pub in enumerate(search_results):
                if processed_count >= 5:  # é™åˆ¶å¤„ç†æ•°é‡é¿å…è¶…æ—¶
                    break
                
                try:
                    bib = pub.get('bib', {})
                    candidate_title = bib.get('title', '')
                    
                    if not candidate_title:
                        continue
                    
                    similarity = self._title_similarity(original_title, candidate_title)
                    print(f"        ğŸ“„ å€™é€‰ {processed_count+1}: {candidate_title}")
                    print(f"           ç›¸ä¼¼åº¦: {similarity:.2f}")
                    
                    if similarity > 0.4:  # scholarlyä½¿ç”¨ç¨é«˜çš„åŒ¹é…é˜ˆå€¼
                        print(f"        âœ… æ‰¾åˆ°åŒ¹é…è®ºæ–‡ï¼Œå°è¯•è·å–PDF...")
                        
                        # å°è¯•å¤šç§PDFè·å–æ–¹å¼
                        pdf_urls = []
                        
                        # æ–¹å¼1: ç›´æ¥PDFé“¾æ¥
                        if bib.get('eprint'):
                            pdf_urls.append(bib['eprint'])
                        
                        # æ–¹å¼2: è®ºæ–‡é¡µé¢é“¾æ¥
                        if bib.get('url'):
                            pdf_urls.append(bib['url'])
                        
                        # å°è¯•ä¸‹è½½æ‰¾åˆ°çš„PDFé“¾æ¥
                        for pdf_url in pdf_urls:
                            print(f"          ğŸ“¥ å°è¯•PDFé“¾æ¥...")
                            pdf_path = self._download_from_url_enhanced(pdf_url, filename, download_dir)
                            if pdf_path:
                                print(f"        âœ… scholarly PDFä¸‹è½½æˆåŠŸ!")
                                return pdf_path
                        
                        print(f"        âŒ scholarly PDFé“¾æ¥å‡ä¸‹è½½å¤±è´¥")
                    
                    processed_count += 1
                    
                except Exception as e:
                    print(f"        âš ï¸ å¤„ç†scholarlyç»“æœå‡ºé”™: {e}")
                    continue
                
                # æ·»åŠ å»¶è¿Ÿé¿å…è¢«é™åˆ¶
                time.sleep(0.5)
            
            print(f"      âŒ scholarlyä¸­æœªæ‰¾åˆ°åŒ¹é…è®ºæ–‡")
            return None
            
        except Exception as e:
            print(f"      âŒ scholarlyæœç´¢å¤±è´¥: {e}")
            return None
    
    def _search_dblp_for_pdf(self, search_query: str, original_title: str, filename: str, download_dir: str) -> Optional[Path]:
        """ä»DBLPæœç´¢ç›¸åŒè®ºæ–‡çš„PDFï¼ˆé€šè¿‡DOIç­‰ï¼‰"""
        try:
            print(f"      ğŸ“š åœ¨DBLPä¸­æœç´¢ç›¸åŒè®ºæ–‡...")
            
            import xml.etree.ElementTree as ET
            
            # DBLP APIæœç´¢
            dblp_url = "https://dblp.org/search/publ/api"
            params = {
                'q': search_query,
                'format': 'xml',
                'h': 10
            }
            
            response = self.session.get(dblp_url, params=params, timeout=15)
            response.raise_for_status()
            
            root = ET.fromstring(response.content)
            hits = root.findall('.//hit')
            
            for i, hit in enumerate(hits):
                try:
                    info = hit.find('info')
                    if info is None:
                        continue
                    
                    title_elem = info.find('title')
                    candidate_title = title_elem.text if title_elem is not None else ''
                    
                    if not candidate_title:
                        continue
                    
                    similarity = self._title_similarity(original_title, candidate_title)
                    print(f"        ğŸ“„ å€™é€‰ {i+1}: {candidate_title}")
                    print(f"           ç›¸ä¼¼åº¦: {similarity:.2f}")
                    
                    if similarity > 0.5:  # DBLPä½¿ç”¨è¾ƒé«˜çš„åŒ¹é…é˜ˆå€¼
                        print(f"        âœ… æ‰¾åˆ°åŒ¹é…è®ºæ–‡ï¼Œå°è¯•è·å–PDF...")
                        
                        # å°è¯•è·å–DOIé“¾æ¥
                        doi_elem = info.find('doi')
                        if doi_elem is not None and doi_elem.text:
                            doi = doi_elem.text
                            doi_url = f"https://doi.org/{doi}"
                            print(f"          ğŸ“¥ å°è¯•DOIé“¾æ¥: {doi}")
                            
                            pdf_path = self._download_from_url_enhanced(doi_url, filename, download_dir)
                            if pdf_path:
                                print(f"        âœ… DBLP DOI PDFä¸‹è½½æˆåŠŸ!")
                                return pdf_path
                            else:
                                print(f"        âŒ DBLP DOI PDFä¸‹è½½å¤±è´¥")
                        
                        # å°è¯•å…¶ä»–é“¾æ¥
                        url_elem = info.find('url')
                        if url_elem is not None and url_elem.text:
                            paper_url = url_elem.text
                            print(f"          ğŸ“¥ å°è¯•è®ºæ–‡é¡µé¢é“¾æ¥...")
                            
                            pdf_path = self._download_from_url_enhanced(paper_url, filename, download_dir)
                            if pdf_path:
                                print(f"        âœ… DBLP é¡µé¢PDFä¸‹è½½æˆåŠŸ!")
                                return pdf_path
                            else:
                                print(f"        âŒ DBLP é¡µé¢PDFä¸‹è½½å¤±è´¥")
                
                except Exception as e:
                    print(f"        âš ï¸ å¤„ç†DBLPç»“æœå‡ºé”™: {e}")
                    continue
            
            print(f"      âŒ DBLPä¸­æœªæ‰¾åˆ°åŒ¹é…è®ºæ–‡")
            return None
            
        except Exception as e:
            print(f"      âŒ DBLPæœç´¢å¤±è´¥: {e}")
            return None
    
    def _title_similarity(self, title1: str, title2: str) -> float:
        """è®¡ç®—æ ‡é¢˜ç›¸ä¼¼åº¦"""
        def clean_title(title):
            return re.sub(r'[^\w\s]', ' ', title.lower()).split()
        
        words1 = set(clean_title(title1))
        words2 = set(clean_title(title2))
        
        if not words1 or not words2:
            return 0
        
        intersection = words1.intersection(words2)
        union = words1.union(words2)
        
        return len(intersection) / len(union)
    
    def _generate_safe_filename(self, title: str) -> str:
        """ç”Ÿæˆå®‰å…¨çš„æ–‡ä»¶å"""
        safe_title = re.sub(r'[^\w\s-]', '', title).strip()
        safe_title = re.sub(r'\s+', '_', safe_title)
        return safe_title[:80]
    
    def extract_text(self, pdf_path: Path) -> Optional[str]:
        """ä»PDFæå–æ–‡æœ¬ï¼Œæ”¯æŒå®Œæ•´æå–æˆ–éƒ¨åˆ†æå–"""
        try:
            doc = fitz.open(str(pdf_path))
            text = ""
            
            # å†³å®šè¦æå–çš„é¡µæ•°
            if EXTRACT_FULL_PDF:
                if MAX_PAGES_TO_EXTRACT is None:
                    max_pages = doc.page_count
                    print(f"ğŸ“– æå–æ‰€æœ‰ {max_pages} é¡µ")
                else:
                    max_pages = min(MAX_PAGES_TO_EXTRACT, doc.page_count)
                    print(f"ğŸ“– æå– {max_pages}/{doc.page_count} é¡µ")
            else:
                max_pages = min(5, doc.page_count)
                print(f"ğŸ“– æå–å‰ {max_pages} é¡µ")
            
            # æå–æ–‡æœ¬
            for page_num in range(max_pages):
                try:
                    page = doc[page_num]
                    page_text = page.get_text()
                    text += page_text + "\n\n"
                    
                    if (page_num + 1) % 10 == 0:
                        print(f"  è¿›åº¦: {page_num + 1}/{max_pages} é¡µ")
                        
                except Exception as e:
                    print(f"âš ï¸ ç¬¬{page_num}é¡µæå–é”™è¯¯: {e}")
                    continue
            
            doc.close()
            
            # å¤„ç†æ–‡æœ¬é•¿åº¦é™åˆ¶
            if MAX_TEXT_LENGTH is not None and len(text) > MAX_TEXT_LENGTH:
                print(f"ğŸ“ æ–‡æœ¬è¿‡é•¿ ({len(text)} å­—ç¬¦)ï¼Œæˆªæ–­è‡³ {MAX_TEXT_LENGTH} å­—ç¬¦")
                text = text[:MAX_TEXT_LENGTH]
            
            if len(text.strip()) > 100:
                print(f"âœ… æˆåŠŸæå– {len(text):,} å­—ç¬¦")
                return text
            else:
                print(f"âš ï¸ æå–çš„æ–‡æœ¬ä¸è¶³")
                return None
                
        except Exception as e:
            print(f"âŒ æ–‡æœ¬æå–å¤±è´¥: {e}")
            return None
    
    def split_text_into_chunks(self, text: str, chunk_size: int = None) -> List[str]:
        """å°†é•¿æ–‡æœ¬åˆ†å‰²æˆå—ï¼Œä¾¿äºAIå¤„ç†"""
        if chunk_size is None:
            chunk_size = int(MAX_INPUT_TOKENS * 3.2)
            
        if len(text) <= chunk_size:
            return [text]
        
        chunks = []
        start = 0
        
        while start < len(text):
            end = start + chunk_size
            
            # å°è¯•åœ¨å¥å·å¤„åˆ†å‰²ï¼Œé¿å…æˆªæ–­å¥å­
            if end < len(text):
                sentence_end = text.rfind('.', start, end)
                if sentence_end > start + chunk_size // 2:
                    end = sentence_end + 1
            
            chunk = text[start:end].strip()
            if chunk:
                chunks.append(chunk)
            
            start = end
        
        print(f"ğŸ“„ åˆ†å‰²ä¸º {len(chunks)} ä¸ªæ–‡æœ¬å—")
        return chunks
    
    def get_text_summary(self, paper: Dict, max_chars: int = 2000) -> str:
        """è·å–è®ºæ–‡æ–‡æœ¬çš„æ‘˜è¦ç‰ˆæœ¬ï¼Œç”¨äºå¿«é€Ÿé¢„è§ˆ"""
        full_text = paper.get('extracted_text', '')
        
        if len(full_text) <= max_chars:
            return full_text
        
        summary = full_text[:max_chars]
        
        last_period = summary.rfind('.')
        if last_period > max_chars // 2:
            summary = summary[:last_period + 1]
        
        return summary + f"\n\n[... å·²æˆªæ–­ï¼Œæ€»å…± {len(full_text):,} å­—ç¬¦]"