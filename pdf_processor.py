import requests
import fitz  # PyMuPDF
from pathlib import Path
from typing import Optional, Dict, List, Set
import time
import re
from urllib.parse import urljoin, urlparse
from bs4 import BeautifulSoup
import arxiv
import random

from config import (
    DOWNLOAD_DIR, 
    EXTRACT_FULL_PDF, 
    MAX_PAGES_TO_EXTRACT, 
    MAX_INPUT_TOKENS,
    MAX_TEXT_LENGTH,
    PDF_CHUNK_SIZE
)

class EnhancedPDFProcessor:
    def __init__(self):
        self.download_dir = Path(DOWNLOAD_DIR)
        self.download_dir.mkdir(exist_ok=True)
        
        # ğŸ”§ å¢å¼ºçš„è¯·æ±‚ä¼šè¯é…ç½®ï¼Œæ›´å¥½åœ°æ¨¡æ‹ŸçœŸå®æµè§ˆå™¨
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7',
            'Accept-Language': 'en-US,en;q=0.9',
            'Accept-Encoding': 'gzip, deflate, br',
            'DNT': '1',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
            'Sec-Fetch-Dest': 'document',
            'Sec-Fetch-Mode': 'navigate',
            'Sec-Fetch-Site': 'none',
            'Sec-Fetch-User': '?1',
            'Cache-Control': 'max-age=0'
        })
        
        # æ”¯æŒçš„PDFåŸŸåå’Œå…¶ç‰¹æ®Šå¤„ç†æ–¹æ³•
        self.pdf_handlers = {
            'arxiv.org': self._handle_arxiv_pdf,
            'researchgate.net': self._handle_researchgate_pdf,
            'academia.edu': self._handle_academia_pdf,
            'semanticscholar.org': self._handle_semantic_scholar_pdf,
            'ieee.org': self._handle_ieee_pdf,
            'acm.org': self._handle_acm_pdf,
            'sciencedirect.com': self._handle_sciencedirect_pdf,
            'springer.com': self._handle_springer_pdf,
            'nature.com': self._handle_nature_pdf,
        }
        
        # ğŸ†• æ·»åŠ ä¸‹è½½çŠ¶æ€è·Ÿè¸ªï¼Œé˜²æ­¢æ— é™å¾ªç¯
        self._attempted_urls: Set[str] = set()
        self._max_recursion_depth = 3
        
        print(f"ğŸ”§ Enhanced PDF Processor åˆå§‹åŒ–å®Œæˆ")
        print(f"   - æ”¯æŒ {len(self.pdf_handlers)} ç§ä¸“é—¨çš„PDFå¤„ç†å™¨")
        print(f"   - å¢å¼ºçš„æµè§ˆå™¨æ¨¡æ‹Ÿ")
        print(f"   - å¤šé‡å¤‡ç”¨ä¸‹è½½æœºåˆ¶")
        print(f"   - ğŸ†• æ·»åŠ å¾ªç¯ä¿æŠ¤å’Œé“¾æ¥éªŒè¯")
    
    def process_paper(self, paper: Dict, download_dir: str) -> Optional[Dict]:
        """
        å¤„ç†å•ç¯‡è®ºæ–‡ï¼šä¸‹è½½+æå– (å¢å¼ºç‰ˆ)
        """
        title = paper.get('title', 'Unknown')
        print(f"ğŸ” æ­£åœ¨å¤„ç†è®ºæ–‡: {title}")
        
        # ğŸ†• é‡ç½®ä¸‹è½½çŠ¶æ€è·Ÿè¸ªï¼ˆæ¯ç¯‡è®ºæ–‡é‡æ–°å¼€å§‹ï¼‰
        self._attempted_urls.clear()
        
        # ğŸ¯ å¤šé‡PDFè·å–ç­–ç•¥
        pdf_path = self._get_pdf_with_enhanced_strategies(paper, download_dir)
        
        if pdf_path:
            text = self.extract_text(pdf_path)
            if text:
                paper['local_path'] = str(pdf_path)
                paper['extracted_text'] = text
                paper['text_length'] = len(text)
                
                if len(text) > PDF_CHUNK_SIZE:
                    paper['text_chunks'] = self.split_text_into_chunks(text)
                    print(f"ğŸ“š è®ºæ–‡å¤„ç†å®Œæˆï¼Œåˆ†ä¸º {len(paper['text_chunks'])} ä¸ªæ–‡æœ¬å—")
                else:
                    paper['text_chunks'] = [text]
                    print(f"ğŸ“š è®ºæ–‡å¤„ç†å®Œæˆï¼Œå•ä¸ªæ–‡æœ¬å—")
                
                return paper
            else:
                print("âŒ æ–‡æœ¬æå–å¤±è´¥")
        else:
            print("âŒ PDFä¸‹è½½å¤±è´¥")
        
        # Fallbackï¼šä½¿ç”¨æ‘˜è¦
        if paper.get('abstract'):
            print("ğŸ“ ä½¿ç”¨æ‘˜è¦ä½œä¸ºfallback")
            paper['extracted_text'] = paper['abstract']
            paper['text_length'] = len(paper['abstract'])
            paper['text_chunks'] = [paper['abstract']]
            paper['local_path'] = None
            return paper
        
        return None
    
    def _is_valid_url(self, url: str) -> bool:
        """ğŸ†• éªŒè¯URLæ˜¯å¦æœ‰æ•ˆä¸”å¯ç”¨äºä¸‹è½½"""
        if not url or not isinstance(url, str):
            return False
        
        url = url.strip()
        
        # è¿‡æ»¤æ˜æ˜¾æ— æ•ˆçš„é“¾æ¥
        invalid_patterns = [
            'javascript:',
            'mailto:',
            '#',
            'tel:',
            'void(0)',
            'return false',
            'onclick',
        ]
        
        url_lower = url.lower()
        for pattern in invalid_patterns:
            if pattern in url_lower:
                return False
        
        # æ£€æŸ¥æ˜¯å¦æ˜¯ç›¸å¯¹URLï¼ˆç›¸å¯¹URLéœ€è¦base_urlæ‰èƒ½å·¥ä½œï¼‰
        if url.startswith('//'):
            return True
        elif url.startswith('/'):
            return True  # ç›¸å¯¹è·¯å¾„ï¼Œéœ€è¦base_url
        elif url.startswith(('http://', 'https://')):
            return True
        elif url.startswith('ftp://'):
            return True
        else:
            return False  # å…¶ä»–æƒ…å†µè®¤ä¸ºæ— æ•ˆ
    
    def _normalize_url(self, url: str) -> str:
        """ğŸ†• æ ‡å‡†åŒ–URLç”¨äºå»é‡"""
        # ç§»é™¤fragment
        if '#' in url:
            url = url.split('#')[0]
        
        # ç§»é™¤å¤šä½™çš„å‚æ•°ï¼ˆä¿ç•™é‡è¦å‚æ•°ï¼‰
        try:
            parsed = urlparse(url)
            # è¿™é‡Œå¯ä»¥æ ¹æ®éœ€è¦è¿‡æ»¤queryå‚æ•°
            return f"{parsed.scheme}://{parsed.netloc}{parsed.path}"
        except:
            return url
    
    def _get_pdf_with_enhanced_strategies(self, paper: Dict, download_dir: str) -> Optional[Path]:
        """å¢å¼ºçš„PDFè·å–ç­–ç•¥"""
        title = paper.get('title', 'Unknown')
        safe_title = self._generate_safe_filename(title)
        
        print(f"  ğŸ¯ å¼€å§‹å¢å¼ºPDFè·å–ç­–ç•¥...")
        
        # ç­–ç•¥1: ä¼˜å…ˆå¤„ç†å·²çŸ¥çš„é«˜æˆåŠŸç‡é“¾æ¥ï¼ˆarXivç­‰ï¼‰
        pdf_links = paper.get('pdf_links', [])
        if paper.get('pdf_url'):
            pdf_links = [paper['pdf_url']] + pdf_links
        
        prioritized_links = self._prioritize_pdf_links(pdf_links)
        
        print(f"  ğŸ“‹ æ‰¾åˆ° {len(prioritized_links)} ä¸ªPDFé“¾æ¥ï¼ŒæŒ‰ä¼˜å…ˆçº§æ’åº")
        
        # ç­–ç•¥2: æŒ‰ä¼˜å…ˆçº§å°è¯•ä¸‹è½½
        for i, link in enumerate(prioritized_links):
            print(f"    å°è¯•é“¾æ¥ {i+1}/{len(prioritized_links)}: {self._truncate_url_for_display(link)}")
            
            # ğŸ†• æ£€æŸ¥é“¾æ¥æœ‰æ•ˆæ€§
            if not self._is_valid_url(link):
                print(f"      âŒ æ— æ•ˆé“¾æ¥ï¼Œè·³è¿‡")
                continue
            
            # ğŸ†• æ£€æŸ¥æ˜¯å¦å·²å°è¯•è¿‡æ­¤é“¾æ¥
            normalized_link = self._normalize_url(link)
            if normalized_link in self._attempted_urls:
                print(f"      âš ï¸ å·²å°è¯•è¿‡æ­¤é“¾æ¥ï¼Œè·³è¿‡")
                continue
            
            # æ ¹æ®åŸŸåé€‰æ‹©ä¸“é—¨çš„å¤„ç†å™¨
            domain = self._extract_domain(link)
            if domain in self.pdf_handlers:
                print(f"      ä½¿ç”¨ä¸“é—¨å¤„ç†å™¨: {domain}")
                pdf_path = self.pdf_handlers[domain](link, safe_title, download_dir)
            else:
                print(f"      ä½¿ç”¨é€šç”¨å¤„ç†å™¨")
                pdf_path = self._download_from_url_enhanced(link, safe_title, download_dir, recursion_depth=0)
            
            if pdf_path:
                print(f"    âœ… æˆåŠŸä¸‹è½½: {domain}")
                return pdf_path
            
            # æ·»åŠ å»¶è¿Ÿé¿å…è¢«é™åˆ¶
            time.sleep(random.uniform(1, 2))
        
        # ç­–ç•¥3: å¦‚æœæœ‰Google Scholaré¡µé¢ï¼Œå°è¯•æ·±åº¦è§£æ
        if paper.get('source') == 'google_scholar' and paper.get('paper_url'):
            print(f"  ğŸ” ç­–ç•¥3: æ·±åº¦è§£æGoogle Scholaré¡µé¢")
            pdf_path = self._deep_parse_scholar_page(paper['paper_url'], safe_title, download_dir)
            if pdf_path:
                return pdf_path
        
        # ç­–ç•¥4: æ™ºèƒ½æœç´¢å¤‡ç”¨æº
        print(f"  ğŸ” ç­–ç•¥4: æ™ºèƒ½æœç´¢å¤‡ç”¨PDFæº")
        pdf_path = self._intelligent_search_fallback(title, safe_title, download_dir)
        if pdf_path:
            return pdf_path
        
        print(f"  âŒ æ‰€æœ‰PDFè·å–ç­–ç•¥éƒ½å¤±è´¥äº†")
        return None
    
    def _prioritize_pdf_links(self, links: List[str]) -> List[str]:
        """æŒ‰æˆåŠŸç‡å¯¹PDFé“¾æ¥æ’åº"""
        def link_priority(link):
            link_lower = link.lower()
            
            # ğŸ†• é¦–å…ˆè¿‡æ»¤æ— æ•ˆé“¾æ¥
            if not self._is_valid_url(link):
                return 999  # æœ€ä½ä¼˜å…ˆçº§
            
            # arXiv - æœ€é«˜ä¼˜å…ˆçº§
            if 'arxiv.org' in link_lower:
                return 1
            
            # å¼€æ”¾è·å–å¹³å°
            if any(domain in link_lower for domain in ['researchgate.net', 'academia.edu', 'semanticscholar.org']):
                return 2
            
            # ç›´æ¥PDFæ–‡ä»¶
            if '.pdf' in link_lower:
                return 3
            
            # PMCç­‰å¼€æ”¾æœŸåˆŠ
            if any(domain in link_lower for domain in ['ncbi.nlm.nih.gov', 'pubmed', 'biorxiv', 'medrxiv']):
                return 4
            
            # æœºæ„ä»“åº“
            if any(keyword in link_lower for keyword in ['repository', 'dspace', 'eprints']):
                return 5
            
            # å‡ºç‰ˆå•†ï¼ˆæˆåŠŸç‡ç›¸å¯¹è¾ƒä½ï¼‰
            if any(domain in link_lower for domain in ['ieee.org', 'acm.org', 'springer.com']):
                return 6
            
            # ScienceDirectç­‰ï¼ˆæˆåŠŸç‡æœ€ä½ï¼‰
            if 'sciencedirect.com' in link_lower:
                return 7
            
            return 8
        
        # ğŸ†• å…ˆè¿‡æ»¤æœ‰æ•ˆé“¾æ¥ï¼Œå†æ’åºå»é‡
        valid_links = [link for link in links if self._is_valid_url(link)]
        return sorted(set(valid_links), key=link_priority)
    
    def _extract_domain(self, url: str) -> str:
        """æå–URLçš„åŸŸå"""
        try:
            parsed = urlparse(url)
            domain = parsed.netloc.lower()
            # ç§»é™¤wwwå‰ç¼€
            if domain.startswith('www.'):
                domain = domain[4:]
            return domain
        except:
            return ""
    
    def _truncate_url_for_display(self, url: str, max_length: int = 80) -> str:
        """æˆªæ–­URLç”¨äºæ˜¾ç¤º (ä¿ç•™å®Œæ•´åŸŸåä¿¡æ¯)"""
        if len(url) <= max_length:
            return url
        
        # å°è¯•ä¿ç•™é‡è¦éƒ¨åˆ†
        parsed = urlparse(url)
        domain = parsed.netloc
        path = parsed.path
        
        if len(domain) + 10 < max_length:
            # æ˜¾ç¤ºåŸŸå + è·¯å¾„çš„å¼€å§‹éƒ¨åˆ†
            remaining = max_length - len(domain) - 10
            if len(path) > remaining:
                return f"{domain}{path[:remaining]}..."
            else:
                return f"{domain}{path}..."
        else:
            # åŸŸåå¤ªé•¿ï¼Œç›´æ¥æˆªæ–­
            return url[:max_length-3] + "..."
    
    # ğŸ¯ ä¸“é—¨çš„PDFå¤„ç†å™¨
    
    def _handle_arxiv_pdf(self, url: str, filename: str, download_dir: str) -> Optional[Path]:
        """arXiv PDFå¤„ç†å™¨"""
        try:
            # ç¡®ä¿ä½¿ç”¨PDF URL
            if '/abs/' in url:
                pdf_url = url.replace('/abs/', '/pdf/') + '.pdf'
            elif '/pdf/' in url and not url.endswith('.pdf'):
                pdf_url = url + '.pdf'
            else:
                pdf_url = url
            
            return self._download_from_url_enhanced(pdf_url, filename, download_dir, recursion_depth=0)
            
        except Exception as e:
            print(f"      âŒ arXivå¤„ç†å¤±è´¥: {e}")
            return None
    
    def _handle_researchgate_pdf(self, url: str, filename: str, download_dir: str) -> Optional[Path]:
        """ResearchGate PDFå¤„ç†å™¨"""
        try:
            # ResearchGateé€šå¸¸éœ€è¦ç‰¹æ®Šå¤„ç†ï¼Œå…ˆå°è¯•ç›´æ¥ä¸‹è½½
            return self._download_from_url_enhanced(url, filename, download_dir, recursion_depth=0)
            
        except Exception as e:
            print(f"      âŒ ResearchGateå¤„ç†å¤±è´¥: {e}")
            return None
    
    def _handle_academia_pdf(self, url: str, filename: str, download_dir: str) -> Optional[Path]:
        """Academia.edu PDFå¤„ç†å™¨"""
        try:
            return self._download_from_url_enhanced(url, filename, download_dir, recursion_depth=0)
            
        except Exception as e:
            print(f"      âŒ Academia.eduå¤„ç†å¤±è´¥: {e}")
            return None
    
    def _handle_semantic_scholar_pdf(self, url: str, filename: str, download_dir: str) -> Optional[Path]:
        """Semantic Scholar PDFå¤„ç†å™¨"""
        try:
            return self._download_from_url_enhanced(url, filename, download_dir, recursion_depth=0)
            
        except Exception as e:
            print(f"      âŒ Semantic Scholarå¤„ç†å¤±è´¥: {e}")
            return None
    
    def _handle_ieee_pdf(self, url: str, filename: str, download_dir: str) -> Optional[Path]:
        """IEEE PDFå¤„ç†å™¨"""
        try:
            # IEEEçš„PDFé€šå¸¸éœ€è¦ç‰¹æ®ŠURLæ ¼å¼
            if '/document/' in url:
                doc_id = re.search(r'/document/(\d+)', url)
                if doc_id:
                    pdf_url = f"https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber={doc_id.group(1)}"
                    return self._download_from_url_enhanced(pdf_url, filename, download_dir, recursion_depth=0)
            
            return self._download_from_url_enhanced(url, filename, download_dir, recursion_depth=0)
            
        except Exception as e:
            print(f"      âŒ IEEEå¤„ç†å¤±è´¥: {e}")
            return None
    
    def _handle_acm_pdf(self, url: str, filename: str, download_dir: str) -> Optional[Path]:
        """ACM PDFå¤„ç†å™¨"""
        try:
            # ACMçš„PDFé“¾æ¥æ¨¡å¼
            if '/doi/' in url and '/pdf/' not in url:
                pdf_url = url.replace('/doi/', '/doi/pdf/')
                return self._download_from_url_enhanced(pdf_url, filename, download_dir, recursion_depth=0)
            
            return self._download_from_url_enhanced(url, filename, download_dir, recursion_depth=0)
            
        except Exception as e:
            print(f"      âŒ ACMå¤„ç†å¤±è´¥: {e}")
            return None
    
    def _handle_sciencedirect_pdf(self, url: str, filename: str, download_dir: str) -> Optional[Path]:
        """ScienceDirect PDFå¤„ç†å™¨ï¼ˆå¢å¼ºæŠ—åçˆ¬è™«ï¼‰"""
        try:
            # ScienceDirectéœ€è¦ç‰¹æ®Šçš„è¯·æ±‚å¤´
            special_headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
                'Accept-Language': 'en-US,en;q=0.5',
                'Accept-Encoding': 'gzip, deflate, br',
                'Referer': 'https://www.google.com/',  # é‡è¦ï¼šæ·»åŠ Googleå¼•è
                'DNT': '1',
                'Connection': 'keep-alive',
                'Upgrade-Insecure-Requests': '1',
                'Sec-Fetch-Dest': 'document',
                'Sec-Fetch-Mode': 'navigate',
                'Sec-Fetch-Site': 'cross-site',
            }
            
            # å°è¯•è½¬æ¢ä¸ºPDF URL
            if '/pii/' in url:
                pdf_url = url.replace('/pii/', '/pdf/') + '.pdf'
                return self._download_with_special_headers(pdf_url, filename, download_dir, special_headers)
            
            return self._download_with_special_headers(url, filename, download_dir, special_headers)
            
        except Exception as e:
            print(f"      âŒ ScienceDirectå¤„ç†å¤±è´¥: {e}")
            return None
    
    def _handle_springer_pdf(self, url: str, filename: str, download_dir: str) -> Optional[Path]:
        """Springer PDFå¤„ç†å™¨"""
        try:
            return self._download_from_url_enhanced(url, filename, download_dir, recursion_depth=0)
            
        except Exception as e:
            print(f"      âŒ Springerå¤„ç†å¤±è´¥: {e}")
            return None
    
    def _handle_nature_pdf(self, url: str, filename: str, download_dir: str) -> Optional[Path]:
        """Nature PDFå¤„ç†å™¨"""
        try:
            return self._download_from_url_enhanced(url, filename, download_dir, recursion_depth=0)
            
        except Exception as e:
            print(f"      âŒ Natureå¤„ç†å¤±è´¥: {e}")
            return None
    
    def _download_with_special_headers(self, url: str, filename: str, download_dir: str, headers: Dict) -> Optional[Path]:
        """ä½¿ç”¨ç‰¹æ®Šè¯·æ±‚å¤´ä¸‹è½½"""
        if not url:
            return None
            
        try:
            if not url.startswith(('http://', 'https://')):
                url = 'https://' + url
            
            # ğŸ†• è®°å½•å°è¯•çš„URL
            normalized_url = self._normalize_url(url)
            self._attempted_urls.add(normalized_url)
            
            download_dir_path = Path(download_dir)
            file_path = download_dir_path / f"{filename}.pdf"
            
            if file_path.exists() and file_path.stat().st_size > 1024:
                print(f"        âœ… æ–‡ä»¶å·²å­˜åœ¨")
                return file_path
            
            # åˆ›å»ºç‰¹æ®Šä¼šè¯
            special_session = requests.Session()
            special_session.headers.update(headers)
            
            print(f"        ğŸ“¥ ä½¿ç”¨ç‰¹æ®Šè¯·æ±‚å¤´ä¸‹è½½...")
            
            response = special_session.get(url, timeout=30, stream=True)
            response.raise_for_status()
            
            content_type = response.headers.get('content-type', '').lower()
            
            if 'application/pdf' in content_type:
                with open(file_path, 'wb') as f:
                    for chunk in response.iter_content(chunk_size=8192):
                        if chunk:
                            f.write(chunk)
                
                file_size = file_path.stat().st_size
                if file_size > 1024:
                    print(f"        âœ… ä¸‹è½½æˆåŠŸ: {file_size/1024:.1f} KB")
                    return file_path
                else:
                    file_path.unlink()
                    return None
            else:
                print(f"        âš ï¸ ä¸æ˜¯PDFæ–‡ä»¶: {content_type}")
                return None
            
        except Exception as e:
            print(f"        âŒ ç‰¹æ®Šä¸‹è½½å¤±è´¥: {e}")
            return None
    
    def _download_from_url_enhanced(self, url: str, filename: str, download_dir: str, recursion_depth: int = 0) -> Optional[Path]:
        """ğŸ†• å¢å¼ºçš„URLä¸‹è½½æ–¹æ³•ï¼ˆæ·»åŠ é€’å½’æ·±åº¦é™åˆ¶ï¼‰"""
        if not url:
            return None
        
        # ğŸ†• æ£€æŸ¥é€’å½’æ·±åº¦
        if recursion_depth >= self._max_recursion_depth:
            print(f"        âš ï¸ è¾¾åˆ°æœ€å¤§é€’å½’æ·±åº¦ {self._max_recursion_depth}ï¼Œåœæ­¢å°è¯•")
            return None
        
        # ğŸ†• éªŒè¯URL
        if not self._is_valid_url(url):
            print(f"        âŒ æ— æ•ˆURL: {url}")
            return None
        
        try:
            if not url.startswith(('http://', 'https://')):
                url = 'https://' + url
            
            # ğŸ†• è®°å½•å°è¯•çš„URL
            normalized_url = self._normalize_url(url)
            if normalized_url in self._attempted_urls:
                print(f"        âš ï¸ å·²å°è¯•è¿‡æ­¤URLï¼Œè·³è¿‡")
                return None
            self._attempted_urls.add(normalized_url)
            
            download_dir_path = Path(download_dir)
            file_path = download_dir_path / f"{filename}.pdf"
            
            if file_path.exists() and file_path.stat().st_size > 1024:
                print(f"        âœ… æ–‡ä»¶å·²å­˜åœ¨")
                return file_path
            
            print(f"        ğŸ“¥ ä¸‹è½½ä¸­... (æ·±åº¦: {recursion_depth})")
            
            # å¢åŠ éšæœºå»¶è¿Ÿ
            time.sleep(random.uniform(0.5, 1.5))
            
            response = self.session.get(url, timeout=30, stream=True)
            response.raise_for_status()
            
            content_type = response.headers.get('content-type', '').lower()
            
            if 'application/pdf' in content_type:
                # ç›´æ¥æ˜¯PDF
                with open(file_path, 'wb') as f:
                    for chunk in response.iter_content(chunk_size=8192):
                        if chunk:
                            f.write(chunk)
                
                file_size = file_path.stat().st_size
                if file_size > 1024:
                    print(f"        âœ… ä¸‹è½½æˆåŠŸ: {file_size/1024:.1f} KB")
                    return file_path
                else:
                    file_path.unlink()
                    return None
            
            elif 'text/html' in content_type:
                # HTMLé¡µé¢ï¼Œå°è¯•è§£æPDFé“¾æ¥
                print(f"        ğŸ” è§£æHTMLé¡µé¢ä¸­çš„PDFé“¾æ¥... (æ·±åº¦: {recursion_depth})")
                soup = BeautifulSoup(response.content, 'html.parser')
                
                pdf_links = self._extract_pdf_links_from_html(soup, url)
                
                # ğŸ†• é™åˆ¶å°è¯•çš„é“¾æ¥æ•°é‡ï¼Œå¹¶é€’å¢æ·±åº¦
                max_attempts = max(1, 5 - recursion_depth)  # éšæ·±åº¦å‡å°‘å°è¯•æ¬¡æ•°
                for i, pdf_link in enumerate(pdf_links[:max_attempts]):
                    print(f"        ğŸ“¥ å°è¯•HTMLä¸­çš„PDFé“¾æ¥ {i+1}/{min(len(pdf_links), max_attempts)}... (æ·±åº¦: {recursion_depth})")
                    
                    # ğŸ†• é€’å½’è°ƒç”¨æ—¶å¢åŠ æ·±åº¦
                    pdf_path = self._download_from_url_enhanced(pdf_link, filename, download_dir, recursion_depth + 1)
                    if pdf_path:
                        return pdf_path
                    
                    # ğŸ†• æ·»åŠ å»¶è¿Ÿé¿å…è¿‡å¿«è¯·æ±‚
                    time.sleep(0.5)
            
            return None
            
        except Exception as e:
            print(f"        âŒ ä¸‹è½½å¤±è´¥: {e}")
            return None
    
    def _extract_pdf_links_from_html(self, soup: BeautifulSoup, base_url: str) -> List[str]:
        """ğŸ†• ä»HTMLä¸­æå–PDFé“¾æ¥ï¼ˆå¢å¼ºéªŒè¯ï¼‰"""
        pdf_links = []
        
        # æŸ¥æ‰¾æ˜ç¡®çš„PDFé“¾æ¥
        for link in soup.find_all('a', href=True):
            href = link['href']
            text = link.get_text().lower()
            
            # ğŸ†• å…ˆéªŒè¯é“¾æ¥æœ‰æ•ˆæ€§
            if not self._is_valid_url(href):
                continue
            
            # æ£€æŸ¥é“¾æ¥å’Œæ–‡æœ¬
            is_pdf_link = False
            
            # ç›´æ¥PDFæ–‡ä»¶é“¾æ¥
            if '.pdf' in href.lower():
                is_pdf_link = True
            
            # åŸºäºæ–‡æœ¬å†…å®¹åˆ¤æ–­
            elif any(keyword in text for keyword in ['download', 'pdf', 'full text', 'view pdf']):
                # ğŸ†• è¿›ä¸€æ­¥éªŒè¯ï¼šç¡®ä¿ä¸æ˜¯å¯¼èˆªé“¾æ¥
                if not any(nav_keyword in text for nav_keyword in ['menu', 'navigation', 'home', 'about', 'contact']):
                    is_pdf_link = True
            
            if is_pdf_link:
                try:
                    full_url = urljoin(base_url, href)
                    # ğŸ†• å†æ¬¡éªŒè¯å®Œæ•´URL
                    if self._is_valid_url(full_url):
                        pdf_links.append(full_url)
                except Exception:
                    continue
        
        # æŸ¥æ‰¾ç‰¹æ®Šçš„PDFæŒ‰é’®æˆ–é“¾æ¥
        pdf_selectors = [
            'a[href*=".pdf"]',
            'a[href*="download"]',
            '.pdf-download a',
            '.download-pdf a',
            '.full-text a'
        ]
        
        for selector in pdf_selectors:
            try:
                elements = soup.select(selector)
                for element in elements:
                    href = element.get('href')
                    if href and self._is_valid_url(href):
                        try:
                            full_url = urljoin(base_url, href)
                            if self._is_valid_url(full_url):
                                pdf_links.append(full_url)
                        except Exception:
                            continue
            except Exception:
                continue
        
        # ğŸ†• å»é‡å¹¶éªŒè¯æ‰€æœ‰é“¾æ¥
        unique_valid_links = []
        seen_normalized = set()
        
        for link in pdf_links:
            if self._is_valid_url(link):
                normalized = self._normalize_url(link)
                if normalized not in seen_normalized and normalized not in self._attempted_urls:
                    unique_valid_links.append(link)
                    seen_normalized.add(normalized)
        
        print(f"          æ‰¾åˆ° {len(unique_valid_links)} ä¸ªæœ‰æ•ˆPDFé“¾æ¥")
        return unique_valid_links
    
    def _deep_parse_scholar_page(self, scholar_url: str, filename: str, download_dir: str) -> Optional[Path]:
        """æ·±åº¦è§£æGoogle Scholaré¡µé¢"""
        try:
            print(f"    ğŸ” æ·±åº¦è§£æScholaré¡µé¢...")
            
            # ğŸ†• æ£€æŸ¥æ˜¯å¦å·²å°è¯•è¿‡
            normalized_url = self._normalize_url(scholar_url)
            if normalized_url in self._attempted_urls:
                print(f"      âš ï¸ å·²å°è¯•è¿‡æ­¤Scholaré¡µé¢ï¼Œè·³è¿‡")
                return None
            self._attempted_urls.add(normalized_url)
            
            # å¢åŠ éšæœºå»¶è¿Ÿ
            time.sleep(random.uniform(2, 4))
            
            response = self.session.get(scholar_url, timeout=15)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # æ›´å…¨é¢çš„PDFé“¾æ¥æœç´¢
            pdf_links = []
            
            # æ–¹æ³•1: æŸ¥æ‰¾ç›´æ¥PDFé“¾æ¥
            for link in soup.find_all('a', href=True):
                href = link['href']
                text = link.get_text().lower()
                
                if (self._is_potential_pdf_link(href, text)):
                    full_url = urljoin(scholar_url, href)
                    if self._is_valid_url(full_url):
                        pdf_links.append(full_url)
            
            # æ–¹æ³•2: æŸ¥æ‰¾ç‰¹æ®Šçš„Scholarå…ƒç´ 
            scholar_pdf_elements = soup.find_all(['div', 'span'], class_=re.compile(r'gs_or|gs_fl|gs_ggs'))
            for element in scholar_pdf_elements:
                links = element.find_all('a', href=True)
                for link in links:
                    href = link['href']
                    if self._is_potential_pdf_link(href, link.get_text().lower()):
                        full_url = urljoin(scholar_url, href)
                        if self._is_valid_url(full_url):
                            pdf_links.append(full_url)
            
            # ğŸ†• å»é‡å¹¶éªŒè¯
            unique_links = []
            seen = set()
            for link in pdf_links:
                normalized = self._normalize_url(link)
                if normalized not in seen and normalized not in self._attempted_urls:
                    unique_links.append(link)
                    seen.add(normalized)
            
            # å°è¯•ä¸‹è½½æ‰¾åˆ°çš„é“¾æ¥
            for i, pdf_link in enumerate(unique_links[:3]):  # ğŸ†• é™åˆ¶å°è¯•æ¬¡æ•°
                print(f"        ğŸ“¥ å°è¯•Scholarè§£æé“¾æ¥ {i+1}/{min(len(unique_links), 3)}...")
                pdf_path = self._download_from_url_enhanced(pdf_link, filename, download_dir, recursion_depth=1)
                if pdf_path:
                    return pdf_path
                time.sleep(1)
            
            return None
            
        except Exception as e:
            print(f"    âŒ Scholaræ·±åº¦è§£æå¤±è´¥: {e}")
            return None
    
    def _is_potential_pdf_link(self, href: str, text: str) -> bool:
        """åˆ¤æ–­æ˜¯å¦ä¸ºæ½œåœ¨çš„PDFé“¾æ¥"""
        # ğŸ†• é¦–å…ˆæ£€æŸ¥åŸºæœ¬æœ‰æ•ˆæ€§
        if not self._is_valid_url(href):
            return False
        
        href_lower = href.lower()
        text_lower = text.lower()
        
        # æ˜ç¡®çš„PDFæŒ‡æ ‡
        if '.pdf' in href_lower:
            return True
        
        # å·²çŸ¥çš„PDFåŸŸå
        pdf_domains = ['arxiv.org', 'researchgate.net', 'academia.edu', 'semanticscholar.org']
        if any(domain in href_lower for domain in pdf_domains):
            return True
        
        # PDFç›¸å…³çš„æ–‡æœ¬ï¼ˆğŸ†• æ›´ä¸¥æ ¼çš„åˆ¤æ–­ï¼‰
        pdf_keywords = ['pdf', 'download', 'full text', 'view paper', 'get pdf']
        if any(keyword in text_lower for keyword in pdf_keywords):
            # ğŸ†• æ’é™¤å¯¼èˆªå’Œæ— å…³é“¾æ¥
            exclusion_keywords = ['menu', 'navigation', 'home', 'about', 'contact', 'login', 'signup']
            if not any(keyword in text_lower for keyword in exclusion_keywords):
                return True
        
        # ä¸‹è½½ç±»å…³é”®è¯ï¼ˆğŸ†• æ›´ä¸¥æ ¼ï¼‰
        if 'download' in href_lower and len(text_lower) < 50:  # é¿å…é•¿æ–‡æœ¬é“¾æ¥
            return True
        
        return False
    
    def _intelligent_search_fallback(self, title: str, filename: str, download_dir: str) -> Optional[Path]:
        """æ™ºèƒ½å¤šæºPDFæœç´¢å¤‡ç”¨æœºåˆ¶"""
        print(f"    ğŸ” å¯åŠ¨æ™ºèƒ½å¤šæºPDFæœç´¢...")
        
        # æ¸…ç†æ ‡é¢˜ç”¨äºæœç´¢
        clean_title = re.sub(r'[^\w\s]', ' ', title).strip()
        search_words = clean_title.split()[:8]  # å–æ›´å¤šå…³é”®è¯æé«˜åŒ¹é…ç‡
        
        if len(search_words) < 2:
            print(f"      âš ï¸ æ ‡é¢˜å…³é”®è¯ä¸è¶³ï¼Œè·³è¿‡å¤šæºæœç´¢")
            return None
        
        search_query = ' '.join(search_words)
        print(f"      ğŸ” æœç´¢æŸ¥è¯¢: {search_query}")
        
        # ç­–ç•¥1: arXivæœç´¢ï¼ˆæœ€å¯é çš„PDFæºï¼‰
        pdf_path = self._search_arxiv_for_pdf(search_query, title, filename, download_dir)
        if pdf_path:
            return pdf_path
        
        # ç­–ç•¥2: scholarlyåº“æœç´¢
        pdf_path = self._search_scholarly_for_pdf(search_query, title, filename, download_dir)
        if pdf_path:
            return pdf_path
        
        # ç­–ç•¥3: DBLPæœç´¢ï¼ˆé€šè¿‡DOIé“¾æ¥ï¼‰
        pdf_path = self._search_dblp_for_pdf(search_query, title, filename, download_dir)
        if pdf_path:
            return pdf_path
        
        print(f"    âŒ æ‰€æœ‰å¤šæºPDFæœç´¢éƒ½å¤±è´¥äº†")
        return None
    
    def _search_arxiv_for_pdf(self, search_query: str, original_title: str, filename: str, download_dir: str) -> Optional[Path]:
        """ä»arXivæœç´¢ç›¸åŒè®ºæ–‡çš„PDF"""
        try:
            print(f"      ğŸ“š åœ¨arXivä¸­æœç´¢ç›¸åŒè®ºæ–‡...")
            
            import arxiv
            search = arxiv.Search(
                query=search_query,
                max_results=10,  # å¢åŠ æœç´¢ç»“æœæ•°é‡
                sort_by=arxiv.SortCriterion.Relevance
            )
            
            for i, result in enumerate(search.results()):
                similarity = self._title_similarity(original_title, result.title)
                
                if similarity > 0.2:  # ç›¸å¯¹å®½æ¾çš„åŒ¹é…é˜ˆå€¼
                    print(f"        âœ… æ‰¾åˆ°åŒ¹é…è®ºæ–‡ï¼Œä¸‹è½½PDF...")
                    pdf_path = self._download_from_url_enhanced(result.pdf_url, filename, download_dir, recursion_depth=0)
                    if pdf_path:
                        print(f"        âœ… arXiv PDFä¸‹è½½æˆåŠŸ!")
                        return pdf_path
                    else:
                        print(f"        âŒ arXiv PDFä¸‹è½½å¤±è´¥")
            
            print(f"      âŒ arXivä¸­æœªæ‰¾åˆ°åŒ¹é…è®ºæ–‡")
            return None
            
        except Exception as e:
            print(f"      âŒ arXivæœç´¢å¤±è´¥: {e}")
            return None
    
    def _search_scholarly_for_pdf(self, search_query: str, original_title: str, filename: str, download_dir: str) -> Optional[Path]:
        """ä»scholarlyåº“æœç´¢ç›¸åŒè®ºæ–‡çš„PDF"""
        try:
            print(f"      ğŸ“š åœ¨scholarlyä¸­æœç´¢ç›¸åŒè®ºæ–‡...")
            
            # æ£€æŸ¥scholarlyæ˜¯å¦å¯ç”¨
            try:
                from scholarly import scholarly
            except ImportError:
                print(f"        âš ï¸ scholarlyåº“ä¸å¯ç”¨")
                return None
            
            search_results = scholarly.search_pubs(search_query)
            
            processed_count = 0
            for i, pub in enumerate(search_results):
                if processed_count >= 5:  # é™åˆ¶å¤„ç†æ•°é‡é¿å…è¶…æ—¶
                    break
                
                try:
                    bib = pub.get('bib', {})
                    candidate_title = bib.get('title', '')
                    
                    if not candidate_title:
                        continue
                    
                    similarity = self._title_similarity(original_title, candidate_title)
                    
                    if similarity > 0.4:  # scholarlyä½¿ç”¨ç¨é«˜çš„åŒ¹é…é˜ˆå€¼
                        print(f"        âœ… æ‰¾åˆ°åŒ¹é…è®ºæ–‡ï¼Œå°è¯•è·å–PDF...")
                        
                        # å°è¯•å¤šç§PDFè·å–æ–¹å¼
                        pdf_urls = []
                        
                        # æ–¹å¼1: ç›´æ¥PDFé“¾æ¥
                        if bib.get('eprint'):
                            pdf_urls.append(bib['eprint'])
                        
                        # æ–¹å¼2: è®ºæ–‡é¡µé¢é“¾æ¥
                        if bib.get('url'):
                            pdf_urls.append(bib['url'])
                        
                        # å°è¯•ä¸‹è½½æ‰¾åˆ°çš„PDFé“¾æ¥
                        for pdf_url in pdf_urls:
                            if self._is_valid_url(pdf_url):
                                print(f"          ğŸ“¥ å°è¯•PDFé“¾æ¥...")
                                pdf_path = self._download_from_url_enhanced(pdf_url, filename, download_dir, recursion_depth=0)
                                if pdf_path:
                                    print(f"        âœ… scholarly PDFä¸‹è½½æˆåŠŸ!")
                                    return pdf_path
                        
                        print(f"        âŒ scholarly PDFé“¾æ¥å‡ä¸‹è½½å¤±è´¥")
                    
                    processed_count += 1
                    
                except Exception as e:
                    print(f"        âš ï¸ å¤„ç†scholarlyç»“æœå‡ºé”™: {e}")
                    continue
                
                # æ·»åŠ å»¶è¿Ÿé¿å…è¢«é™åˆ¶
                time.sleep(0.5)
            
            print(f"      âŒ scholarlyä¸­æœªæ‰¾åˆ°åŒ¹é…è®ºæ–‡")
            return None
            
        except Exception as e:
            print(f"      âŒ scholarlyæœç´¢å¤±è´¥: {e}")
            return None
    
    def _search_dblp_for_pdf(self, search_query: str, original_title: str, filename: str, download_dir: str) -> Optional[Path]:
        """ä»DBLPæœç´¢ç›¸åŒè®ºæ–‡çš„PDFï¼ˆé€šè¿‡DOIç­‰ï¼‰"""
        try:
            print(f"      ğŸ“š åœ¨DBLPä¸­æœç´¢ç›¸åŒè®ºæ–‡...")
            
            import xml.etree.ElementTree as ET
            
            # DBLP APIæœç´¢
            dblp_url = "https://dblp.org/search/publ/api"
            params = {
                'q': search_query,
                'format': 'xml',
                'h': 10
            }
            
            response = self.session.get(dblp_url, params=params, timeout=15)
            response.raise_for_status()
            
            root = ET.fromstring(response.content)
            hits = root.findall('.//hit')
            
            for i, hit in enumerate(hits):
                try:
                    info = hit.find('info')
                    if info is None:
                        continue
                    
                    title_elem = info.find('title')
                    candidate_title = title_elem.text if title_elem is not None else ''
                    
                    if not candidate_title:
                        continue
                    
                    similarity = self._title_similarity(original_title, candidate_title)
                    
                    if similarity > 0.5:  # DBLPä½¿ç”¨è¾ƒé«˜çš„åŒ¹é…é˜ˆå€¼
                        print(f"        âœ… æ‰¾åˆ°åŒ¹é…è®ºæ–‡ï¼Œå°è¯•è·å–PDF...")
                        
                        # å°è¯•è·å–DOIé“¾æ¥
                        doi_elem = info.find('doi')
                        if doi_elem is not None and doi_elem.text:
                            doi = doi_elem.text
                            doi_url = f"https://doi.org/{doi}"
                            print(f"          ğŸ“¥ å°è¯•DOIé“¾æ¥: {doi}")
                            
                            pdf_path = self._download_from_url_enhanced(doi_url, filename, download_dir, recursion_depth=0)
                            if pdf_path:
                                print(f"        âœ… DBLP DOI PDFä¸‹è½½æˆåŠŸ!")
                                return pdf_path
                            else:
                                print(f"        âŒ DBLP DOI PDFä¸‹è½½å¤±è´¥")
                        
                        # å°è¯•å…¶ä»–é“¾æ¥
                        url_elem = info.find('url')
                        if url_elem is not None and url_elem.text:
                            paper_url = url_elem.text
                            if self._is_valid_url(paper_url):
                                print(f"          ğŸ“¥ å°è¯•è®ºæ–‡é¡µé¢é“¾æ¥...")
                                
                                pdf_path = self._download_from_url_enhanced(paper_url, filename, download_dir, recursion_depth=0)
                                if pdf_path:
                                    print(f"        âœ… DBLP é¡µé¢PDFä¸‹è½½æˆåŠŸ!")
                                    return pdf_path
                                else:
                                    print(f"        âŒ DBLP é¡µé¢PDFä¸‹è½½å¤±è´¥")
                
                except Exception as e:
                    print(f"        âš ï¸ å¤„ç†DBLPç»“æœå‡ºé”™: {e}")
                    continue
            
            print(f"      âŒ DBLPä¸­æœªæ‰¾åˆ°åŒ¹é…è®ºæ–‡")
            return None
            
        except Exception as e:
            print(f"      âŒ DBLPæœç´¢å¤±è´¥: {e}")
            return None
    
    def _title_similarity(self, title1: str, title2: str) -> float:
        """è®¡ç®—æ ‡é¢˜ç›¸ä¼¼åº¦"""
        def clean_title(title):
            return re.sub(r'[^\w\s]', ' ', title.lower()).split()
        
        words1 = set(clean_title(title1))
        words2 = set(clean_title(title2))
        
        if not words1 or not words2:
            return 0
        
        intersection = words1.intersection(words2)
        union = words1.union(words2)
        
        return len(intersection) / len(union)
    
    def _generate_safe_filename(self, title: str) -> str:
        """ç”Ÿæˆå®‰å…¨çš„æ–‡ä»¶å"""
        safe_title = re.sub(r'[^\w\s-]', '', title).strip()
        safe_title = re.sub(r'\s+', '_', safe_title)
        return safe_title[:80]
    
    def extract_text(self, pdf_path: Path) -> Optional[str]:
        """ä»PDFæå–æ–‡æœ¬ï¼Œæ”¯æŒå®Œæ•´æå–æˆ–éƒ¨åˆ†æå–"""
        try:
            doc = fitz.open(str(pdf_path))
            text = ""
            
            # å†³å®šè¦æå–çš„é¡µæ•°
            if EXTRACT_FULL_PDF:
                if MAX_PAGES_TO_EXTRACT is None:
                    max_pages = doc.page_count
                    print(f"ğŸ“– æå–æ‰€æœ‰ {max_pages} é¡µ")
                else:
                    max_pages = min(MAX_PAGES_TO_EXTRACT, doc.page_count)
                    print(f"ğŸ“– æå– {max_pages}/{doc.page_count} é¡µ")
            else:
                max_pages = min(5, doc.page_count)
                print(f"ğŸ“– æå–å‰ {max_pages} é¡µ")
            
            # æå–æ–‡æœ¬
            for page_num in range(max_pages):
                try:
                    page = doc[page_num]
                    page_text = page.get_text()
                    text += page_text + "\n\n"
                    
                    if (page_num + 1) % 10 == 0:
                        print(f"  è¿›åº¦: {page_num + 1}/{max_pages} é¡µ")
                        
                except Exception as e:
                    print(f"âš ï¸ ç¬¬{page_num}é¡µæå–é”™è¯¯: {e}")
                    continue
            
            doc.close()
            
            # å¤„ç†æ–‡æœ¬é•¿åº¦é™åˆ¶
            if MAX_TEXT_LENGTH is not None and len(text) > MAX_TEXT_LENGTH:
                print(f"ğŸ“ æ–‡æœ¬è¿‡é•¿ ({len(text)} å­—ç¬¦)ï¼Œæˆªæ–­è‡³ {MAX_TEXT_LENGTH} å­—ç¬¦")
                text = text[:MAX_TEXT_LENGTH]
            
            if len(text.strip()) > 100:
                print(f"âœ… æˆåŠŸæå– {len(text):,} å­—ç¬¦")
                return text
            else:
                print(f"âš ï¸ æå–çš„æ–‡æœ¬ä¸è¶³")
                return None
                
        except Exception as e:
            print(f"âŒ æ–‡æœ¬æå–å¤±è´¥: {e}")
            return None
    
    def split_text_into_chunks(self, text: str, chunk_size: int = None) -> List[str]:
        """å°†é•¿æ–‡æœ¬åˆ†å‰²æˆå—ï¼Œä¾¿äºAIå¤„ç†"""
        if chunk_size is None:
            chunk_size = int(MAX_INPUT_TOKENS * 3.2)
            
        if len(text) <= chunk_size:
            return [text]
        
        chunks = []
        start = 0
        
        while start < len(text):
            end = start + chunk_size
            
            # å°è¯•åœ¨å¥å·å¤„åˆ†å‰²ï¼Œé¿å…æˆªæ–­å¥å­
            if end < len(text):
                sentence_end = text.rfind('.', start, end)
                if sentence_end > start + chunk_size // 2:
                    end = sentence_end + 1
            
            chunk = text[start:end].strip()
            if chunk:
                chunks.append(chunk)
            
            start = end
        
        print(f"ğŸ“„ åˆ†å‰²ä¸º {len(chunks)} ä¸ªæ–‡æœ¬å—")
        return chunks
    
    def get_text_summary(self, paper: Dict, max_chars: int = 2000) -> str:
        """è·å–è®ºæ–‡æ–‡æœ¬çš„æ‘˜è¦ç‰ˆæœ¬ï¼Œç”¨äºå¿«é€Ÿé¢„è§ˆ"""
        full_text = paper.get('extracted_text', '')
        
        if len(full_text) <= max_chars:
            return full_text
        
        summary = full_text[:max_chars]
        
        last_period = summary.rfind('.')
        if last_period > max_chars // 2:
            summary = summary[:last_period + 1]
        
        return summary + f"\n\n[... å·²æˆªæ–­ï¼Œæ€»å…± {len(full_text):,} å­—ç¬¦]"
