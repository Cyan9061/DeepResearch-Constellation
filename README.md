# Ê∑±Á†îÊòüÂõæ / DeepResearch Constellation

<div align="center">

**Âü∫‰∫éDeepSeek-R1ÁöÑÊô∫ËÉΩÂ≠¶ÊúØÊ∑±Â∫¶Á†îÁ©∂Á≥ªÁªü**
**Intelligent Academic Deep Research System based on DeepSeek-R1**

[![Python](https://img.shields.io/badge/Python-3.8+-blue.svg)](https://python.org)
[![License](https://img.shields.io/badge/License-MIT-green.svg)](LICENSE)
[![DeepSeek](https://img.shields.io/badge/AI-DeepSeek--R1-orange.svg)](https://deepseek.com)

[‰∏≠Êñá](#‰∏≠ÊñáÊñáÊ°£) | [English](#english-documentation)

</div>

---

## ‰∏≠ÊñáÊñáÊ°£

### üìñ È°πÁõÆÁÆÄ‰ªã

**Ê∑±Á†îÊòüÂõæ**ÊòØ‰∏Ä‰∏™Âü∫‰∫éDeepSeek-R1ÔºàÂΩìÁÑ∂Ôºå‰Ω†ÂèØ‰ª•‰øÆÊîπÊàê‰ªª‰Ωï‰Ω†ÂñúÊ¨¢ÁöÑÂ§ßÊ®°ÂûãÔºâÂº∫Êé®ÁêÜËÉΩÂäõÁöÑÊô∫ËÉΩÂ≠¶ÊúØÁ†îÁ©∂Á≥ªÁªüÔºå‰∏ìÈó®ËÆæËÆ°Áî®‰∫éËá™Âä®ÂåñÁöÑÂ≠¶ÊúØÊñáÁåÆÊ∑±Â∫¶Á†îÁ©∂„ÄÇÁ≥ªÁªüËÉΩÂ§üËá™Âä®ÊêúÁ¥¢ÔºàÂÖ∂‰∏≠ÂÖÅËÆ∏Á≠õÈÄâË∂ÖËøá60ÁßçËÆ°ÁÆóÊú∫È¢ÜÂüüÈ°∂‰ºöÈ°∂ÂàäÔºÅÔºâ„ÄÅ‰∏ãËΩΩ„ÄÅÂàÜÊûêË∂ÖËøá100ÁØáÂ≠¶ÊúØËÆ∫ÊñáÔºåÂπ∂ÈÄöËøáÂ§öËΩÆËø≠‰ª£‰ºòÂåñÁ†îÁ©∂ÁöÑÂÖÖÂàÜÊÄßÔºåÊúÄÁªàÁîüÊàêÈ´òË¥®ÈáèÁöÑÁ†îÁ©∂ÁªºËø∞Êä•Âëä„ÄÇ

### üéØ È°πÁõÆÂä®Êú∫

ÂΩìÂâçÁöÑDeepResearch AIÁ†îÁ©∂Âä©ÊâãÔºàGPTÔºågeminiÁ≠âÔºâÂ≠òÂú®‰ª•‰∏ãÂ±ÄÈôêÊÄßÔºö

* Áº∫Â∞ëÈíàÂØπDeepSeek-R1Âº∫Êé®ÁêÜ‰∏éÂàÜÊûêËÉΩÂäõÁöÑ‰∏ìÈó®‰ºòÂåñ
* Áé∞ÊúâDeepResearchÁ≥ªÁªüÂæàÂ∞ëÁõ¥Êé•ÈòÖËØªÂ§ßÈáèÊñáÁåÆÁöÑÂÖ®ÊñáÂÜÖÂÆπ
* Áº∫‰πèÊô∫ËÉΩÁöÑÁ†îÁ©∂ÂÖÖÂàÜÊÄßËØÑ‰º∞ÂíåËø≠‰ª£‰ºòÂåñÊú∫Âà∂
* ËÆ∫ÊñáËé∑ÂèñÊàêÂäüÁéá‰∏çÈ´òÔºåÂÆπÈîôËÉΩÂäõÊúâÈôê
* **Ë¥µ**ÔºàÁõ∏ÊØî200ÁæéÂàÄ‰∏Ä‰∏™ÊúàÁöÑGPTÁ≠âÔºåÊú¨È°πÁõÆÈááÁî®DeepSeek R1-0528Êó∂ËøêË°å‰∏ÄÊ¨°Âè™Ë¶Å0.2RMBÂ∑¶Âè≥!!!!!!!!Ôºâ

### üåü Ê†∏ÂøÉÁâπÊÄß

* **üß† Âº∫Êé®ÁêÜÂàÜÊûê**ÔºöÂü∫‰∫éDeepSeek-R1ÁöÑÂº∫Êé®ÁêÜËÉΩÂäõËøõË°åÊ∑±Â∫¶ËÆ∫ÊñáÂàÜÊûê
* **üîç Â§öÊ∫êÊô∫ËÉΩÊêúÁ¥¢**ÔºöGoogle Scholar ‚Üí scholarly ‚Üí DBLP ‚Üí arXiv ÂõõÁ∫ßÊêúÁ¥¢Á≠ñÁï•
* **üì• Â¢ûÂº∫PDFËé∑Âèñ**ÔºöÂ§öÈáçÂ§áÁî®‰∏ãËΩΩÊú∫Âà∂ÔºåÊîØÊåÅ10+ÁßçÂ≠¶ÊúØÂπ≥Âè∞
* **üìä ÂÖÖÂàÜÊÄßËØÑ‰º∞**ÔºöÊØèËΩÆËá™Âä®ËØÑ‰º∞Á†îÁ©∂ÂÖÖÂàÜÊÄßÔºåÊô∫ËÉΩÂÜ≥ÂÆöÊòØÂê¶ÁªßÁª≠ÊêúÁ¥¢
* **‚ö° Âπ∂ÂèëÂ§ÑÁêÜ**ÔºöÊîØÊåÅÂ§öÁ∫øÁ®ãÂπ∂ÂèëËÆ∫ÊñáÂàÜÊûêÔºåÂ§ßÂπÖÊèêÂçáÊïàÁéá
* **üéØ Ê®°Á≥äÂåπÈÖç**ÔºöÊô∫ËÉΩÂéªÈáçÂíåÊ®°Á≥äÂåπÈÖçÔºåÈÅøÂÖçÈáçÂ§çÂàÜÊûê
* **üìà Ëø≠‰ª£‰ºòÂåñ**ÔºöÂ§öËΩÆÊ∑±Â∫¶ÊêúÁ¥¢ÔºåÈÄêÊ≠•ÂÆåÂñÑÁ†îÁ©∂Ë¶ÜÁõñÈù¢

### üìÅ È°πÁõÆÁªìÊûÑ

```
Ê∑±Á†îÊòüÂõæ/
‚îú‚îÄ‚îÄ üìÑ main_DeepResearch.py      # ‰∏ªÁ®ãÂ∫èÂÖ•Âè£
‚îú‚îÄ‚îÄ üß† deepseek_client.py        # DeepSeek AIÂÆ¢Êà∑Á´Ø
‚îú‚îÄ‚îÄ üîç paper_searcher.py         # Â§öÊ∫êËÆ∫ÊñáÊêúÁ¥¢Âô®  
‚îú‚îÄ‚îÄ üì• pdf_processor.py          # PDF‰∏ãËΩΩ‰∏éÂ§ÑÁêÜÂô®
‚îú‚îÄ‚îÄ ‚öôÔ∏è config.py                 # Á≥ªÁªüÈÖçÁΩÆÊñá‰ª∂
‚îú‚îÄ‚îÄ üéÆ demo_research.py          # Âø´ÈÄüÊºîÁ§∫ËÑöÊú¨
‚îú‚îÄ‚îÄ üìã requirements.txt          # ‰æùËµñÂåÖÂàóË°®
‚îú‚îÄ‚îÄ üìÅ downloads/               # ËÆ∫Êñá‰∏ãËΩΩÁõÆÂΩï (Ëá™Âä®ÂàõÂª∫)
‚îú‚îÄ‚îÄ üìÅ output/                  # ÁªìÊûúËæìÂá∫ÁõÆÂΩï (Ëá™Âä®ÂàõÂª∫)
‚îú‚îÄ‚îÄ üìÅ demo_downloads/          # ÊºîÁ§∫‰∏ãËΩΩÁõÆÂΩï (Ëá™Âä®ÂàõÂª∫)
‚îî‚îÄ‚îÄ üìÑ README.md                # È°πÁõÆËØ¥ÊòéÊñáÊ°£
```

### üîÑ Á≥ªÁªüÊµÅÁ®ã

```mermaid
graph TD
    A[üéØ ÊòéÁ°ÆÁ†îÁ©∂‰∏ªÈ¢ò] --> B[üß† AIÁîüÊàêÊêúÁ¥¢Êü•ËØ¢]
    B --> C[üîç Â§öÊ∫êËÆ∫ÊñáÊêúÁ¥¢]
    C --> D[üì• PDF‰∏ãËΩΩ‰∏éÂ§ÑÁêÜ]
    D --> E[üìñ ÊñáÊú¨ÊèêÂèñ‰∏éÂàÜÂùó]
    E --> F[üß† AIÊ∑±Â∫¶ÂàÜÊûêËÆ∫Êñá]
    F --> G[üìù ÁîüÊàêÁ†îÁ©∂ÊÄªÁªì]
    G --> H[üìä ÂÖÖÂàÜÊÄßËØÑ‰º∞]
    H --> I{ÊòØÂê¶ÂÖÖÂàÜ?}
    I -->|Âê¶| J[üéØ ÁîüÊàêÊ∑±Â∫¶ÊêúÁ¥¢Êü•ËØ¢]
    J --> C
    I -->|ÊòØ| K[üìÑ ËæìÂá∫ÊúÄÁªàÊä•Âëä]
    
    style A fill:#e1f5fe
    style C fill:#f3e5f5
    style F fill:#fff3e0
    style H fill:#e8f5e8
    style K fill:#ffebee
```

### ‚öôÔ∏è ÊäÄÊúØÂÆûÁé∞

#### üîç Â§öÊ∫êËÆ∫ÊñáÊêúÁ¥¢

ÈááÁî®**ÂõõÁ∫ßÊêúÁ¥¢Á≠ñÁï•**Á°Æ‰øùÊúÄÂ§ßË¶ÜÁõñÁéáÔºö

```
ü•á Google Scholar (‰∏ªË¶Å) ‚Üí ü•à scholarlyÂ∫ì (Ë°•ÂÖÖ) ‚Üí ü•â DBLP (Â≠¶ÊúØ) ‚Üí üèÖ arXiv (ÂºÄÊîæ)
```

**Ê†∏ÂøÉÁâπÊÄßÔºö**

* Êô∫ËÉΩÊü•ËØ¢ÁîüÊàêÂíå‰ºòÂåñ
* Ê®°Á≥äÂåπÈÖçÂíåÁ≤æÁ°ÆÂéªÈáç
* ‰ºöËÆÆ/ÊúüÂàäÁ≠õÈÄâÊîØÊåÅ
* ÂºïÁî®Êï∞ÂíåÊó∂Èó¥ËåÉÂõ¥ËøáÊª§

#### üì• Â¢ûÂº∫PDFËé∑ÂèñÁ≥ªÁªü

**Â§öÈáçÂÆπÈîôÊú∫Âà∂**Èò≤Ê≠¢‰∏ãËΩΩÂ§±Ë¥•Ôºö

```python
# ‰ºòÂÖàÁ∫ßÊéíÂ∫èÁ≠ñÁï•
arXiv (ÊúÄÈ´ò) ‚Üí ÂºÄÊîæÂπ≥Âè∞ ‚Üí Áõ¥Êé•PDF ‚Üí Êú∫ÊûÑ‰ªìÂ∫ì ‚Üí Âá∫ÁâàÂïÜÂπ≥Âè∞
```

**‰∏ìÈó®Â§ÑÁêÜÂô®ÊîØÊåÅÔºö**

* arXiv„ÄÅResearchGate„ÄÅAcademia.edu
* IEEE„ÄÅACM„ÄÅSpringer„ÄÅNature
* ScienceDirect„ÄÅSemantic Scholar

#### üß† AIÊ∑±Â∫¶ÂàÜÊûêÂºïÊìé

**Á¥ØÁßØÂºèÂàÜÊûêÊñπÊ≥ïÔºö**

```
ÊëòË¶Å + Á¨¨1Âùó ‚Üí ÂàÜÊûê1 ‚Üí ÂàÜÊûê1 + Á¨¨2Âùó ‚Üí ÂàÜÊûê2 ‚Üí ... ‚Üí ÊúÄÁªàÂàÜÊûê
```

**Âπ∂ÂèëÂ§ÑÁêÜËÉΩÂäõÔºö**

* ÊîØÊåÅÊúÄÂ§ö 8 ‰∏™Âπ∂ÂèëÂàÜÊûêÁ∫øÁ®ã
* Êô∫ËÉΩ API ÂØÜÈí•ËΩÆÊç¢
* Ëá™Âä®Ë¥üËΩΩÂùáË°°ÂíåÈîôËØØÊÅ¢Â§ç

#### üìä Êô∫ËÉΩÂÖÖÂàÜÊÄßËØÑ‰º∞

**Â§öÁª¥Â∫¶ËØÑ‰º∞ÊåáÊ†áÔºö**

* Ë¶ÜÁõñÂ∫¶ËØÑ‰º∞ (‰∏ªË¶ÅÁ†îÁ©∂ÊñπÂêë)
* Ê∑±Â∫¶ËØÑ‰º∞ (ÁêÜËÆ∫Âü∫Á°ÄÂíåÊäÄÊúØÁªÜËäÇ)
* Êó∂ÊïàÊÄßËØÑ‰º∞ (ÊúÄÊñ∞Á†îÁ©∂ËøõÂ±ï)
* ÂÆåÊï¥ÊÄßËØÑ‰º∞ (Á†îÁ©∂Á©∫ÁôΩËØÜÂà´)

### üöÄ Âø´ÈÄüÂºÄÂßã

#### 1. ÁéØÂ¢ÉÂáÜÂ§á

```bash
# ÂÖãÈöÜÈ°πÁõÆ
git clone https://github.com/Cyan9061/DeepResearch-Constellation.git
cd DeepResearch-Constellation

# ÂÆâË£Ö‰æùËµñ
pip install -r requirements.txt
```

### ‚öôÔ∏è ËØ¶ÁªÜÈÖçÁΩÆËØ¥Êòé

#### Ê†∏ÂøÉ API ÈÖçÁΩÆ (ÂøÖÈ°ªËÆæÁΩÆ)

ÁºñËæë `config.py` Êñá‰ª∂ÔºåÊ†πÊçÆÊÇ®ÁöÑÊÉÖÂÜµÈÖçÁΩÆ‰ª•‰∏ãÂèÇÊï∞Ôºö

##### 1. **API ÂØÜÈí•ÈÖçÁΩÆ**

```python
# ÊôÆÈÄöÂàÜÊûê API ÂØÜÈí• (ÂøÖÈ°ª)
API_KEYS = [
    "sk-your-api-key-1",
    "sk-your-api-key-2",
    "sk-your-api-key-3",
    # Âª∫ËÆÆËá≥Â∞ëÈÖçÁΩÆ 3-5 ‰∏™ÂØÜÈí•ÊîØÊåÅÂπ∂ÂèëÂ§ÑÁêÜ
]

# È´òÁ∫ßÊÄªÁªì API ÂØÜÈí• (ÂèØÈÄâÔºåÁî®‰∫éÊõ¥Âº∫Â§ßÁöÑÊ®°Âûã)
API_KEYS_2 = [
    "sk-your-premium-api-key-1",
    "sk-your-premium-api-key-2",
    # Â¶ÇÊûú‰∏∫Á©∫ÔºåÂ∞Ü‰ΩøÁî®ÊôÆÈÄö API_KEYS
]
```

**ÈÖçÁΩÆÂª∫ËÆÆÔºö**

* **API\_KEYS**ÔºöÁî®‰∫éÂ∏∏ËßÑËÆ∫ÊñáÂàÜÊûêÔºåÂª∫ËÆÆÈÖçÁΩÆ 3-8 ‰∏™ÂØÜÈí•‰ª•ÊîØÊåÅÂπ∂ÂèëÂ§ÑÁêÜ
* **API\_KEYS\_2**ÔºöÁî®‰∫éÊúÄÁªàÁ†îÁ©∂ÊÄªÁªìÂíåÂÖÖÂàÜÊÄßËØÑ‰º∞ÔºåÂèØ‰ΩøÁî®Êõ¥Âº∫Â§ßÁöÑÊ®°ÂûãÂØÜÈí•
* Â¶ÇÊûúÂè™Êúâ‰∏ÄÂ•óÂØÜÈí•ÔºåÂèØÂ∞Ü `API_KEYS_2` ÁïôÁ©∫ÔºåÁ≥ªÁªü‰ºöËá™Âä®‰ΩøÁî® `API_KEYS`

##### 2. **API Á´ØÁÇπÂíåÊ®°ÂûãÈÖçÁΩÆ**

```python
# API ÊúçÂä°Á´ØÁÇπ
API_ENDPOINT = "https://api.siliconflow.cn/v1/chat/completions"
SUMMARY_API_ENDPOINT = "https://api.siliconflow.cn/v1/chat/completions"

# Ê®°ÂûãÈÄâÊã©
MODEL_NAME = "deepseek-ai/DeepSeek-R1-0528-Qwen3-8B"  # ÊôÆÈÄöÂàÜÊûêÊ®°Âûã
SUMMARY_MODEL_NAME = "deepseek-ai/DeepSeek-R1"        # È´òÁ∫ßÊÄªÁªìÊ®°Âûã
```

**ÈÖçÁΩÆÂª∫ËÆÆÔºö**

* **API\_ENDPOINT**ÔºöÊ†πÊçÆÊÇ®ÁöÑ API Êèê‰æõÂïÜ‰øÆÊîπÔºåÊîØÊåÅ OpenAI ÂÖºÂÆπÊé•Âè£
* **MODEL\_NAME**ÔºöÁî®‰∫éËÆ∫ÊñáÂàÜÊûêÔºåÂª∫ËÆÆ‰ΩøÁî®Âπ≥Ë°°ÁâàÊú¨ÔºàÊàêÊú¨ vs ÊÄßËÉΩÔºâ
* **SUMMARY\_MODEL\_NAME**ÔºöÁî®‰∫éÊúÄÁªàÊÄªÁªìÔºåÂª∫ËÆÆ‰ΩøÁî®ÊúÄÂº∫ÁâàÊú¨‰ª•Ëé∑ÂæóÊúÄ‰Ω≥Ë¥®Èáè

#### ÊÄßËÉΩ‰∏éÂ§ÑÁêÜÈÖçÁΩÆ

##### 3. **Token Âíå‰∏ä‰∏ãÊñáÈÖçÁΩÆ**

```python
MODEL_CONTEXT_SIZE = 96 * 1000         # 96K tokens ‰∏ä‰∏ãÊñá
MAX_TOKENS_PER_REQUEST = 8192          # ÊôÆÈÄöÂàÜÊûêÊØèÊ¨°ÊúÄÂ§ß token Êï∞
SUMMARY_MAX_TOKENS_PER_REQUEST = 16384 # ÊÄªÁªìÂàÜÊûêÊØèÊ¨°ÊúÄÂ§ß token Êï∞Ôºå‰∏çË¶ÅÂ§ß‰∫é max_tokenÔºå‰πüÂèØ‰ª•‰øùÂÆàËÆæÁΩÆ‰∏∫ 8192
```

**ÈÖçÁΩÆÂª∫ËÆÆÔºö**

* **MODEL\_CONTEXT\_SIZE**ÔºöÊ†πÊçÆÊÇ®‰ΩøÁî®ÁöÑÊ®°ÂûãË∞ÉÊï¥ÔºàÂ¶Ç 32K„ÄÅ128K Á≠âÔºâ
* **MAX\_TOKENS\_PER\_REQUEST**ÔºöÊôÆÈÄöÂàÜÊûêËæìÂá∫ÈïøÂ∫¶Ôºå8192 ÈÄÇÂêàÂ§ßÂ§öÊï∞ÊÉÖÂÜµ
* **SUMMARY\_MAX\_TOKENS\_PER\_REQUEST**ÔºöÊúÄÁªàÊÄªÁªìËæìÂá∫ÈïøÂ∫¶ÔºåÂª∫ËÆÆËÆæÁΩÆÊõ¥Â§ßÂÄº

##### 4. **ÊêúÁ¥¢Ê∑±Â∫¶ÈÖçÁΩÆ**

```python
SEARCH_DEPTH = 5                # ÊúÄÂ§ßÊêúÁ¥¢ËΩÆÊï∞
MAX_PAPERS_PER_DEPTH = 48       # ÊØèËΩÆÊúÄÂ§ßÂ§ÑÁêÜËÆ∫ÊñáÊï∞
```

**‰∏™‰∫∫ÂåñËÆæÁΩÆÂª∫ËÆÆÔºö**

* **Âø´ÈÄüÁ†îÁ©∂** (15 ÂàÜÈíüÂÜÖ)Ôºö`SEARCH_DEPTH=2`, `MAX_PAPERS_PER_DEPTH=20`
* **Ê†áÂáÜÁ†îÁ©∂** (1 Â∞èÊó∂ÂÜÖ)Ôºö`SEARCH_DEPTH=3`, `MAX_PAPERS_PER_DEPTH=30`
* **Ê∑±Â∫¶Á†îÁ©∂** (1-2 Â∞èÊó∂)Ôºö`SEARCH_DEPTH=5`, `MAX_PAPERS_PER_DEPTH=48`
* **ÂÖ®Èù¢Á†îÁ©∂** (2 Â∞èÊó∂‰ª•‰∏ä)Ôºö`SEARCH_DEPTH=7`, `MAX_PAPERS_PER_DEPTH=60`

##### 5. **Âπ∂ÂèëÂ§ÑÁêÜÈÖçÁΩÆ**

```python
MAX_CONCURRENT_ANALYSIS = 8     # ÊúÄÂ§ßÂπ∂ÂèëÂàÜÊûêÁ∫øÁ®ãÊï∞Ôºå‰∏çËÉΩÂ§ß‰∫é API_KEYS Êï∞Èáè
CONCURRENT_BATCH_SIZE = 8       # ÊØèÊâπÂπ∂ÂèëÂ§ÑÁêÜÁöÑËÆ∫ÊñáÊï∞ÈáèÔºå‰∏çËÉΩÂ§ß‰∫é API_KEYS Êï∞Èáè
```

**Ê≥®ÊÑè‰∫ãÈ°πÔºö**

* Âπ∂ÂèëÊï∞‰∏çÂ∫îË∂ÖËøá API ÂØÜÈí•Êï∞Èáè
* ËøáÈ´òÁöÑÂπ∂ÂèëÂèØËÉΩËß¶Âèë API ÈôêÊµÅ
* Âª∫ËÆÆ‰ªéËæÉ‰ΩéÈÖçÁΩÆÂºÄÂßãÊµãËØï

#### Âø´ÈÄüÈÖçÁΩÆÊ®°Êùø

##### üöÄ Êñ∞ÊâãÊé®ËçêÈÖçÁΩÆ

```python
SEARCH_DEPTH = 2
MAX_PAPERS_PER_DEPTH = 20
MAX_CONCURRENT_ANALYSIS = 3
CONCURRENT_BATCH_SIZE = 4
# È¢ÑËÆ°Áî®Êó∂Ôºö20-40 ÂàÜÈíü
```

#### 3. ËøêË°åÊºîÁ§∫

```bash
# Âø´ÈÄüÊºîÁ§∫
python demo_research.py

# ÂÆåÊï¥Á†îÁ©∂ÊµÅÁ®ã,Âª∫ËÆÆÂàùÊ¨°‰ΩøÁî®‰∏çÁ≠õÈÄâ‰ºöËÆÆ
python main_DeepResearch.py
```

### üéÆ Êú™Êù•ÊîπËøõÊñπÂêë

#### 1. **üì• Â¢ûÂº∫ËÆ∫Êñá‰∏ãËΩΩÁ≥ªÁªü**

* **Êô∫ËÉΩ Fallback Êú∫Âà∂**ÔºöÈúÄÊ±Ç 20 ÁØá ‚Üí Á¨¨‰∏ÄËΩÆ 10 ÁØáÊàêÂäü ‚Üí ÁªßÁª≠Áà¨Âèñ ‚Üí Áõ¥Âà∞ËææÂà∞ÁõÆÊ†áÊàñ‰∏äÈôê
* **‰ª£ÁêÜÊ±†ÊîØÊåÅ**ÔºöÂàÜÂ∏ÉÂºèÁà¨Ëô´ÈÅøÂÖç IP ÈôêÂà∂
* **Â¢ûÈáèÊõ¥Êñ∞**ÔºöÊîØÊåÅËÆ∫ÊñáÂ∫ìÂ¢ûÈáèÊõ¥Êñ∞ÂíåÁºìÂ≠òÊú∫Âà∂

#### 2. **üß† Prompt ‰ºòÂåñÂ∑•Á®ã**

* **È¢ÜÂüüËá™ÈÄÇÂ∫î Prompt**ÔºöÈíàÂØπ‰∏çÂêåÂ≠¶ÁßëÂÆöÂà∂ÂåñÊèêÁ§∫ËØç
* **Chain-of-Thought ‰ºòÂåñ**ÔºöÂº∫ÂåñÂ§öÊ≠•Êé®ÁêÜÂíåÂàÜÊûêÈìæË∑Ø
* **Few-shot Á§∫‰æãÂ∫ì**ÔºöÊûÑÂª∫È´òË¥®ÈáèÂàÜÊûêÁ§∫‰æãÂ∫ì

#### 3. **üìä ÂÖÖÂàÜÊÄßËØÑ‰º∞ÂçáÁ∫ß**

* **Â§ö‰∏ìÂÆ∂ÈõÜÊàêËØÑ‰º∞**ÔºöÁªìÂêàÂ§ö‰∏™ AI ‰∏ìÂÆ∂ËØÑ‰º∞ÊÑèËßÅ
* **ÈáèÂåñÊåáÊ†á‰ΩìÁ≥ª**ÔºöÂª∫Á´ãÊõ¥Á≤æÁ°ÆÁöÑËØÑ‰º∞ÈáèÂåñÊ†áÂáÜ
* **È¢ÜÂüüÁü•ËØÜÂõæË∞±**ÔºöÈõÜÊàêÂ≠¶ÁßëÁü•ËØÜÂõæË∞±ÊåáÂØºËØÑ‰º∞

#### 4. **üìñ Êô∫ËÉΩÊñáÊú¨ÂàÜÂùó**

* **ÁªìÊûÑÂåñÂàÜÂùó**ÔºöÂü∫‰∫éËÆ∫ÊñáÁõÆÂΩïÂíåÁ´†ËäÇÁöÑÊô∫ËÉΩÂàÜÂâ≤
* **ËØ≠‰πâÁõ∏ÂÖ≥ÊÄßÂàÜÂùó**Ôºö‰øùÊåÅËØ≠‰πâËøûË¥ØÊÄßÁöÑÂàÜÂùóÁ≠ñÁï•
* **Â§öÁ≤íÂ∫¶ÂàÜÊûê**ÔºöÊîØÊåÅÊÆµËêΩ„ÄÅÁ´†ËäÇ„ÄÅÂÖ®ÊñáÂ§öÂ±ÇÊ¨°ÂàÜÊûê

#### 5. **üå≥ Â§ßËßÑÊ®°ÂàÜÊûêÊû∂ÊûÑ**

* **ÂΩíÂπ∂Ê†ëÁªìÊûÑ**ÔºöÂ§ÑÁêÜË∂ÖÂ§ßËßÑÊ®°ËÆ∫ÊñáÈõÜÁöÑÂ±ÇÊ¨°ÂåñÂàÜÊûê
* **ÂàÜÂ∏ÉÂºèË∞ÉÂ∫¶**ÔºöÊîØÊåÅÈõÜÁæ§ÂåñÁöÑÂ§ßËßÑÊ®°Âπ∂Ë°åÂ§ÑÁêÜ
* **Â¢ûÈáèÂàÜÊûê**ÔºöÊîØÊåÅÂ¢ûÈáèËÆ∫ÊñáÊ∑ªÂä†ÂíåÊõ¥Êñ∞ÂàÜÊûê

#### 6. **üé® Áî®Êà∑ÁïåÈù¢‰ºòÂåñ**

* **Web ÂèØËßÜÂåñÁïåÈù¢**ÔºöÂÆûÊó∂ËøõÂ∫¶Â±ïÁ§∫Âíå‰∫§‰∫íÊìç‰Ωú
* **Á†îÁ©∂Âú∞ÂõæÂèØËßÜÂåñ**ÔºöËÆ∫ÊñáÂÖ≥Á≥ªÁΩëÁªúÂíåÁü•ËØÜÂõæË∞±Â±ïÁ§∫
* **ÂØºÂá∫Â§öÊ†ºÂºè**ÔºöÊîØÊåÅ PDF„ÄÅWord„ÄÅLaTeX Á≠âÂ§öÁßçÂØºÂá∫Ê†ºÂºè

#### 7. **‚ö° ÊÄßËÉΩ‰∏éÊâ©Â±ïÊÄß**

* **Ê®°ÂûãÈÄÇÈÖçÂô®**ÔºöÊîØÊåÅÂ§öÁßçÂ§ßÊ®°ÂûãÁöÑÊó†ÁºùÂàáÊç¢
* **ÁºìÂ≠ò‰ºòÂåñ**ÔºöÊô∫ËÉΩÁºìÂ≠òÂáèÂ∞ëÈáçÂ§çËÆ°ÁÆó
* **ÊµÅÂºèÂ§ÑÁêÜ**ÔºöÊîØÊåÅÂÆûÊó∂ÊµÅÂºèÂàÜÊûêÂíåÂèçÈ¶àÔºå‰πÉËá≥ÊµÅÊ∞¥Á∫øÂ§ÑÁêÜ

#### 8. **üîí ‰ºÅ‰∏öÁ∫ßÁâπÊÄß**

* **ÊùÉÈôêÁÆ°ÁêÜ**ÔºöÂ§öÁî®Êà∑ÊùÉÈôêÊéßÂà∂ÂíåÈ°πÁõÆÁÆ°ÁêÜ
* **ÂÆ°ËÆ°Êó•Âøó**ÔºöÂÆåÊï¥ÁöÑÊìç‰ΩúËÆ∞ÂΩïÂíåÂèØËøΩÊ∫ØÊÄß
* **ÁßÅÊúâÈÉ®ÁΩ≤**ÔºöÊîØÊåÅ‰ºÅ‰∏öÂÜÖÁΩëÁßÅÊúâÂåñÈÉ®ÁΩ≤

---

## License

Êú¨È°πÁõÆÈááÁî® MIT ËÆ∏ÂèØËØÅ„ÄÇËØ¶ÊÉÖËØ∑ÂèÇËßÅ [LICENSE](LICENSE)„ÄÇ

‚≠ê **Â¶ÇÊûúËøô‰∏™È°πÁõÆÂØπÊÇ®ÊúâÂ∏ÆÂä©ÔºåËØ∑ÁªôÊàë‰ª¨‰∏Ä‰∏™StarÔºÅ**  

---

## English Documentation

### üìñ Project Overview

**DeepResearch Constellation** is an intelligent academic deep research system built on the powerful reasoning capabilities of DeepSeek-R1. It is specifically designed for automated deep-dive exploration of academic literature. The system can automatically searchÔºàMore than 60 top conferences and journals in the computer science field are accessible!Ôºâ, download, and analyze a large number of academic papers, and through multiple iterative rounds, optimize research completeness, ultimately generating a high-quality research review report.

### üéØ Motivation

Current DeepResearch AI research assistants have the following limitations:

* Lack of specialized optimization for DeepSeek-R1‚Äôs strong reasoning and analytical capabilities
* Existing DeepResearch systems rarely read the full text of a large volume of literature directly
* Lack of intelligent research sufficiency evaluation and iterative optimization mechanisms
* Low success rate in paper acquisition and limited fault tolerance
* **expensive**(Compared to GPT and others that cost around $200 per month, this project only costs about 0.2 RMB(about $0.0278) per run when using DeepSeek R1-0528!!!!!!!!)

### üåü Core Features

* **üß† Advanced Reasoning and Analysis**: Deep paper analysis based on DeepSeek-R1‚Äôs strong reasoning capabilities
* **üîç Multi-Source Intelligent Search**: Four-level search strategy: Google Scholar ‚Üí scholarly ‚Üí DBLP ‚Üí arXiv
* **üì• Enhanced PDF Acquisition**: Multiple fallback download mechanisms supporting 10+ academic platforms
* **üìä Sufficiency Evaluation**: Automatic research sufficiency assessment each round, intelligently deciding whether to continue searching
* **‚ö° Concurrent Processing**: Multi-threaded concurrent paper analysis, significantly improving efficiency
* **üéØ Fuzzy Matching**: Intelligent deduplication and fuzzy matching to avoid repeated analysis
* **üìà Iterative Optimization**: Multi-round deep search to gradually improve research coverage

### üìÅ Project Structure

```
DeepResearch Constellation/
‚îú‚îÄ‚îÄ üìÑ main_DeepResearch.py      # Main entry point
‚îú‚îÄ‚îÄ üß† deepseek_client.py        # DeepSeek AI client
‚îú‚îÄ‚îÄ üîç paper_searcher.py         # Multi-source paper searcher  
‚îú‚îÄ‚îÄ üì• pdf_processor.py          # PDF downloader and processor
‚îú‚îÄ‚îÄ ‚öôÔ∏è config.py                 # System configuration file
‚îú‚îÄ‚îÄ üéÆ demo_research.py          # Quick demonstration script
‚îú‚îÄ‚îÄ üìã requirements.txt          # Dependencies list
‚îú‚îÄ‚îÄ üìÅ downloads/               # Paper download directory (auto-created)
‚îú‚îÄ‚îÄ üìÅ output/                  # Output directory (auto-created)
‚îú‚îÄ‚îÄ üìÅ demo_downloads/          # Demo download directory (auto-created)
‚îî‚îÄ‚îÄ üìÑ README.md                # Project documentation
```

### üîÑ System Workflow

```mermaid
graph TD
    A[üéØ Define Research Topic] --> B[üß† AI Generates Search Queries]
    B --> C[üîç Multi-Source Paper Search]
    C --> D[üì• PDF Download and Processing]
    D --> E[üìñ Text Extraction and Chunking]
    E --> F[üß† AI Deep Paper Analysis]
    F --> G[üìù Generate Research Summary]
    G --> H[üìä Sufficiency Evaluation]
    H --> I{Is It Sufficient?}
    I -->|No| J[üéØ Generate Deep Search Queries]
    J --> C
    I -->|Yes| K[üìÑ Output Final Report]
    
    style A fill:#e1f5fe
    style C fill:#f3e5f5
    style F fill:#fff3e0
    style H fill:#e8f5e8
    style K fill:#ffebee
```

### ‚öôÔ∏è Technical Implementation

#### üîç Multi-Source Paper Search

The system adopts a **four-level search strategy** to ensure maximum coverage:

```
ü•á Google Scholar (primary) ‚Üí ü•à scholarly library (supplement) ‚Üí ü•â DBLP (academic) ‚Üí üèÖ arXiv (open access)
```

**Key Features:**

* Intelligent query generation and optimization
* Fuzzy matching and precise deduplication
* Conference/journal filtering support
* Citation count and time range filtering

#### üì• Enhanced PDF Acquisition System

A **multi-layered fallback mechanism** prevents download failures:

```python
# Priority ranking
arXiv (highest) ‚Üí Open platforms ‚Üí Direct PDF ‚Üí Institutional repositories ‚Üí Publisher platforms
```

**Supported Sources:**

* arXiv, ResearchGate, Academia.edu
* IEEE, ACM, Springer, Nature
* ScienceDirect, Semantic Scholar

#### üß† AI Deep Analysis Engine

**Cumulative Analysis Method:**

```
Abstract + Chunk 1 ‚Üí Analysis 1 ‚Üí Analysis 1 + Chunk 2 ‚Üí Analysis 2 ‚Üí ... ‚Üí Final Analysis
```

**Concurrency Features:**

* Up to 8 concurrent analysis threads
* Intelligent API key rotation
* Automatic load balancing and error recovery

#### üìä Intelligent Sufficiency Evaluation

**Multi-dimensional Evaluation Metrics:**

* Coverage Evaluation (main research directions)
* Depth Evaluation (theoretical basis and technical details)
* Timeliness Evaluation (latest research progress)
* Completeness Evaluation (identifying research gaps)

---

### üöÄ Quick Start

#### 1. Environment Setup

```bash
# Clone the repository
git clone https://github.com/Cyan9061/DeepResearch-Constellation.git
cd DeepResearch-Constellation

# Install dependencies
pip install -r requirements.txt
```

### ‚öôÔ∏è Detailed Configuration Guide

#### Core API Configuration (Required)

Edit the `config.py` file and configure the following parameters according to your environment:

##### 1. **API Key Configuration**

```python
# Regular Analysis API Keys (required)
API_KEYS = [
    "sk-your-api-key-1",
    "sk-your-api-key-2",
    "sk-your-api-key-3",
    # It is recommended to configure at least 3-5 keys for concurrent processing
]

# Advanced Summarization API Keys (optional, for more powerful models)
API_KEYS_2 = [
    "sk-your-premium-api-key-1",
    "sk-your-premium-api-key-2",
    # If empty, the system will use API_KEYS
]
```

**Configuration Recommendations:**

* **API\_KEYS**: Used for regular paper analysis. It is recommended to configure 3-8 keys to support concurrent processing.
* **API\_KEYS\_2**: Used for the final research summary and sufficiency evaluation. You can use stronger model keys.
* If you only have one set of keys, leave `API_KEYS_2` empty. The system will automatically use `API_KEYS`.

##### 2. **API Endpoint and Model Configuration**

```python
# API service endpoints
API_ENDPOINT = "https://api.siliconflow.cn/v1/chat/completions"
SUMMARY_API_ENDPOINT = "https://api.siliconflow.cn/v1/chat/completions"

# Model selection
MODEL_NAME = "deepseek-ai/DeepSeek-R1-0528-Qwen3-8B"  # Model for regular analysis
SUMMARY_MODEL_NAME = "deepseek-ai/DeepSeek-R1"        # Model for advanced summarization
```

**Configuration Recommendations:**

* **API\_ENDPOINT**: Modify according to your API provider, supports OpenAI-compatible interfaces.
* **MODEL\_NAME**: Used for paper analysis. It is recommended to choose a balanced version (cost vs. performance).
* **SUMMARY\_MODEL\_NAME**: Used for final summarization. It is recommended to choose the strongest version for the best quality.

#### Performance and Processing Configuration

##### 3. **Token and Context Configuration**

```python
MODEL_CONTEXT_SIZE = 96 * 1000         # 96K tokens context size
MAX_TOKENS_PER_REQUEST = 8192          # Max tokens per request for regular analysis
SUMMARY_MAX_TOKENS_PER_REQUEST = 16384 # Max tokens per request for summarization (should not exceed model's max tokens; 8192 is also conservative)
```

**Configuration Recommendations:**

* **MODEL\_CONTEXT\_SIZE**: Adjust according to the model you are using (e.g., 32K, 128K, etc.).
* **MAX\_TOKENS\_PER\_REQUEST**: Output length for regular analysis. 8192 fits most use cases.
* **SUMMARY\_MAX\_TOKENS\_PER\_REQUEST**: Output length for final summarization. It is recommended to set a larger value.

##### 4. **Search Depth Configuration**

```python
SEARCH_DEPTH = 5                # Maximum number of search rounds
MAX_PAPERS_PER_DEPTH = 48       # Maximum number of papers processed per round
```

**Personalized Settings Recommendations:**

* **Quick Research** (within 15 minutes): `SEARCH_DEPTH=2`, `MAX_PAPERS_PER_DEPTH=20`
* **Standard Research** (within 1 hour): `SEARCH_DEPTH=3`, `MAX_PAPERS_PER_DEPTH=30`
* **Deep Research** (1‚Äì2 hours): `SEARCH_DEPTH=5`, `MAX_PAPERS_PER_DEPTH=48`
* **Comprehensive Research** (over 2 hours): `SEARCH_DEPTH=7`, `MAX_PAPERS_PER_DEPTH=60`

##### 5. **Concurrency Configuration**

```python
MAX_CONCURRENT_ANALYSIS = 8     # Maximum concurrent analysis threads, should not exceed number of API_KEYS
CONCURRENT_BATCH_SIZE = 8       # Number of papers processed concurrently per batch, should not exceed number of API_KEYS
```

**Notes:**

* Concurrency should not exceed the number of API keys.
* Too high concurrency may trigger API rate limits.
* It is recommended to start testing at a lower configuration.

#### Quick Configuration Template

##### üöÄ Beginner Recommended Configuration

```python
SEARCH_DEPTH = 2
MAX_PAPERS_PER_DEPTH = 20
MAX_CONCURRENT_ANALYSIS = 3
CONCURRENT_BATCH_SIZE = 4
# Estimated time: 20‚Äì40 minutes
```

#### 3. Run Demonstration

```bash
# Quick demonstration
python demo_research.py

# Full research pipeline
python main_DeepResearch.py
```

### üéÆ Usage Examples

#### 1. **üì• Enhanced Paper Download System**

* **Intelligent Fallback Mechanism**: Request 20 papers ‚Üí First round 10 succeed ‚Üí Continue crawling ‚Üí Until goal or limit is reached
* **Proxy Pool Support**: Distributed crawling to avoid IP restrictions
* **Incremental Updates**: Supports incremental updates and caching of the paper library

#### 2. **üß† Prompt Engineering Optimization**

* **Domain-Adaptive Prompts**: Customized prompts for different disciplines
* **Chain-of-Thought Optimization**: Strengthened multi-step reasoning and analysis chains
* **Few-shot Example Library**: Build a high-quality example library for analysis

#### 3. **üìä Enhanced Sufficiency Evaluation**

* **Multi-Expert Ensemble Evaluation**: Combine evaluations from multiple AI experts
* **Quantitative Metric System**: Establish more precise quantitative evaluation standards
* **Domain Knowledge Graph**: Integrate disciplinary knowledge graph to guide evaluation

#### 4. **üìñ Intelligent Text Chunking**

* **Structured Chunking**: Intelligent splitting based on paper outline and sections
* **Semantic Coherence Chunking**: Chunking strategy that maintains semantic continuity
* **Multi-Granularity Analysis**: Support multi-level analysis of paragraph, section, and full text

#### 5. **üå≥ Large-Scale Analysis Architecture**

* **Merge Tree Structure**: Hierarchical analysis for ultra-large-scale paper collections
* **Distributed Scheduling**: Support for cluster-based large-scale parallel processing
* **Incremental Analysis**: Support incremental addition and update of papers for analysis

#### 6. **üé® User Interface Optimization**

* **Web Visualization Interface**: Real-time progress display and interactive operations
* **Research Map Visualization**: Display paper relationship networks and knowledge graphs
* **Multiple Export Formats**: Support export to PDF, Word, LaTeX, etc.

#### 7. **‚ö° Performance and Scalability**

* **Model Adapters**: Support seamless switching between various large models
* **Cache Optimization**: Intelligent caching to reduce redundant computations
* **Streaming Processing**: Support real-time streaming analysis and feedback, even pipeline processing

#### 8. **üîí Enterprise-Level Features**

* **Permission Management**: Multi-user permission control and project management
* **Audit Logs**: Complete operation records and traceability
* **Private Deployment**: Support for private network deployment within enterprises

---

## License

This project is licensed under the MIT License. See the [LICENSE](LICENSE) file for details.

Â¶ÇÊûúËøô‰∏™È°πÁõÆÂØπÊÇ®ÊúâÂ∏ÆÂä©ÔºåËØ∑ÁªôÊàë‰ª¨‰∏Ä‰∏™ StarÔºÅ

---

## Ëã±ÊñáÊñáÊ°£

### üìñ Project Introduction

**DeepResearch Constellation** is an intelligent academic deep research system based on the powerful reasoning capabilities of DeepSeek-R1. It is designed specifically for automated deep research into academic literature. The system can automatically search, download, and analyze a large volume of academic papers, and through multiple iterative rounds, optimize the sufficiency of research, ultimately generating a high-quality review report of the research findings. We apologize for not maintaining complete English annotations; we will supplement them in the future.

### üéØ Project Motivation

Current AI research assistants have the following limitations:

* Lack of specialized optimization for DeepSeek-R1‚Äôs strong reasoning and analytical capabilities
* Existing DeepResearch systems seldom read the full text of a large number of papers directly
* Lack of an intelligent mechanism for evaluating research sufficiency and iterative optimization
* Low success rate for paper acquisition and limited fault tolerance

### üåü Core Features

* **üß† Advanced Reasoning and Analysis**: Deep paper analysis powered by DeepSeek-R1‚Äôs strong reasoning capabilities
* **üîç Multi-Source Intelligent Search**: A four-level search strategy: Google Scholar ‚Üí scholarly ‚Üí DBLP ‚Üí arXiv
* **üì• Enhanced PDF Acquisition**: Multiple fallback download mechanisms supporting over 10 academic platforms
* **üìä Sufficiency Evaluation**: Automated assessment of research sufficiency each round, intelligently deciding whether to continue searching
* **‚ö° Concurrent Processing**: Multi-threaded concurrent analysis of papers, significantly improving efficiency
* **üéØ Fuzzy Matching**: Intelligent deduplication and fuzzy matching to avoid redundant analysis
* **üìà Iterative Optimization**: Multi-round deep search to gradually improve research coverage

### üìÅ Project Structure

```
DeepResearch Constellation/
‚îú‚îÄ‚îÄ üìÑ main_DeepResearch.py      # Main program entry point
‚îú‚îÄ‚îÄ üß† deepseek_client.py        # DeepSeek AI client
‚îú‚îÄ‚îÄ üîç paper_searcher.py         # Multi-source paper search module  
‚îú‚îÄ‚îÄ üì• pdf_processor.py          # PDF download and processing module
‚îú‚îÄ‚îÄ ‚öôÔ∏è config.py                 # System configuration file
‚îú‚îÄ‚îÄ üéÆ demo_research.py          # Quick demonstration script
‚îú‚îÄ‚îÄ üìã requirements.txt          # Dependencies list
‚îú‚îÄ‚îÄ üìÅ downloads/               # Directory for downloaded papers (auto-created)
‚îú‚îÄ‚îÄ üìÅ output/                  # Directory for output results (auto-created)
‚îú‚îÄ‚îÄ üìÅ demo_downloads/          # Directory for demo downloads (auto-created)
‚îî‚îÄ‚îÄ üìÑ README.md                # Project documentation
```

### üîÑ System Workflow

```mermaid
graph TD
    A[üéØ Define Research Topic] --> B[üß† AI Generates Search Queries]
    B --> C[üîç Multi-Source Paper Search]
    C --> D[üì• PDF Download and Processing]
    D --> E[üìñ Text Extraction and Chunking]
    E --> F[üß† AI Deep Paper Analysis]
    F --> G[üìù Generate Research Summary]
    G --> H[üìä Sufficiency Evaluation]
    H --> I{Is It Sufficient?}
    I -->|No| J[üéØ Generate Deep Search Queries]
    J --> C
    I -->|Yes| K[üìÑ Output Final Report]
    
    style A fill:#e1f5fe
    style C fill:#f3e5f5
    style F fill:#fff3e0
    style H fill:#e8f5e8
    style K fill:#ffebee
```

### ‚öôÔ∏è Technical Implementation

#### üîç Multi-Source Paper Search

The system employs a **four-level search strategy** to ensure maximum coverage:

```
ü•á Google Scholar (primary) ‚Üí ü•à scholarly library (supplement) ‚Üí ü•â DBLP (academic) ‚Üí üèÖ arXiv (open access)
```

**Key Features:**

* Intelligent query generation and optimization
* Fuzzy matching and precise deduplication
* Conference/journal filtering support
* Citation count and time range filtering

#### üì• Enhanced PDF Acquisition System

A **multi-layered fallback mechanism** prevents download failures:

```python
# Priority ranking
arXiv (highest) ‚Üí Open platforms ‚Üí Direct PDF ‚Üí Institutional repositories ‚Üí Publisher platforms
```

**Supported Sources Include:**

* arXiv, ResearchGate, Academia.edu
* IEEE, ACM, Springer, Nature
* ScienceDirect, Semantic Scholar

#### üß† AI Deep Analysis Engine

**Cumulative Analysis Method:**

```
Abstract + Chunk 1 ‚Üí Analysis 1 ‚Üí Analysis 1 + Chunk 2 ‚Üí Analysis 2 ‚Üí ... ‚Üí Final Analysis
```

**Concurrency Features:**

* Supports up to 8 concurrent analysis threads
* Intelligent API key rotation
* Automatic load balancing and error recovery

#### üìä Intelligent Sufficiency Evaluation

**Multi-dimensional Evaluation Metrics:**

* Coverage Evaluation (main research directions)
* Depth Evaluation (theoretical basis and technical details)
* Timeliness Evaluation (latest research progress)
* Completeness Evaluation (identification of research gaps)

---

### üöÄ Quick Start

#### 1. Environment Setup

```bash
# Clone the repository
git clone https://github.com/Cyan9061/DeepResearch-Constellation.git
cd DeepResearch-Constellation

# Install dependencies
pip install -r requirements.txt
```

### ‚öôÔ∏è Detailed Configuration Guide

#### Core API Configuration (Required)

Edit the `config.py` file and configure the following parameters according to your environment:

##### 1. **API Key Configuration**

```python
# Regular Analysis API Keys (required)
API_KEYS = [
    "sk-your-api-key-1",
    "sk-your-api-key-2",
    "sk-your-api-key-3",
    # It is recommended to configure at least 3-5 keys for concurrent processing
]

# Advanced Summarization API Keys (optional, for more powerful models)
API_KEYS_2 = [
    "sk-your-premium-api-key-1",
    "sk-your-premium-api-key-2",
    # If empty, the system will use API_KEYS
]
```

**Configuration Recommendations:**

* **API\_KEYS**: Used for regular paper analysis. It is recommended to configure 3-8 keys to support concurrent processing.
* **API\_KEYS\_2**: Used for the final research summary and sufficiency evaluation. You can use stronger model keys.
* If you only have one set of keys, leave `API_KEYS_2` empty. The system will automatically use `API_KEYS`.

##### 2. **API Endpoint and Model Configuration**

```python
# API service endpoints
API_ENDPOINT = "https://api.siliconflow.cn/v1/chat/completions"
SUMMARY_API_ENDPOINT = "https://api.siliconflow.cn/v1/chat/completions"

# Model selection
MODEL_NAME = "deepseek-ai/DeepSeek-R1-0528-Qwen3-8B"  # Model for regular analysis
SUMMARY_MODEL_NAME = "deepseek-ai/DeepSeek-R1"        # Model for advanced summarization
```

**Configuration Recommendations:**

* **API\_ENDPOINT**: Modify according to your API provider, supports OpenAI-compatible interfaces.
* **MODEL\_NAME**: Used for paper analysis. It is recommended to choose a balanced version (cost vs. performance).
* **SUMMARY\_MODEL\_NAME**: Used for final summarization. It is recommended to choose the strongest version for the best quality.

#### Performance and Processing Configuration

##### 3. **Token and Context Configuration**

```python
MODEL_CONTEXT_SIZE = 96 * 1000         # 96K tokens context size
MAX_TOKENS_PER_REQUEST = 8192          # Max tokens per request for regular analysis
SUMMARY_MAX_TOKENS_PER_REQUEST = 16384 # Max tokens per request for summarization (should not exceed the model's max tokens; 8192 is also conservative)
```

**Configuration Recommendations:**

* **MODEL\_CONTEXT\_SIZE**: Adjust according to the model you are using (e.g., 32K, 128K, etc.)
* **MAX\_TOKENS\_PER\_REQUEST**: Output length for regular analysis. 8192 fits most use cases.
* **SUMMARY\_MAX\_TOKENS\_PER\_REQUEST**: Output length for final summarization. It is recommended to set a larger value.

##### 4. **Search Depth Configuration**

```python
SEARCH_DEPTH = 5                # Maximum number of search rounds
MAX_PAPERS_PER_DEPTH = 48       # Maximum number of papers processed per round
```

**Personalized Settings Recommendations:**

* **Quick Research** (within 15 minutes): `SEARCH_DEPTH=2`, `MAX_PAPERS_PER_DEPTH=20`
* **Standard Research** (within 1 hour): `SEARCH_DEPTH=3`, `MAX_PAPERS_PER_DEPTH=30`
* **Deep Research** (1‚Äì2 hours): `SEARCH_DEPTH=5`, `MAX_PAPERS_PER_DEPTH=48`
* **Comprehensive Research** (over 2 hours): `SEARCH_DEPTH=7`, `MAX_PAPERS_PER_DEPTH=60`

##### 5. **Concurrency Configuration**

```python
MAX_CONCURRENT_ANALYSIS = 8     # Maximum concurrent analysis threads, should not exceed number of API_KEYS
CONCURRENT_BATCH_SIZE = 8       # Number of papers processed concurrently per batch, should not exceed number of API_KEYS
```

**Notes:**

* Concurrency should not exceed the number of API keys.
* Too high concurrency may trigger API rate limits.
* It is recommended to start testing at a lower configuration.

#### Quick Configuration Template

##### üöÄ Beginner Recommended Configuration

```python
SEARCH_DEPTH = 2
MAX_PAPERS_PER_DEPTH = 20
MAX_CONCURRENT_ANALYSIS = 3
CONCURRENT_BATCH_SIZE = 4
# Estimated time: 20‚Äì40 minutes
```

#### 3. Run Demonstration

```bash
# Quick demonstration
python demo_research.py

# Full research pipeline,recommend turning off conference filter when first running
python main_DeepResearch.py
```

### üéÆ Future improvement directions

#### 1. **üì• Enhanced Paper Download System**

* **Intelligent Fallback Mechanism**: Request 20 papers ‚Üí First round 10 succeed ‚Üí Continue crawling ‚Üí Until goal or limit is reached
* **Proxy Pool Support**: Distributed crawling to avoid IP restrictions
* **Incremental Updates**: Supports incremental updates and caching of the paper library

#### 2. **üß† Prompt Engineering Optimization**

* **Domain-Adaptive Prompts**: Customized prompts for different disciplines
* **Chain-of-Thought Optimization**: Strengthened multi-step reasoning and analysis chains
* **Few-shot Example Library**: Build a high-quality example library for analysis

#### 3. **üìä Enhanced Sufficiency Evaluation**

* **Multi-Expert Ensemble Evaluation**: Combine evaluations from multiple AI experts
* **Quantitative Metric System**: Establish more precise quantitative evaluation standards
* **Domain Knowledge Graph**: Integrate disciplinary knowledge graph to guide evaluation

#### 4. **üìñ Intelligent Text Chunking**

* **Structured Chunking**: Intelligent splitting based on paper outline and sections
* **Semantic Coherence Chunking**: Chunking strategy that maintains semantic continuity
* **Multi-Granularity Analysis**: Support multi-level analysis of paragraph, section, and full text

#### 5. **üå≥ Large-Scale Analysis Architecture**

* **Merge Tree Structure**: Hierarchical analysis for ultra-large-scale paper collections
* **Distributed Scheduling**: Support for cluster-based large-scale parallel processing
* **Incremental Analysis**: Support incremental addition and update of papers for analysis

#### 6. **üé® User Interface Optimization**

* **Web Visualization Interface**: Real-time progress display and interactive operations
* **Research Map Visualization**: Display paper relationship networks and knowledge graphs
* **Multiple Export Formats**: Support export to PDF, Word, LaTeX, etc.

#### 7. **‚ö° Performance and Scalability**

* **Model Adapters**: Support seamless switching between various large models
* **Cache Optimization**: Intelligent caching to reduce redundant computations
* **Streaming Processing**: Support real-time streaming analysis and feedback, even pipeline processing

#### 8. **üîí Enterprise-Level Features**

* **Permission Management**: Multi-user permission control and project management
* **Audit Logs**: Complete operation records and traceability
* **Private Deployment**: Support for private network deployment within enterprises

---

## License

This project is licensed under the MIT License. See the [LICENSE](LICENSE) file for details.

‚≠ê **If this project helps you, please give us a Star!**
