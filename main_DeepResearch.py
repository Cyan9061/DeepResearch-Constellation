#!/usr/bin/env python3
"""
Academic Deep Research Demo - Enhanced Version with Paper Searcher
ç»Ÿä¸€ä½¿ç”¨å¢å¼ºæ¨¡å¼ï¼šGoogle Scholarä¼˜å…ˆã€arXivå¤‡ç”¨ã€ä¼šè®®ç­›é€‰ã€å¼•ç”¨æ•°è¿‡æ»¤ã€æ¯è½®å……åˆ†æ€§è¯„ä¼°
"""

import json
import os
from pathlib import Path
from datetime import datetime
from deepseek_client import DeepSeekClient
from paper_searcher import EnhancedPaperSearcher, SearchFilters
from pdf_processor import EnhancedPDFProcessor
import time
from config import (
    OUTPUT_DIR, 
    MAX_ANALYSIS_PAPERS,
    NUM_SEARCH_QUERIES,
    PAPERS_PER_QUERY,
    USE_RETRY_ON_FAILURE,
    EXTRACT_FULL_PDF,
    ENABLE_CONCURRENT_ANALYSIS,
    MAX_CONCURRENT_ANALYSIS,
    SEARCH_DEPTH,
    MAX_PAPERS_PER_DEPTH,
    MIN_PAPERS_FOR_NEXT_DEPTH,
    DEPTH_SEARCH_QUERIES,
    PAPERS_PER_DEPTH_QUERY,
    ADEQUACY_EVALUATION_THRESHOLD,
    MIN_DEPTH_SEARCH_SCORE,
)

# æ¼”ç¤ºç¨‹åºé…ç½®å‚æ•°
DEFAULT_RESEARCH_TOPIC = "transformer attention mechanisms in neural networks"
ENABLE_DETAILED_ANALYSIS = True
GENERATE_RESEARCH_SUMMARY = True
SAVE_FULL_TEXT = True
SHOW_PROGRESS_DETAILS = True
MIN_PAPERS_FOR_CONTINUE = 3  # ç»§ç»­æœç´¢æ‰€éœ€çš„æœ€å°è®ºæ–‡æ•°é‡

def print_banner():
    """æ‰“å°ç¨‹åºæ¨ªå¹…"""
    banner = """
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                 æ·±ç ”æ˜Ÿå›¾-å­¦æœ¯æ·±åº¦ç ”ç©¶ç³»ç»Ÿ                         â•‘
â•‘           Enhanced Academic Deep Research System             â•‘
â•‘    Google Scholar + arXiv + Conference + Citation Filter     â•‘
â•‘              + Adequacy Evaluation + Smart Stop              â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    """
    print(banner)

def get_research_topic():
    """è·å–ç ”ç©¶ä¸»é¢˜"""
    print(f"ğŸ¯ è¯·è¾“å…¥æ‚¨çš„ç ”ç©¶ä¸»é¢˜ (æŒ‰å›è½¦ä½¿ç”¨é»˜è®¤ä¸»é¢˜):")
    print(f"   é»˜è®¤ä¸»é¢˜: {DEFAULT_RESEARCH_TOPIC}")
    
    research_topic = input("\n>>> ").strip()
    if not research_topic:
        research_topic = DEFAULT_RESEARCH_TOPIC
        print(f"ä½¿ç”¨é»˜è®¤ä¸»é¢˜: {research_topic}")
    
    return research_topic

def display_config():
    """æ˜¾ç¤ºå½“å‰é…ç½®"""
    if SHOW_PROGRESS_DETAILS:
        print("\n" + "="*60)
        print("ğŸ“‹æ¨¡å¼é…ç½®:")
        print("="*60)
        print(f"  ğŸ” ä¸»è¦æœç´¢æº: Google Scholar")
        print(f"  ğŸ“š å¤‡ç”¨æœç´¢æº: arXiv")
        print(f"  ğŸ”„ æœç´¢æ·±åº¦: {SEARCH_DEPTH} è½®")
        print(f"  ğŸ“Š ç¬¬ä¸€è½®æœç´¢æŸ¥è¯¢æ•°: {NUM_SEARCH_QUERIES}")
        print(f"  ğŸ“„ æ¯ä¸ªæŸ¥è¯¢çš„è®ºæ–‡æ•°: {PAPERS_PER_QUERY}")
        print(f"  ğŸ“š æ¯è½®æœ€å¤§å¤„ç†è®ºæ–‡æ•°: {MAX_PAPERS_PER_DEPTH}")
        print(f"  ğŸ¯ æ·±åº¦æœç´¢æŸ¥è¯¢æ•°: {DEPTH_SEARCH_QUERIES}")
        print(f"  ğŸ“– æå–å®Œæ•´PDF: {'æ˜¯' if EXTRACT_FULL_PDF else 'å¦'}")
        print(f"  ğŸ’¾ ä¿å­˜å®Œæ•´æ–‡æœ¬: {'æ˜¯' if SAVE_FULL_TEXT else 'å¦'}")
        print(f"  ğŸš€ å¹¶å‘åˆ†æ: {'å¯ç”¨' if ENABLE_CONCURRENT_ANALYSIS else 'ç¦ç”¨'}")
        if ENABLE_CONCURRENT_ANALYSIS:
            print(f"  âš¡ æœ€å¤§å¹¶å‘æ•°: {MAX_CONCURRENT_ANALYSIS}")
        print(f"  ğŸšï¸ å……åˆ†æ€§è¯„ä¼°é˜ˆå€¼: {ADEQUACY_EVALUATION_THRESHOLD}")
        print(f"  ğŸ“Š æœ€å°è®ºæ–‡æ•°è¦æ±‚: {MIN_PAPERS_FOR_CONTINUE} (æ¯è½®)")
        print(f"  ğŸ”„ æ¯è½®åè¿›è¡Œå……åˆ†æ€§è¯„ä¼°: æ˜¯")
        print(f"  ğŸ§  æ™ºèƒ½ç»ˆæ­¢: å¯ç”¨")
        print("="*60)

def create_download_folder(research_topic: str):
    """åˆ›å»ºä¸‹è½½æ–‡ä»¶å¤¹"""
    sanitized_topic = research_topic.replace(" ", "_")
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    folder_name = f"{sanitized_topic}_{timestamp}"
    result_dir = os.path.join(os.getcwd(), "downloads", folder_name)
    os.makedirs(result_dir, exist_ok=True)
    print(f"ğŸ“ å·²åˆ›å»ºæ–‡ä»¶å¤¹ï¼š{result_dir}")
    return result_dir

def process_papers_batch(papers_to_process, processor, download_dir, batch_name="è®ºæ–‡"):
    """å¤„ç†ä¸€æ‰¹è®ºæ–‡çš„é€šç”¨å‡½æ•°"""
    print(f"\nğŸ“¥ æ­£åœ¨å¤„ç†{batch_name}...")
    processed_papers = []
    
    for i, paper in enumerate(papers_to_process):
        if SHOW_PROGRESS_DETAILS:
            print(f"\nğŸ“„ æ­£åœ¨å¤„ç†{batch_name} {i+1}/{len(papers_to_process)}: {paper['title']}")
            if paper.get('citations', 0) > 0:
                print(f"    å¼•ç”¨æ•°: {paper['citations']}")
            print(f"    æ¥æº: {paper.get('source', 'unknown')}")
        else:
            print(f"ğŸ“„ æ­£åœ¨å¤„ç†{batch_name} {i+1}/{len(papers_to_process)}...")
        
        processed_paper = processor.process_paper(paper, download_dir=download_dir)
        if processed_paper:
            processed_papers.append(processed_paper)
            if SHOW_PROGRESS_DETAILS:
                text_len = processed_paper.get('text_length', 0)
                chunks = len(processed_paper.get('text_chunks', []))
                print(f"âœ… å¤„ç†æˆåŠŸ ({text_len:,} å­—ç¬¦, {chunks} å—)")
            else:
                print("âœ… å¤„ç†æˆåŠŸ")
        else:
            print("âŒ å¤„ç†å¤±è´¥")
    
    return processed_papers

def analyze_papers_batch(processed_papers, ai_client, batch_name="è®ºæ–‡"):
    """åˆ†æä¸€æ‰¹è®ºæ–‡çš„é€šç”¨å‡½æ•°"""
    analyses = []
    
    if ENABLE_DETAILED_ANALYSIS and processed_papers:
        papers_to_analyze = processed_papers[:MAX_ANALYSIS_PAPERS]
        
        if ENABLE_CONCURRENT_ANALYSIS and len(papers_to_analyze) > 1:
            print(f"\nğŸš€ æ­£åœ¨ä½¿ç”¨å¹¶å‘æ¨¡å¼åˆ†æ{len(papers_to_analyze)}ç¯‡{batch_name}...")
            analyses = ai_client.analyze_papers_concurrently(papers_to_analyze)
        else:
            print(f"\nğŸ§  æ­£åœ¨ä½¿ç”¨ä¸²è¡Œæ¨¡å¼åˆ†æ{len(papers_to_analyze)}ç¯‡{batch_name}...")
            for i, paper in enumerate(papers_to_analyze):
                if SHOW_PROGRESS_DETAILS:
                    print(f"  æ­£åœ¨åˆ†æ {i+1}/{len(papers_to_analyze)}: {paper['title']}...")
                else:
                    print(f"  æ­£åœ¨åˆ†æ{batch_name} {i+1}/{len(papers_to_analyze)}...")
                
                try:
                    text_chunks = paper.get('text_chunks', [])
                    analysis = ai_client.analyze_paper_text(paper['title'], paper['abstract'], text_chunks)
                    
                    analyses.append({
                        'paper': paper['title'],
                        'paper_id': paper.get('arxiv_id', ''),
                        'analysis': analysis,
                        'text_length': paper.get('text_length', 0),
                        'citations': paper.get('citations', 0),
                        'source': paper.get('source', 'unknown')
                    })
                    if SHOW_PROGRESS_DETAILS:
                        print(f"  âœ… åˆ†æå®Œæˆ")
                except Exception as e:
                    print(f"  âŒ åˆ†æå¤±è´¥: {e}")
    
    return analyses

def perform_adequacy_evaluation_after_round(ai_client, all_analyses, research_topic, round_num):
    """æ¯è½®æœç´¢åè¿›è¡Œå……åˆ†æ€§è¯„ä¼°"""
    print(f"\nğŸ” ç¬¬{round_num}è½®å……åˆ†æ€§è¯„ä¼°...")
    
    if not all_analyses:
        print("âš ï¸ æ²¡æœ‰åˆ†ææ•°æ®ï¼Œæ— æ³•è¿›è¡Œå……åˆ†æ€§è¯„ä¼°")
        return 0.0, "æ²¡æœ‰åˆ†ææ•°æ®è¿›è¡Œè¯„ä¼°", []
    
    # ç”Ÿæˆå½“å‰ç ”ç©¶æ€»ç»“ç”¨äºè¯„ä¼°
    print(f"ğŸ“ æ­£åœ¨ç”Ÿæˆç¬¬{round_num}è½®ç ”ç©¶æ€»ç»“ç”¨äºå……åˆ†æ€§è¯„ä¼°...")
    try:
        current_summary = ai_client.analyze_multiple_papers_summary(
            all_analyses, research_topic, depth_round=round_num
        )
        
        # è¿›è¡Œå……åˆ†æ€§è¯„ä¼°
        print(f"ğŸ§  æ­£åœ¨ä½¿ç”¨é«˜çº§æ¨¡å‹è¯„ä¼°ç ”ç©¶èµ„æ–™å……åˆ†æ€§...")
        adequacy_score, evaluation_report, missing_areas = ai_client.evaluate_research_adequacy(
            current_summary, research_topic, len(all_analyses)
        )
        
        print(f"ğŸ“Š ç¬¬{round_num}è½®å……åˆ†æ€§è¯„ä¼°ç»“æœ: {adequacy_score:.2f}/1.0")
        
        return adequacy_score, evaluation_report, missing_areas
        
    except Exception as e:
        print(f"âŒ å……åˆ†æ€§è¯„ä¼°å¤±è´¥: {e}")
        return 0.0, f"è¯„ä¼°å¤±è´¥: {e}", []

def should_continue_search(round_num, papers_found, adequacy_score):
    """åˆ¤æ–­æ˜¯å¦åº”è¯¥ç»§ç»­æœç´¢"""
    print(f"\nğŸ¤” åˆ¤æ–­æ˜¯å¦ç»§ç»­æœç´¢...")
    print(f"   ç¬¬{round_num}è½®è®ºæ–‡æ•°é‡: {papers_found}")
    print(f"   å……åˆ†æ€§è¯„åˆ†: {adequacy_score:.2f}")
    print(f"   å……åˆ†æ€§é˜ˆå€¼: {ADEQUACY_EVALUATION_THRESHOLD}")
    print(f"   æœ€å°è®ºæ–‡æ•°è¦æ±‚: {MIN_PAPERS_FOR_CONTINUE}")
    
    # æ¡ä»¶1: è®ºæ–‡æ•°é‡ä¸è¶³
    if papers_found < MIN_PAPERS_FOR_CONTINUE:
        print(f"âœ… ç»§ç»­æœç´¢ - è®ºæ–‡æ•°é‡ä¸è¶³({papers_found} < {MIN_PAPERS_FOR_CONTINUE})")
        return True, "è®ºæ–‡æ•°é‡ä¸è¶³"
    
    # æ¡ä»¶2: å……åˆ†æ€§è¯„åˆ†ä¸è¶³
    if adequacy_score < ADEQUACY_EVALUATION_THRESHOLD:
        print(f"âœ… ç»§ç»­æœç´¢ - å……åˆ†æ€§è¯„åˆ†ä¸è¶³({adequacy_score:.2f} < {ADEQUACY_EVALUATION_THRESHOLD})")
        return True, "å……åˆ†æ€§è¯„åˆ†ä¸è¶³"
    
    # ä¸¤ä¸ªæ¡ä»¶éƒ½æ»¡è¶³ï¼Œå¯ä»¥ç»“æŸ
    print(f"ğŸ›‘ ç»“æŸæœç´¢ - è®ºæ–‡æ•°é‡å……è¶³ä¸”å……åˆ†æ€§è¯„åˆ†è¾¾æ ‡")
    return False, "è®ºæ–‡æ•°é‡å’Œå……åˆ†æ€§éƒ½è¾¾æ ‡"

def perform_search_round(ai_client, searcher, research_topic, filters, round_num, previous_missing_areas=None, all_queries_used=None):
    """æ‰§è¡Œæœç´¢è½®æ¬¡"""
    print(f"\nğŸ” ç¬¬{round_num}è½®æœç´¢ ...")
    
    if round_num == 1:
        print(f"ğŸ§  æ­£åœ¨ä½¿ç”¨DeepSeekç”Ÿæˆ{NUM_SEARCH_QUERIES}ä¸ªåˆå§‹æœç´¢æŸ¥è¯¢...")
        queries = ai_client.generate_search_queries(research_topic, num_queries=NUM_SEARCH_QUERIES)
    else:
        # åç»­è½®æ¬¡ï¼šåŸºäºå‰ä¸€è½®çš„è¯„ä¼°ç»“æœç”Ÿæˆæ·±åº¦æœç´¢æŸ¥è¯¢
        if previous_missing_areas:
            print(f"ğŸ¯ åŸºäºç¬¬{round_num-1}è½®è¯„ä¼°ç»“æœç”Ÿæˆæ·±åº¦æœç´¢æŸ¥è¯¢...")
            print(f"   ç¼ºå¤±é¢†åŸŸ: {', '.join(previous_missing_areas)}")
            
            # ç”Ÿæˆé’ˆå¯¹ç¼ºå¤±é¢†åŸŸçš„æŸ¥è¯¢
            queries = ai_client.generate_depth_search_queries(
                research_topic, previous_missing_areas, all_queries_used or [], num_queries=DEPTH_SEARCH_QUERIES
            )
            if not queries:  # å¦‚æœç”Ÿæˆå¤±è´¥ï¼Œä½¿ç”¨é€šç”¨æ·±åº¦æŸ¥è¯¢
                queries = ai_client.generate_search_queries(f"advanced {research_topic}", num_queries=DEPTH_SEARCH_QUERIES)
        else:
            print(f"ğŸ¯ ç”Ÿæˆé€šç”¨æ·±åº¦æœç´¢æŸ¥è¯¢...")
            queries = ai_client.generate_search_queries(f"advanced {research_topic}", num_queries=DEPTH_SEARCH_QUERIES)
    
    if SHOW_PROGRESS_DETAILS:
        print(f"ç¬¬{round_num}è½®æŸ¥è¯¢:")
        for i, query in enumerate(queries, 1):
            print(f"  {i}. {query}")
    
    # ä½¿ç”¨å¢å¼ºæœç´¢å™¨
    papers = searcher.search_multiple_queries_enhanced(queries, filters)
    
    if not papers:
        print(f"âŒ ç¬¬{round_num}è½®æœªæ‰¾åˆ°ä»»ä½•è®ºæ–‡!")
        return [], queries
    
    print(f"ğŸ“š ç¬¬{round_num}è½®æ‰¾åˆ°{len(papers)}ç¯‡ç¬¦åˆæ¡ä»¶çš„è®ºæ–‡")
    
    # æ˜¾ç¤ºè¯¦ç»†ç»Ÿè®¡
    source_stats = {}
    citation_stats = []
    
    for paper in papers:
        source = paper.get('source', 'unknown')
        source_stats[source] = source_stats.get(source, 0) + 1
        if paper.get('citations', 0) > 0:
            citation_stats.append(paper['citations'])
    
    print(f"ğŸ“Š æ¥æºåˆ†å¸ƒ: {', '.join([f'{k}: {v}ç¯‡' for k, v in source_stats.items()])}")
    
    if citation_stats:
        print(f"ğŸ“ˆ å¼•ç”¨æ•°ç»Ÿè®¡: èŒƒå›´ {min(citation_stats)}-{max(citation_stats)}, å¹³å‡ {sum(citation_stats)/len(citation_stats):.1f}")
    
    return papers, queries

def main():
    print_banner()
    
    # åˆå§‹åŒ–ç»„ä»¶
    print("ğŸš€ æ­£åœ¨åˆå§‹æœç´¢ç³»ç»Ÿ...")
    ai_client = DeepSeekClient()
    searcher = EnhancedPaperSearcher()
    processor = EnhancedPDFProcessor()
    
    print("âœ… å­¦æœ¯æœç´¢ç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ")
    print("   - ä¸»è¦æœç´¢æº: Google Scholar")
    print("   - å¤‡ç”¨æœç´¢æº: arXiv")
    print("   - æ™ºèƒ½è¿‡æ»¤: ä¼šè®®ã€å¼•ç”¨æ•°ã€æ—¶é—´èŒƒå›´")
    print("   - å……åˆ†æ€§è¯„ä¼°: æ¯è½®è‡ªåŠ¨è¯„ä¼°")
    print("   - æ™ºèƒ½ç»ˆæ­¢: åŸºäºè¯„ä¼°ç»“æœå†³å®š")
    
    # è¾“å‡ºç›®å½•
    output_dir = Path(OUTPUT_DIR)
    output_dir.mkdir(exist_ok=True)
    
    # è·å–ç ”ç©¶ä¸»é¢˜
    research_topic = get_research_topic()
    # å¼€å§‹è®¡æ—¶
    start_time = time.time()

    print(f"\nğŸ“‹ ç ”ç©¶ä¸»é¢˜: {research_topic}")
    download_dir = create_download_folder(research_topic)
    
    # è·å–æœç´¢è¿‡æ»¤å™¨ï¼ˆå¢å¼ºæ¨¡å¼ï¼‰
    filters = searcher.get_user_search_preferences()
    
    # æ˜¾ç¤ºé…ç½®
    display_config()
    
    # åˆå§‹åŒ–å¤šè½®æœç´¢å˜é‡
    all_processed_papers = []
    all_analyses = []
    all_queries_used = []
    search_rounds_results = []
    final_adequacy_score = 0.0
    final_evaluation_report = ""
    
    try:
        # å¤šè½®æœç´¢å¾ªç¯
        for search_round in range(1, SEARCH_DEPTH + 1):
            print(f"\n{'='*80}")
            print(f"ğŸ”„ å¼€å§‹ç¬¬ {search_round}/{SEARCH_DEPTH} è½®æœç´¢")
            print(f"{'='*80}")
            
            # è·å–ä¸Šä¸€è½®çš„ç¼ºå¤±é¢†åŸŸï¼ˆå¦‚æœæœ‰ï¼‰
            previous_missing_areas = None
            if len(search_rounds_results) > 0:
                previous_missing_areas = search_rounds_results[-1].get('missing_areas', [])
            
            # æ‰§è¡Œæœç´¢
            papers, queries = perform_search_round(
                ai_client, searcher, research_topic, filters, search_round, 
                previous_missing_areas, all_queries_used
            )
            
            # è®°å½•ä½¿ç”¨çš„æŸ¥è¯¢
            all_queries_used.extend(queries)
            
            if not papers:
                print(f"âš ï¸ ç¬¬{search_round}è½®æœªæ‰¾åˆ°è®ºæ–‡ï¼Œè·³è¿‡æ­¤è½®")
                
                # å³ä½¿æ²¡æ‰¾åˆ°è®ºæ–‡ï¼Œä¹Ÿè®°å½•æœ¬è½®ç»“æœ
                round_result = {
                    'round': search_round,
                    'queries': queries,
                    'papers_found': 0,
                    'papers_processed': 0,
                    'papers_analyzed': 0,
                    'adequacy_score': 0.0,
                    'evaluation_report': "æœ¬è½®æœªæ‰¾åˆ°è®ºæ–‡",
                    'missing_areas': [],
                    'should_continue': False,
                    'continue_reason': "æœªæ‰¾åˆ°è®ºæ–‡",
                    'source_distribution': {},
                    'cumulative_papers_analyzed': len(all_analyses),
                    'cumulative_papers_processed': len(all_processed_papers)
                }
                search_rounds_results.append(round_result)
                continue
            
            # æ˜¾ç¤ºæœç´¢ç»“æœæ‘˜è¦
            if SHOW_PROGRESS_DETAILS:
                searcher.display_search_results(papers, max_display=15)
            
            # é™åˆ¶å¤„ç†çš„è®ºæ–‡æ•°é‡
            papers_to_process = papers[:MAX_PAPERS_PER_DEPTH]
            print(f"ğŸ“„ ç¬¬{search_round}è½®å°†å¤„ç†{len(papers_to_process)}ç¯‡è®ºæ–‡")
            
            # å¤„ç†è®ºæ–‡
            processed_papers = process_papers_batch(
                papers_to_process, processor, download_dir, f"ç¬¬{search_round}è½®è®ºæ–‡"
            )
            
            if not processed_papers:
                print(f"âŒ ç¬¬{search_round}è½®æ²¡æœ‰è®ºæ–‡èƒ½å¤ŸæˆåŠŸå¤„ç†!")
                
                # è®°å½•å¤±è´¥çš„è½®æ¬¡
                round_result = {
                    'round': search_round,
                    'queries': queries,
                    'papers_found': len(papers),
                    'papers_processed': 0,
                    'papers_analyzed': 0,
                    'adequacy_score': 0.0,
                    'evaluation_report': "è®ºæ–‡å¤„ç†å¤±è´¥",
                    'missing_areas': [],
                    'should_continue': False,
                    'continue_reason': "è®ºæ–‡å¤„ç†å¤±è´¥",
                    'source_distribution': {},
                    'cumulative_papers_analyzed': len(all_analyses),
                    'cumulative_papers_processed': len(all_processed_papers)
                }
                search_rounds_results.append(round_result)
                continue
            
            print(f"\nğŸ‰ ç¬¬{search_round}è½®æˆåŠŸå¤„ç†äº†{len(processed_papers)}ç¯‡è®ºæ–‡!")
            
            # åˆ†æè®ºæ–‡
            analyses = analyze_papers_batch(processed_papers, ai_client, f"ç¬¬{search_round}è½®è®ºæ–‡")
            
            # ç´¯ç§¯ç»“æœ
            all_processed_papers.extend(processed_papers)
            all_analyses.extend(analyses)
            
            # æ¯è½®åç«‹å³è¿›è¡Œå……åˆ†æ€§è¯„ä¼°
            adequacy_score, evaluation_report, missing_areas = perform_adequacy_evaluation_after_round(
                ai_client, all_analyses, research_topic, search_round
            )
            
            # åˆ¤æ–­æ˜¯å¦åº”è¯¥ç»§ç»­æœç´¢
            should_continue, continue_reason = should_continue_search(
                search_round, len(papers), adequacy_score
            )
            
            # ç»Ÿè®¡æ¥æºåˆ†å¸ƒ
            source_distribution = {}
            for paper in papers:
                source = paper.get('source', 'unknown')
                source_distribution[source] = source_distribution.get(source, 0) + 1
            
            # è®°å½•æœ¬è½®è¯¦ç»†ç»“æœ
            round_result = {
                'round': search_round,
                'queries': queries,
                'papers_found': len(papers),
                'papers_processed': len(processed_papers),
                'papers_analyzed': len(analyses),
                'adequacy_score': adequacy_score,
                'evaluation_report': evaluation_report,
                'missing_areas': missing_areas,
                'should_continue': should_continue,
                'continue_reason': continue_reason,
                'source_distribution': source_distribution,
                'cumulative_papers_analyzed': len(all_analyses),
                'cumulative_papers_processed': len(all_processed_papers)
            }
            search_rounds_results.append(round_result)
            
            print(f"\nğŸ“Š ç¬¬{search_round}è½®è¯¦ç»†ç»Ÿè®¡:")
            print(f"   æ‰¾åˆ°è®ºæ–‡: {len(papers)}")
            print(f"   æˆåŠŸå¤„ç†: {len(processed_papers)}")
            print(f"   å®Œæˆåˆ†æ: {len(analyses)}")
            print(f"   æ¥æºåˆ†å¸ƒ: {source_distribution}")
            print(f"   å……åˆ†æ€§è¯„åˆ†: {adequacy_score:.2f}/1.0")
            print(f"   ç´¯è®¡å·²åˆ†æ: {len(all_analyses)} ç¯‡")
            
            # æ˜¾ç¤ºå¼•ç”¨æ•°ç»Ÿè®¡ï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰
            if analyses:
                citations_data = [a.get('citations', 0) for a in analyses if a.get('citations', 0) > 0]
                if citations_data:
                    print(f"   å¼•ç”¨æ•°èŒƒå›´: {min(citations_data)} - {max(citations_data)}")
                    print(f"   å¹³å‡å¼•ç”¨æ•°: {sum(citations_data) / len(citations_data):.1f}")
            
            # æ ¹æ®å……åˆ†æ€§è¯„ä¼°ç»“æœå†³å®šæ˜¯å¦ç»§ç»­
            if not should_continue:
                print(f"\nğŸ¯ ç¬¬{search_round}è½®åå†³å®šç»“æŸæœç´¢")
                print(f"   åŸå› : {continue_reason}")
                print(f"   æœ€ç»ˆå……åˆ†æ€§è¯„åˆ†: {adequacy_score:.2f}")
                final_adequacy_score = adequacy_score
                final_evaluation_report = evaluation_report
                break
            elif search_round < SEARCH_DEPTH:
                print(f"\nâ¡ï¸ ç¬¬{search_round}è½®åå†³å®šç»§ç»­æœç´¢")
                print(f"   åŸå› : {continue_reason}")
                if missing_areas:
                    print(f"   å°†é‡ç‚¹å…³æ³¨: {', '.join(missing_areas[:3])}...")
                final_adequacy_score = adequacy_score
                final_evaluation_report = evaluation_report
            else:
                print(f"\nğŸ”š å·²è¾¾åˆ°æœ€å¤§æœç´¢æ·±åº¦ {SEARCH_DEPTH}")
                final_adequacy_score = adequacy_score
                final_evaluation_report = evaluation_report
        
        # ç”Ÿæˆæœ€ç»ˆç ”ç©¶æ€»ç»“
        final_research_summary = ""
        if GENERATE_RESEARCH_SUMMARY and all_analyses:
            print(f"\nğŸ“ æ­£åœ¨ç”Ÿæˆæœ€ç»ˆç ”ç©¶æ€»ç»“ (åŸºäº{len(all_analyses)}ç¯‡è®ºæ–‡çš„åˆ†æ)...")
            final_research_summary = ai_client.analyze_multiple_papers_summary(
                all_analyses, research_topic, depth_round=len(search_rounds_results)
            )
        
        # ä¿å­˜ç»“æœ
        results = {
            'research_topic': research_topic,
            'search_mode': 'enhanced',
            'search_source': 'google_scholar_arxiv',
            'search_depth': SEARCH_DEPTH,
            'actual_rounds_completed': len(search_rounds_results),
            'early_termination': len(search_rounds_results) < SEARCH_DEPTH,
            'termination_reason': search_rounds_results[-1].get('continue_reason', 'Unknown') if search_rounds_results else 'Unknown',
            'total_papers_found': sum(r['papers_found'] for r in search_rounds_results),
            'total_papers_processed': len(all_processed_papers),
            'total_papers_analyzed': len(all_analyses),
            'final_adequacy_score': final_adequacy_score,
            'final_evaluation_report': final_evaluation_report,
            'adequacy_threshold_used': ADEQUACY_EVALUATION_THRESHOLD,
            'search_rounds_results': search_rounds_results,  # åŒ…å«æ¯è½®çš„å……åˆ†æ€§è¯„ä¼°
            'all_queries_used': all_queries_used,
            'filters_used': filters.__dict__ if filters else None,
            'adequacy_evaluation_timeline': [  # å……åˆ†æ€§è¯„ä¼°æ—¶é—´çº¿
                {
                    'round': r['round'],
                    'adequacy_score': r['adequacy_score'],
                    'missing_areas': r['missing_areas'],
                    'cumulative_papers': r['cumulative_papers_analyzed'],
                    'should_continue': r['should_continue'],
                    'continue_reason': r['continue_reason']
                }
                for r in search_rounds_results
            ],
            'configuration': {
                'search_depth': SEARCH_DEPTH,
                'num_search_queries': NUM_SEARCH_QUERIES,
                'papers_per_query': PAPERS_PER_QUERY,
                'depth_search_queries': DEPTH_SEARCH_QUERIES,
                'papers_per_depth_query': PAPERS_PER_DEPTH_QUERY,
                'max_papers_per_depth': MAX_PAPERS_PER_DEPTH,
                'extract_full_pdf': EXTRACT_FULL_PDF,
                'max_analysis_papers': MAX_ANALYSIS_PAPERS,
                'concurrent_analysis_enabled': ENABLE_CONCURRENT_ANALYSIS,
                'max_concurrent_analysis': MAX_CONCURRENT_ANALYSIS if ENABLE_CONCURRENT_ANALYSIS else 0,
                'adequacy_evaluation_threshold': ADEQUACY_EVALUATION_THRESHOLD,
                'min_papers_for_continue': MIN_PAPERS_FOR_CONTINUE
            },
            'paper_analyses': all_analyses if ENABLE_DETAILED_ANALYSIS else [],
            'final_research_summary': final_research_summary
        }
        
        # å¯é€‰ï¼šä¿å­˜å®Œæ•´æ–‡æœ¬
        if SAVE_FULL_TEXT:
            results['processed_papers'] = []
            for paper in all_processed_papers:
                paper_data = paper.copy()
                if not EXTRACT_FULL_PDF:
                    pass
                else:
                    if 'extracted_text' in paper_data and len(paper_data['extracted_text']) > 10000:
                        paper_data['extracted_text_preview'] = paper_data['extracted_text'][:5000] + "..."
                        paper_data['extracted_text_full'] = paper_data['extracted_text']
                
                results['processed_papers'].append(paper_data)
        
        # ä¿å­˜åˆ°æ–‡ä»¶
        safe_topic = "".join(c for c in research_topic if c.isalnum() or c in (' ', '-', '_')).replace(' ', '_')
        output_file = output_dir / f"research_results_{safe_topic}_enhanced_depth{SEARCH_DEPTH}.json"
        
        print(f"\nğŸ’¾ æ­£åœ¨ä¿å­˜ç»“æœåˆ° {output_file}...")
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(results, f, indent=2, ensure_ascii=False, default=str)
        
        # æ˜¾ç¤ºæœ€ç»ˆç»“æœ
        print("\n" + "="*80)
        print("ğŸ“Š å­¦æœ¯æ·±åº¦ç ”ç©¶ç»“æœæ€»ç»“")
        print("="*80)
        print(f"ğŸ¯ ä¸»é¢˜: {research_topic}")
        print(f"ğŸ”§ æœç´¢æ¨¡å¼: é»˜è®¤ä¸ºå¢å¼ºæ¨¡å¼ (Google Scholar + arXiv + æ™ºèƒ½è¿‡æ»¤)")
        print(f"ğŸ”„ æœç´¢æ·±åº¦: {SEARCH_DEPTH} è½® (å®é™…å®Œæˆ: {len(search_rounds_results)} è½®)")
        
        # æ˜¾ç¤ºæ˜¯å¦æå‰ç»ˆæ­¢
        if len(search_rounds_results) < SEARCH_DEPTH:
            print(f"â¹ï¸ æå‰ç»ˆæ­¢: æ˜¯ (åŸå› : {results['termination_reason']})")
        else:
            print(f"â¹ï¸ æå‰ç»ˆæ­¢: å¦ (å®Œæˆå…¨éƒ¨æœç´¢è½®æ¬¡)")
        
        print(f"ğŸ“ ä½¿ç”¨çš„æœç´¢æŸ¥è¯¢æ€»æ•°: {len(all_queries_used)}")
        print(f"ğŸ“š æ‰¾åˆ°çš„è®ºæ–‡æ€»æ•°: {sum(r['papers_found'] for r in search_rounds_results)}")
        print(f"âœ… æˆåŠŸå¤„ç†çš„è®ºæ–‡: {len(all_processed_papers)}")
        print(f"ğŸ§  å·²åˆ†æçš„è®ºæ–‡: {len(all_analyses)}")
        print(f"ğŸ“Š æœ€ç»ˆå……åˆ†æ€§è¯„åˆ†: {final_adequacy_score:.2f}/1.0 (é˜ˆå€¼: {ADEQUACY_EVALUATION_THRESHOLD})")
        
        # æ˜¾ç¤ºæ€»ä½“æ¥æºåˆ†å¸ƒ
        total_source_stats = {}
        for round_result in search_rounds_results:
            for source, count in round_result.get('source_distribution', {}).items():
                total_source_stats[source] = total_source_stats.get(source, 0) + count
        
        if total_source_stats:
            print(f"\nğŸ“Š æ€»ä½“æ¥æºåˆ†å¸ƒ:")
            for source, count in total_source_stats.items():
                percentage = (count / sum(total_source_stats.values())) * 100
                print(f"   {source}: {count}ç¯‡ ({percentage:.1f}%)")
        
        if filters:
            print(f"\nğŸ”§ ä½¿ç”¨çš„è¿‡æ»¤æ¡ä»¶:")
            if filters.conferences:
                print(f"   ä¼šè®®ç­›é€‰: {', '.join(filters.conferences)}")
            if filters.exclude_conferences:
                print(f"   æ’é™¤ä¼šè®®: {', '.join(filters.exclude_conferences)}")
            if filters.min_citations > 0:
                print(f"   æœ€å°å¼•ç”¨æ•°: {filters.min_citations}")
            if filters.max_citations:
                print(f"   æœ€å¤§å¼•ç”¨æ•°: {filters.max_citations}")
            if filters.start_date:
                print(f"   å¼€å§‹æ—¥æœŸ: {filters.start_date.strftime('%Y-%m-%d')}")
            if filters.end_date:
                print(f"   ç»“æŸæ—¥æœŸ: {filters.end_date.strftime('%Y-%m-%d')}")
        
        # æ˜¾ç¤ºå¼•ç”¨æ•°ç»Ÿè®¡
        if all_analyses:
            citations_data = [a.get('citations', 0) for a in all_analyses if a.get('citations', 0) > 0]
            if citations_data:
                print(f"\nğŸ“Š å¼•ç”¨æ•°ç»Ÿè®¡:")
                print(f"   æœ‰å¼•ç”¨æ•°çš„è®ºæ–‡: {len(citations_data)}/{len(all_analyses)}")
                print(f"   å¼•ç”¨æ•°èŒƒå›´: {min(citations_data)} - {max(citations_data)}")
                print(f"   å¹³å‡å¼•ç”¨æ•°: {sum(citations_data) / len(citations_data):.1f}")
                print(f"   é«˜å¼•ç”¨è®ºæ–‡ (>100): {len([c for c in citations_data if c > 100])}")
        
        # æ˜¾ç¤ºå„è½®å……åˆ†æ€§è¯„ä¼°ç»“æœ
        print(f"\nğŸ“ˆ å„è½®æœç´¢ä¸å……åˆ†æ€§è¯„ä¼°ç»Ÿè®¡:")
        for round_result in search_rounds_results:
            round_num = round_result['round']
            print(f"  ç¬¬{round_num}è½®:")
            print(f"    ğŸ“š è®ºæ–‡: æ‰¾åˆ°{round_result['papers_found']}ç¯‡ | å¤„ç†{round_result['papers_processed']}ç¯‡ | åˆ†æ{round_result['papers_analyzed']}ç¯‡")
            print(f"    ğŸ“Š å……åˆ†æ€§è¯„åˆ†: {round_result['adequacy_score']:.2f}/1.0")
            print(f"    ğŸ¤” ç»§ç»­æœç´¢: {'æ˜¯' if round_result['should_continue'] else 'å¦'} ({round_result['continue_reason']})")
            
            if round_result.get('missing_areas'):
                print(f"    ğŸ¯ ç¼ºå¤±é¢†åŸŸ: {', '.join(round_result['missing_areas'][:3])}{'...' if len(round_result['missing_areas']) > 3 else ''}")
            
            source_dist = round_result.get('source_distribution', {})
            if source_dist:
                print(f"    ğŸ” æ¥æºåˆ†å¸ƒ: {', '.join([f'{k}:{v}' for k, v in source_dist.items()])}")
            print()
        
        # æ˜¾ç¤ºå……åˆ†æ€§è¯„ä¼°æ—¶é—´çº¿
        print(f"ğŸ“ˆ å……åˆ†æ€§è¯„ä¼°æ—¶é—´çº¿:")
        for eval_point in results['adequacy_evaluation_timeline']:
            print(f"  ç¬¬{eval_point['round']}è½®å: è¯„åˆ† {eval_point['adequacy_score']:.2f} | ç´¯è®¡åˆ†æ {eval_point['cumulative_papers']}ç¯‡ | {'ç»§ç»­' if eval_point['should_continue'] else 'ç»“æŸ'}")
        
        if final_research_summary:
            print(f"\nğŸ“ æœ€ç»ˆç ”ç©¶æ€»ç»“:")
            print("-" * 60)
            print(final_research_summary)
        
        if final_evaluation_report:
            print(f"\nğŸ” æœ€ç»ˆå……åˆ†æ€§è¯„ä¼°æŠ¥å‘Š:")
            print("-" * 60)
            print(final_evaluation_report)
        
        print(f"\nğŸ’¾ å®Œæ•´ç»“æœå·²ä¿å­˜åˆ°: {output_file}")
        
        print("\nğŸ‰ å­¦æœ¯æ·±åº¦ç ”ç©¶ç³»ç»Ÿè¿è¡Œå®Œæˆ!")
        end_time = time.time()
        duration = end_time - start_time
        # æ™ºèƒ½æ ¼å¼åŒ–
        hours = int(duration // 3600)
        minutes = int((duration % 3600) // 60)
        seconds = duration % 60

        if hours > 0:
            formatted = f"{hours}å°æ—¶{minutes}åˆ†é’Ÿ{seconds:.2f}ç§’"
        elif minutes > 0:
            formatted = f"{minutes}åˆ†é’Ÿ{seconds:.2f}ç§’"
        else:
            formatted = f"{seconds:.2f}ç§’"

        print(f"âœ¨ æ™ºèƒ½æœç´¢ + è¿‡æ»¤ + æ¯è½®å……åˆ†æ€§è¯„ä¼° + è‡ªåŠ¨ç»ˆæ­¢å†³ç­–æ€»ç”¨æ—¶: {formatted}")
            
        
    except Exception as e:
        print(f"\nâŒ ç ”ç©¶è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        if USE_RETRY_ON_FAILURE:
            print("ğŸ”„ æ‚¨å¯ä»¥å°è¯•é‡æ–°è¿è¡Œç¨‹åºã€‚")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()
